Epoch 1/10 LR 0.0001 and batch_size 40
Batch 60: Loss = 1.9383
Batch 120: Loss = 1.7657
Batch 180: Loss = 1.7650
Batch 240: Loss = 1.6552
Batch 300: Loss = 1.6271
Batch 360: Loss = 1.9213
Batch 420: Loss = 1.9436
Batch 480: Loss = 1.7121
Batch 540: Loss = 1.6567
Batch 600: Loss = 1.4895
Batch 660: Loss = 1.7017
Batch 720: Loss = 1.6568
Batch 780: Loss = 1.5133
Batch 840: Loss = 1.6146
Batch 900: Loss = 1.4281
Batch 960: Loss = 1.5974
Batch 1020: Loss = 1.4964
Batch 1080: Loss = 1.4847
Batch 1140: Loss = 1.6067
Batch 1200: Loss = 1.4830
Batch 1260: Loss = 1.7709
Batch 1320: Loss = 1.5488
Batch 1380: Loss = 1.5001
Batch 1440: Loss = 1.5966
Batch 1500: Loss = 1.6129
Batch 1560: Loss = 1.5097
Batch 1620: Loss = 1.5770
Batch 1680: Loss = 1.5849
Batch 1740: Loss = 1.2973
Batch 1800: Loss = 1.4100
Batch 1860: Loss = 1.3809
Batch 1920: Loss = 1.4517
Batch 1980: Loss = 1.3410
Batch 2040: Loss = 1.3570
Batch 2100: Loss = 1.2433
Batch 2160: Loss = 1.3598
Batch 2220: Loss = 1.5156
  Train Loss: 1.5992 | Train Acc: 41.59% | Val Loss: 1.4054 | Val Acc: 48.71%
Epoch 2/10 LR 0.0001 and batch_size 40
Batch 60: Loss = 1.4823
Batch 120: Loss = 1.0998
Batch 180: Loss = 1.4360
Batch 240: Loss = 1.2565
Batch 300: Loss = 1.4166
Batch 360: Loss = 1.3164
Batch 420: Loss = 1.4165
Batch 480: Loss = 1.3097
Batch 540: Loss = 1.4194
Batch 600: Loss = 1.0913
Batch 660: Loss = 1.2505
Batch 720: Loss = 1.3982
Batch 780: Loss = 1.2139
Batch 840: Loss = 1.1765
Batch 900: Loss = 0.9791
Batch 960: Loss = 1.1784
Batch 1020: Loss = 1.3519
Batch 1080: Loss = 1.0613
Batch 1140: Loss = 1.3954
Batch 1200: Loss = 1.3444
Batch 1260: Loss = 1.1961
Batch 1320: Loss = 1.1649
Batch 1380: Loss = 1.2429
Batch 1440: Loss = 1.2971
Batch 1500: Loss = 1.5125
Batch 1560: Loss = 1.5554
Batch 1620: Loss = 1.3330
Batch 1680: Loss = 1.3909
Batch 1740: Loss = 1.5689
Batch 1800: Loss = 1.1612
Batch 1860: Loss = 0.9996
Batch 1920: Loss = 1.3895
Batch 1980: Loss = 1.2295
Batch 2040: Loss = 1.1911
Batch 2100: Loss = 1.2751
Batch 2160: Loss = 0.9324
Batch 2220: Loss = 1.5865
  Train Loss: 1.3242 | Train Acc: 51.93% | Val Loss: 1.2792 | Val Acc: 53.43%
Epoch 3/10 LR 0.0001 and batch_size 40
Batch 60: Loss = 1.0026
Batch 120: Loss = 1.0523
Batch 180: Loss = 1.2787
Batch 240: Loss = 1.2864
Batch 300: Loss = 1.2816
Batch 360: Loss = 1.1371
Batch 420: Loss = 1.1612
Batch 480: Loss = 1.3645
Batch 540: Loss = 1.0529
Batch 600: Loss = 1.4868
Batch 660: Loss = 1.3618
Batch 720: Loss = 1.2511
Batch 780: Loss = 1.4005
Batch 840: Loss = 1.1938
Batch 900: Loss = 1.4092
Batch 960: Loss = 1.0903
Batch 1020: Loss = 1.1683
Batch 1080: Loss = 1.0363
Batch 1140: Loss = 1.0313
Batch 1200: Loss = 1.1822
Batch 1260: Loss = 0.8740
Batch 1320: Loss = 1.1926
Batch 1380: Loss = 1.0075
Batch 1440: Loss = 1.2335
Batch 1500: Loss = 1.3007
Batch 1560: Loss = 1.1524
Batch 1620: Loss = 1.4774
Batch 1680: Loss = 1.4263
Batch 1740: Loss = 1.0634
Batch 1800: Loss = 1.0132
Batch 1860: Loss = 1.0824
Batch 1920: Loss = 1.0812
Batch 1980: Loss = 1.1400
Batch 2040: Loss = 1.1272
Batch 2100: Loss = 0.7908
Batch 2160: Loss = 2.0038
Batch 2220: Loss = 1.5245
  Train Loss: 1.1846 | Train Acc: 57.40% | Val Loss: 1.2236 | Val Acc: 55.92%
Epoch 4/10 LR 0.0001 and batch_size 40
Batch 60: Loss = 0.9974
Batch 120: Loss = 1.0023
Batch 180: Loss = 0.9410
Batch 240: Loss = 1.0883
Batch 300: Loss = 1.5916
Batch 360: Loss = 0.7792
Batch 420: Loss = 0.8965
Batch 480: Loss = 0.9333
Batch 540: Loss = 1.1143
Batch 600: Loss = 1.3093
Batch 660: Loss = 1.1566
Batch 720: Loss = 1.0123
Batch 780: Loss = 1.1517
Batch 840: Loss = 0.6488
Batch 900: Loss = 0.8064
Batch 960: Loss = 1.0219
Batch 1020: Loss = 1.0392
Batch 1080: Loss = 1.2616
Batch 1140: Loss = 1.0807
Batch 1200: Loss = 0.9485
Batch 1260: Loss = 0.9634
Batch 1320: Loss = 0.8339
Batch 1380: Loss = 0.7665
Batch 1440: Loss = 1.4285
Batch 1500: Loss = 0.9352
Batch 1560: Loss = 0.8333
Batch 1620: Loss = 1.0152
Batch 1680: Loss = 1.0707
Batch 1740: Loss = 0.9631
Batch 1800: Loss = 1.2337
Batch 1860: Loss = 1.1375
Batch 1920: Loss = 1.1117
Batch 1980: Loss = 1.0800
Batch 2040: Loss = 0.9240
Batch 2100: Loss = 1.4254
Batch 2160: Loss = 1.0325
Batch 2220: Loss = 0.9861
  Train Loss: 1.0687 | Train Acc: 61.73% | Val Loss: 1.2268 | Val Acc: 56.74%
Epoch 5/10 LR 0.0001 and batch_size 40
Batch 60: Loss = 1.2028
Batch 120: Loss = 1.0214
Batch 180: Loss = 1.1579
Batch 240: Loss = 0.9083
Batch 300: Loss = 1.1131
Batch 360: Loss = 0.8998
Batch 420: Loss = 1.0214
Batch 480: Loss = 0.9645
Batch 540: Loss = 0.8362
Batch 600: Loss = 1.2740
Batch 660: Loss = 0.8222
Batch 720: Loss = 0.9128
Batch 780: Loss = 0.8558
Batch 840: Loss = 0.8052
Batch 900: Loss = 0.9135
Batch 960: Loss = 1.3266
Batch 1020: Loss = 1.0584
Batch 1080: Loss = 0.5445
Batch 1140: Loss = 1.2505
Batch 1200: Loss = 1.0833
Batch 1260: Loss = 0.7645
Batch 1320: Loss = 0.9542
Batch 1380: Loss = 0.9219
Batch 1440: Loss = 1.1179
Batch 1500: Loss = 0.9026
Batch 1560: Loss = 1.2931
Batch 1620: Loss = 1.0117
Batch 1680: Loss = 0.9562
Batch 1740: Loss = 0.9059
Batch 1800: Loss = 0.8403
Batch 1860: Loss = 0.7163
Batch 1920: Loss = 0.7113
Batch 1980: Loss = 0.9252
Batch 2040: Loss = 1.3156
Batch 2100: Loss = 0.9642
Batch 2160: Loss = 1.1348
Batch 2220: Loss = 0.6528
  Train Loss: 0.9577 | Train Acc: 65.76% | Val Loss: 1.1920 | Val Acc: 57.74%
Epoch 6/10 LR 0.0001 and batch_size 40
Batch 60: Loss = 0.5666
Batch 120: Loss = 0.8253
Batch 180: Loss = 0.7845
Batch 240: Loss = 0.8371
Batch 300: Loss = 0.5861
Batch 360: Loss = 0.7584
Batch 420: Loss = 0.8679
Batch 480: Loss = 0.6871
Batch 540: Loss = 0.7656
Batch 600: Loss = 0.9066
Batch 660: Loss = 1.1227
Batch 720: Loss = 1.0729
Batch 780: Loss = 0.6981
Batch 840: Loss = 0.7097
Batch 900: Loss = 0.9624
Batch 960: Loss = 0.7288
Batch 1020: Loss = 0.9881
Batch 1080: Loss = 0.9312
Batch 1140: Loss = 1.0663
Batch 1200: Loss = 0.9073
Batch 1260: Loss = 0.8365
Batch 1320: Loss = 0.7808
Batch 1380: Loss = 0.7723
Batch 1440: Loss = 0.8960
Batch 1500: Loss = 0.4763
Batch 1560: Loss = 0.7403
Batch 1620: Loss = 0.7601
Batch 1680: Loss = 0.8765
Batch 1740: Loss = 0.8638
Batch 1800: Loss = 1.0190
Batch 1860: Loss = 0.7126
Batch 1920: Loss = 0.7406
Batch 1980: Loss = 0.8509
Batch 2040: Loss = 1.0124
Batch 2100: Loss = 0.9127
Batch 2160: Loss = 0.8419
Batch 2220: Loss = 1.0332
  Train Loss: 0.8452 | Train Acc: 69.74% | Val Loss: 1.2968 | Val Acc: 56.25%
Epoch 7/10 LR 0.0001 and batch_size 40
Batch 60: Loss = 0.6911
Batch 120: Loss = 0.5399
Batch 180: Loss = 0.8058
Batch 240: Loss = 0.6127
Batch 300: Loss = 0.5696
Batch 360: Loss = 1.1060
Batch 420: Loss = 0.7928
Batch 480: Loss = 0.9226
Batch 540: Loss = 0.7400
Batch 600: Loss = 0.6274
Batch 660: Loss = 0.6437
Batch 720: Loss = 0.7750
Batch 780: Loss = 0.8992
Batch 840: Loss = 0.5464
Batch 900: Loss = 0.5479
Batch 960: Loss = 0.8652
Batch 1020: Loss = 0.7872
Batch 1080: Loss = 0.4895
Batch 1140: Loss = 0.7134
Batch 1200: Loss = 0.8837
Batch 1260: Loss = 0.6092
Batch 1320: Loss = 0.8124
Batch 1380: Loss = 0.8449
Batch 1440: Loss = 0.8134
Batch 1500: Loss = 0.5354
Batch 1560: Loss = 0.9892
Batch 1620: Loss = 0.5877
Batch 1680: Loss = 0.7136
Batch 1740: Loss = 0.7947
Batch 1800: Loss = 1.0016
Batch 1860: Loss = 1.2737
Batch 1920: Loss = 0.6675
Batch 1980: Loss = 0.4645
Batch 2040: Loss = 0.9399
Batch 2100: Loss = 0.9509
Batch 2160: Loss = 0.4814
Batch 2220: Loss = 0.6034
  Train Loss: 0.7348 | Train Acc: 73.69% | Val Loss: 1.2502 | Val Acc: 58.81%
Epoch 8/10 LR 0.0001 and batch_size 40
Batch 60: Loss = 0.6366
Batch 120: Loss = 0.4401
Batch 180: Loss = 0.3588
Batch 240: Loss = 0.4096
Batch 300: Loss = 0.8358
Batch 360: Loss = 0.4885
Batch 420: Loss = 1.0261
Batch 480: Loss = 0.4409
Batch 540: Loss = 1.0122
Batch 600: Loss = 0.3718
Batch 660: Loss = 0.7478
Batch 720: Loss = 0.6037
Batch 780: Loss = 0.8132
Batch 840: Loss = 0.4542
Batch 900: Loss = 0.3946
Batch 960: Loss = 0.5060
Batch 1020: Loss = 0.4258
Batch 1080: Loss = 0.6520
Batch 1140: Loss = 0.5695
Batch 1200: Loss = 0.6604
Batch 1260: Loss = 0.8436
Batch 1320: Loss = 0.4706
Batch 1380: Loss = 0.3468
Batch 1440: Loss = 0.6926
Batch 1500: Loss = 0.8941
Batch 1560: Loss = 0.7186
Batch 1620: Loss = 0.6658
Batch 1680: Loss = 0.7515
Batch 1740: Loss = 0.4767
Batch 1800: Loss = 0.4800
Batch 1860: Loss = 0.8745
Batch 1920: Loss = 0.7166
Batch 1980: Loss = 0.5346
Batch 2040: Loss = 0.5962
Batch 2100: Loss = 0.6373
Batch 2160: Loss = 0.6216
Batch 2220: Loss = 0.7294
  Train Loss: 0.6244 | Train Acc: 77.63% | Val Loss: 1.2506 | Val Acc: 59.43%
Epoch 9/10 LR 0.0001 and batch_size 40
Batch 60: Loss = 0.4113
Batch 120: Loss = 0.3455
Batch 180: Loss = 0.3423
Batch 240: Loss = 0.3862
Batch 300: Loss = 0.4344
Batch 360: Loss = 0.2868
Batch 420: Loss = 0.6108
Batch 480: Loss = 0.6664
Batch 540: Loss = 0.5460
Batch 600: Loss = 0.2089
Batch 660: Loss = 0.3777
Batch 720: Loss = 0.6361
Batch 780: Loss = 0.4595
Batch 840: Loss = 0.5419
Batch 900: Loss = 0.6353
Batch 960: Loss = 0.5534
Batch 1020: Loss = 0.5091
Batch 1080: Loss = 0.4604
Batch 1140: Loss = 0.4004
Batch 1200: Loss = 0.5991
Batch 1260: Loss = 0.6005
Batch 1320: Loss = 0.4966
Batch 1380: Loss = 0.3758
Batch 1440: Loss = 0.7213
Batch 1500: Loss = 0.6092
Batch 1560: Loss = 0.5017
Batch 1620: Loss = 0.7139
Batch 1680: Loss = 0.4476
Batch 1740: Loss = 0.6586
Batch 1800: Loss = 0.4987
Batch 1860: Loss = 0.4332
Batch 1920: Loss = 0.4510
Batch 1980: Loss = 0.6046
Batch 2040: Loss = 0.5702
Batch 2100: Loss = 0.3763
Batch 2160: Loss = 0.3131
Batch 2220: Loss = 0.4738
  Train Loss: 0.5202 | Train Acc: 81.33% | Val Loss: 1.3193 | Val Acc: 59.84%
Epoch 10/10 LR 0.0001 and batch_size 40
Batch 60: Loss = 0.3689
Batch 120: Loss = 0.2558
Batch 180: Loss = 0.2248
Batch 240: Loss = 0.2092
Batch 300: Loss = 0.4027
Batch 360: Loss = 0.4313
Batch 420: Loss = 0.3869
Batch 480: Loss = 0.3657
Batch 540: Loss = 0.8538
Batch 600: Loss = 0.5238
Batch 660: Loss = 0.4414
Batch 720: Loss = 0.2143
Batch 780: Loss = 0.6848
Batch 840: Loss = 0.2855
Batch 900: Loss = 0.4838
Batch 960: Loss = 0.4310
Batch 1020: Loss = 0.3375
Batch 1080: Loss = 0.3200
Batch 1140: Loss = 0.3546
Batch 1200: Loss = 0.6500
Batch 1260: Loss = 0.4476
Batch 1320: Loss = 0.5205
Batch 1380: Loss = 0.4348
Batch 1440: Loss = 0.6842
Batch 1500: Loss = 0.4438
Batch 1560: Loss = 0.5321
Batch 1620: Loss = 0.3176
Batch 1680: Loss = 0.4264
Batch 1740: Loss = 0.4160
Batch 1800: Loss = 0.7610
Batch 1860: Loss = 0.4708
Batch 1920: Loss = 0.3581
Batch 1980: Loss = 0.5400
Batch 2040: Loss = 0.3303
Batch 2100: Loss = 0.2941
Batch 2160: Loss = 0.4656
Batch 2220: Loss = 0.5524
  Train Loss: 0.4342 | Train Acc: 84.71% | Val Loss: 1.4997 | Val Acc: 57.77%
Final Test Loss: 1.3309, Test Acc: 59.62%
Epoch 1/10 LR 0.0001 and batch_size 60
Batch 60: Loss = 1.9741
Batch 120: Loss = 1.8845
Batch 180: Loss = 1.9826
Batch 240: Loss = 1.7042
Batch 300: Loss = 1.8634
Batch 360: Loss = 1.6895
Batch 420: Loss = 1.7664
Batch 480: Loss = 1.6117
Batch 540: Loss = 1.5791
Batch 600: Loss = 1.6101
Batch 660: Loss = 1.7121
Batch 720: Loss = 1.5972
Batch 780: Loss = 1.5804
Batch 840: Loss = 1.4020
Batch 900: Loss = 1.5800
Batch 960: Loss = 1.4628
Batch 1020: Loss = 1.5480
Batch 1080: Loss = 1.4908
Batch 1140: Loss = 1.3461
Batch 1200: Loss = 1.2570
Batch 1260: Loss = 1.4784
Batch 1320: Loss = 1.3088
Batch 1380: Loss = 1.5658
Batch 1440: Loss = 1.6166
Batch 1500: Loss = 1.5605
  Train Loss: 1.6104 | Train Acc: 40.94% | Val Loss: 1.4409 | Val Acc: 47.54%
Epoch 2/10 LR 0.0001 and batch_size 60
Batch 60: Loss = 1.3463
Batch 120: Loss = 1.2761
Batch 180: Loss = 1.2140
Batch 240: Loss = 1.3276
Batch 300: Loss = 1.1651
Batch 360: Loss = 1.6976
Batch 420: Loss = 1.3396
Batch 480: Loss = 1.4455
Batch 540: Loss = 1.4216
Batch 600: Loss = 1.4033
Batch 660: Loss = 1.3152
Batch 720: Loss = 1.5487
Batch 780: Loss = 1.3044
Batch 840: Loss = 1.1653
Batch 900: Loss = 1.3534
Batch 960: Loss = 1.2631
Batch 1020: Loss = 1.1360
Batch 1080: Loss = 1.3842
Batch 1140: Loss = 1.1289
Batch 1200: Loss = 1.4044
Batch 1260: Loss = 1.1779
Batch 1320: Loss = 1.3287
Batch 1380: Loss = 1.3464
Batch 1440: Loss = 1.2745
Batch 1500: Loss = 1.2489
  Train Loss: 1.3285 | Train Acc: 51.70% | Val Loss: 1.3501 | Val Acc: 50.95%
Epoch 3/10 LR 0.0001 and batch_size 60
Batch 60: Loss = 1.2625
Batch 120: Loss = 1.0584
Batch 180: Loss = 1.0555
Batch 240: Loss = 1.1639
Batch 300: Loss = 1.3069
Batch 360: Loss = 1.0479
Batch 420: Loss = 1.2153
Batch 480: Loss = 1.2899
Batch 540: Loss = 1.0884
Batch 600: Loss = 1.0821
Batch 660: Loss = 1.2112
Batch 720: Loss = 1.2301
Batch 780: Loss = 0.9371
Batch 840: Loss = 1.2698
Batch 900: Loss = 0.8632
Batch 960: Loss = 0.9220
Batch 1020: Loss = 1.0756
Batch 1080: Loss = 1.0520
Batch 1140: Loss = 1.3510
Batch 1200: Loss = 1.2742
Batch 1260: Loss = 1.1276
Batch 1320: Loss = 0.9674
Batch 1380: Loss = 0.9186
Batch 1440: Loss = 1.3066
Batch 1500: Loss = 1.0086
  Train Loss: 1.1820 | Train Acc: 57.20% | Val Loss: 1.2436 | Val Acc: 55.10%
Epoch 4/10 LR 0.0001 and batch_size 60
Batch 60: Loss = 1.0390
Batch 120: Loss = 0.8315
Batch 180: Loss = 0.8816
Batch 240: Loss = 0.9780
Batch 300: Loss = 0.9268
Batch 360: Loss = 0.9429
Batch 420: Loss = 0.9742
Batch 480: Loss = 0.9673
Batch 540: Loss = 1.1465
Batch 600: Loss = 1.3169
Batch 660: Loss = 0.9306
Batch 720: Loss = 0.7862
Batch 780: Loss = 0.9305
Batch 840: Loss = 1.0429
Batch 900: Loss = 1.1502
Batch 960: Loss = 0.9788
Batch 1020: Loss = 0.8721
Batch 1080: Loss = 0.9606
Batch 1140: Loss = 1.1504
Batch 1200: Loss = 0.9349
Batch 1260: Loss = 1.0197
Batch 1320: Loss = 0.8310
Batch 1380: Loss = 0.9937
Batch 1440: Loss = 1.3028
Batch 1500: Loss = 1.0337
  Train Loss: 1.0594 | Train Acc: 61.82% | Val Loss: 1.2068 | Val Acc: 56.85%
Epoch 5/10 LR 0.0001 and batch_size 60
Batch 60: Loss = 0.8097
Batch 120: Loss = 0.6374
Batch 180: Loss = 0.9382
Batch 240: Loss = 0.8363
Batch 300: Loss = 0.7399
Batch 360: Loss = 0.9845
Batch 420: Loss = 0.8434
Batch 480: Loss = 0.8850
Batch 540: Loss = 0.8079
Batch 600: Loss = 0.8423
Batch 660: Loss = 0.9169
Batch 720: Loss = 1.0057
Batch 780: Loss = 1.1220
Batch 840: Loss = 1.0295
Batch 900: Loss = 0.9736
Batch 960: Loss = 0.9692
Batch 1020: Loss = 1.0621
Batch 1080: Loss = 0.9377
Batch 1140: Loss = 1.1061
Batch 1200: Loss = 0.9388
Batch 1260: Loss = 0.9980
Batch 1320: Loss = 0.8739
Batch 1380: Loss = 0.8530
Batch 1440: Loss = 0.8937
Batch 1500: Loss = 1.0984
  Train Loss: 0.9418 | Train Acc: 66.20% | Val Loss: 1.1938 | Val Acc: 58.09%
Epoch 6/10 LR 0.0001 and batch_size 60
Batch 60: Loss = 0.6320
Batch 120: Loss = 0.7178
Batch 180: Loss = 0.5958
Batch 240: Loss = 0.7679
Batch 300: Loss = 1.0667
Batch 360: Loss = 0.9231
Batch 420: Loss = 0.8813
Batch 480: Loss = 0.8631
Batch 540: Loss = 0.6637
Batch 600: Loss = 0.8339
Batch 660: Loss = 0.7961
Batch 720: Loss = 0.7165
Batch 780: Loss = 0.6788
Batch 840: Loss = 0.8275
Batch 900: Loss = 0.8156
Batch 960: Loss = 0.6789
Batch 1020: Loss = 0.7636
Batch 1080: Loss = 0.7595
Batch 1140: Loss = 0.8579
Batch 1200: Loss = 0.8143
Batch 1260: Loss = 0.7412
Batch 1320: Loss = 0.7760
Batch 1380: Loss = 0.9420
Batch 1440: Loss = 1.0442
Batch 1500: Loss = 0.8347
  Train Loss: 0.8200 | Train Acc: 70.66% | Val Loss: 1.3396 | Val Acc: 54.96%
Epoch 7/10 LR 0.0001 and batch_size 60
Batch 60: Loss = 0.5937
Batch 120: Loss = 0.4852
Batch 180: Loss = 0.6110
Batch 240: Loss = 0.6143
Batch 300: Loss = 0.6345
Batch 360: Loss = 0.5260
Batch 420: Loss = 0.7030
Batch 480: Loss = 0.5774
Batch 540: Loss = 0.7872
Batch 600: Loss = 0.6281
Batch 660: Loss = 0.6236
Batch 720: Loss = 0.7628
Batch 780: Loss = 1.0430
Batch 840: Loss = 0.7089
Batch 900: Loss = 0.6446
Batch 960: Loss = 0.6805
Batch 1020: Loss = 0.6246
Batch 1080: Loss = 0.6737
Batch 1140: Loss = 0.7773
Batch 1200: Loss = 0.5886
Batch 1260: Loss = 0.8509
Batch 1320: Loss = 0.8675
Batch 1380: Loss = 0.5940
Batch 1440: Loss = 0.6652
Batch 1500: Loss = 0.6448
  Train Loss: 0.6961 | Train Acc: 75.19% | Val Loss: 1.2989 | Val Acc: 57.68%
Epoch 8/10 LR 0.0001 and batch_size 60
Batch 60: Loss = 0.4556
Batch 120: Loss = 0.3629
Batch 180: Loss = 0.4524
Batch 240: Loss = 0.3778
Batch 300: Loss = 0.5078
Batch 360: Loss = 0.8068
Batch 420: Loss = 0.5066
Batch 480: Loss = 0.6119
Batch 540: Loss = 0.5266
Batch 600: Loss = 0.5534
Batch 660: Loss = 0.5372
Batch 720: Loss = 0.6915
Batch 780: Loss = 0.6325
Batch 840: Loss = 0.6312
Batch 900: Loss = 0.6290
Batch 960: Loss = 0.6108
Batch 1020: Loss = 0.5553
Batch 1080: Loss = 0.6076
Batch 1140: Loss = 0.5222
Batch 1200: Loss = 0.5774
Batch 1260: Loss = 0.6471
Batch 1320: Loss = 0.4934
Batch 1380: Loss = 0.6868
Batch 1440: Loss = 0.8568
Batch 1500: Loss = 0.5283
  Train Loss: 0.5735 | Train Acc: 79.64% | Val Loss: 1.4095 | Val Acc: 57.54%
Epoch 9/10 LR 0.0001 and batch_size 60
Batch 60: Loss = 0.2334
Batch 120: Loss = 0.3670
Batch 180: Loss = 0.3072
Batch 240: Loss = 0.3796
Batch 300: Loss = 0.5659
Batch 360: Loss = 0.5891
Batch 420: Loss = 0.4339
Batch 480: Loss = 0.6033
Batch 540: Loss = 0.4740
Batch 600: Loss = 0.4242
Batch 660: Loss = 0.5063
Batch 720: Loss = 0.3425
Batch 780: Loss = 0.8764
Batch 840: Loss = 0.6583
Batch 900: Loss = 0.3058
Batch 960: Loss = 0.5045
Batch 1020: Loss = 0.5438
Batch 1080: Loss = 0.4832
Batch 1140: Loss = 0.4347
Batch 1200: Loss = 0.2882
Batch 1260: Loss = 0.5758
Batch 1320: Loss = 0.5005
Batch 1380: Loss = 0.4336
Batch 1440: Loss = 0.3955
Batch 1500: Loss = 0.2983
  Train Loss: 0.4685 | Train Acc: 83.16% | Val Loss: 1.5673 | Val Acc: 56.13%
Epoch 10/10 LR 0.0001 and batch_size 60
Batch 60: Loss = 0.2401
Batch 120: Loss = 0.2890
Batch 180: Loss = 0.2289
Batch 240: Loss = 0.2998
Batch 300: Loss = 0.4605
Batch 360: Loss = 0.3166
Batch 420: Loss = 0.6910
Batch 480: Loss = 0.3091
Batch 540: Loss = 0.4191
Batch 600: Loss = 0.4406
Batch 660: Loss = 0.6905
Batch 720: Loss = 0.3688
Batch 780: Loss = 0.3292
Batch 840: Loss = 0.3434
Batch 900: Loss = 0.5117
Batch 960: Loss = 0.3220
Batch 1020: Loss = 0.3842
Batch 1080: Loss = 0.3796
Batch 1140: Loss = 0.3390
Batch 1200: Loss = 0.2690
Batch 1260: Loss = 0.5861
Batch 1320: Loss = 0.2393
Batch 1380: Loss = 0.4519
Batch 1440: Loss = 0.5581
Batch 1500: Loss = 0.5060
  Train Loss: 0.3831 | Train Acc: 86.44% | Val Loss: 1.5898 | Val Acc: 57.22%
Final Test Loss: 1.2000, Test Acc: 58.02%
Epoch 1/10 LR 0.0001 and batch_size 80
Batch 60: Loss = 1.7858
Batch 120: Loss = 1.7508
Batch 180: Loss = 1.7824
Batch 240: Loss = 1.9442
Batch 300: Loss = 1.8542
Batch 360: Loss = 1.6614
Batch 420: Loss = 1.6479
Batch 480: Loss = 1.6072
Batch 540: Loss = 1.5769
Batch 600: Loss = 1.2532
Batch 660: Loss = 1.5987
Batch 720: Loss = 1.5044
Batch 780: Loss = 1.5077
Batch 840: Loss = 1.6652
Batch 900: Loss = 1.5065
Batch 960: Loss = 1.1830
Batch 1020: Loss = 1.3856
Batch 1080: Loss = 1.4381
  Train Loss: 1.6088 | Train Acc: 40.96% | Val Loss: 1.4367 | Val Acc: 47.50%
Epoch 2/10 LR 0.0001 and batch_size 80
Batch 60: Loss = 1.3094
Batch 120: Loss = 1.3514
Batch 180: Loss = 1.1389
Batch 240: Loss = 1.4455
Batch 300: Loss = 1.3363
Batch 360: Loss = 1.3002
Batch 420: Loss = 1.4146
Batch 480: Loss = 1.4003
Batch 540: Loss = 1.4301
Batch 600: Loss = 1.2515
Batch 660: Loss = 1.2995
Batch 720: Loss = 1.4382
Batch 780: Loss = 1.1776
Batch 840: Loss = 1.4920
Batch 900: Loss = 1.4239
Batch 960: Loss = 1.2598
Batch 1020: Loss = 1.4000
Batch 1080: Loss = 1.3387
  Train Loss: 1.3293 | Train Acc: 51.50% | Val Loss: 1.4071 | Val Acc: 49.01%
Epoch 3/10 LR 0.0001 and batch_size 80
Batch 60: Loss = 1.0354
Batch 120: Loss = 1.1529
Batch 180: Loss = 1.1251
Batch 240: Loss = 0.9535
Batch 300: Loss = 1.2629
Batch 360: Loss = 1.0112
Batch 420: Loss = 1.2671
Batch 480: Loss = 1.1490
Batch 540: Loss = 1.0780
Batch 600: Loss = 1.1884
Batch 660: Loss = 1.0158
Batch 720: Loss = 1.1100
Batch 780: Loss = 1.3769
Batch 840: Loss = 1.2432
Batch 900: Loss = 1.2219
Batch 960: Loss = 1.1586
Batch 1020: Loss = 1.2698
Batch 1080: Loss = 1.1011
  Train Loss: 1.1845 | Train Acc: 57.05% | Val Loss: 1.2872 | Val Acc: 53.97%
Epoch 4/10 LR 0.0001 and batch_size 80
Batch 60: Loss = 1.0063
Batch 120: Loss = 0.9663
Batch 180: Loss = 1.0357
Batch 240: Loss = 1.1693
Batch 300: Loss = 1.0265
Batch 360: Loss = 0.9085
Batch 420: Loss = 0.9697
Batch 480: Loss = 1.0730
Batch 540: Loss = 1.1047
Batch 600: Loss = 1.1068
Batch 660: Loss = 1.2178
Batch 720: Loss = 0.9681
Batch 780: Loss = 1.0502
Batch 840: Loss = 0.9710
Batch 900: Loss = 1.1467
Batch 960: Loss = 1.1255
Batch 1020: Loss = 1.1611
Batch 1080: Loss = 0.8849
  Train Loss: 1.0604 | Train Acc: 61.61% | Val Loss: 1.2646 | Val Acc: 54.91%
Epoch 5/10 LR 0.0001 and batch_size 80
Batch 60: Loss = 0.8269
Batch 120: Loss = 0.8870
Batch 180: Loss = 0.9151
Batch 240: Loss = 0.8959
Batch 300: Loss = 0.8892
Batch 360: Loss = 0.8360
Batch 420: Loss = 0.7254
Batch 480: Loss = 1.0848
Batch 540: Loss = 1.0311
Batch 600: Loss = 1.0547
Batch 660: Loss = 0.8760
Batch 720: Loss = 1.1495
Batch 780: Loss = 0.9472
Batch 840: Loss = 0.9518
Batch 900: Loss = 0.8402
Batch 960: Loss = 1.0013
Batch 1020: Loss = 0.9465
Batch 1080: Loss = 0.8462
  Train Loss: 0.9345 | Train Acc: 66.52% | Val Loss: 1.2230 | Val Acc: 56.76%
Epoch 6/10 LR 0.0001 and batch_size 80
Batch 60: Loss = 0.6603
Batch 120: Loss = 0.7471
Batch 180: Loss = 0.7553
Batch 240: Loss = 0.7614
Batch 300: Loss = 0.8253
Batch 360: Loss = 0.6652
Batch 420: Loss = 1.0949
Batch 480: Loss = 0.7460
Batch 540: Loss = 0.8117
Batch 600: Loss = 0.6671
Batch 660: Loss = 0.8536
Batch 720: Loss = 0.9643
Batch 780: Loss = 0.8062
Batch 840: Loss = 0.9722
Batch 900: Loss = 0.7864
Batch 960: Loss = 0.9232
Batch 1020: Loss = 1.0548
Batch 1080: Loss = 0.9882
  Train Loss: 0.8024 | Train Acc: 71.12% | Val Loss: 1.2872 | Val Acc: 56.73%
Epoch 7/10 LR 0.0001 and batch_size 80
Batch 60: Loss = 0.5735
Batch 120: Loss = 0.6307
Batch 180: Loss = 0.6190
Batch 240: Loss = 0.5743
Batch 300: Loss = 0.4496
Batch 360: Loss = 0.6933
Batch 420: Loss = 0.4254
Batch 480: Loss = 0.5923
Batch 540: Loss = 0.6315
Batch 600: Loss = 0.6280
Batch 660: Loss = 0.6698
Batch 720: Loss = 0.5447
Batch 780: Loss = 0.9302
Batch 840: Loss = 0.6411
Batch 900: Loss = 0.6645
Batch 960: Loss = 0.7228
Batch 1020: Loss = 0.7805
Batch 1080: Loss = 0.9345
  Train Loss: 0.6712 | Train Acc: 75.90% | Val Loss: 1.3324 | Val Acc: 56.38%
Epoch 8/10 LR 0.0001 and batch_size 80
Batch 60: Loss = 0.3910
Batch 120: Loss = 0.3774
Batch 180: Loss = 0.3937
Batch 240: Loss = 0.5084
Batch 300: Loss = 0.5766
Batch 360: Loss = 0.4856
Batch 420: Loss = 0.5926
Batch 480: Loss = 0.5175
Batch 540: Loss = 0.5952
Batch 600: Loss = 0.4413
Batch 660: Loss = 0.5146
Batch 720: Loss = 0.6090
Batch 780: Loss = 0.7351
Batch 840: Loss = 0.5228
Batch 900: Loss = 0.7302
Batch 960: Loss = 0.6140
Batch 1020: Loss = 0.6598
Batch 1080: Loss = 0.7536
  Train Loss: 0.5453 | Train Acc: 80.42% | Val Loss: 1.5928 | Val Acc: 54.35%
Epoch 9/10 LR 0.0001 and batch_size 80
Batch 60: Loss = 0.4179
Batch 120: Loss = 0.2881
Batch 180: Loss = 0.3794
Batch 240: Loss = 0.2859
Batch 300: Loss = 0.3942
Batch 360: Loss = 0.3305
Batch 420: Loss = 0.3785
Batch 480: Loss = 0.2489
Batch 540: Loss = 0.5476
Batch 600: Loss = 0.5411
Batch 660: Loss = 0.3598
Batch 720: Loss = 0.4510
Batch 780: Loss = 0.5230
Batch 840: Loss = 0.5482
Batch 900: Loss = 0.7171
Batch 960: Loss = 0.3060
Batch 1020: Loss = 0.6439
Batch 1080: Loss = 0.4223
  Train Loss: 0.4407 | Train Acc: 84.26% | Val Loss: 1.5523 | Val Acc: 55.89%
Epoch 10/10 LR 0.0001 and batch_size 80
Batch 60: Loss = 0.3058
Batch 120: Loss = 0.2198
Batch 180: Loss = 0.2401
Batch 240: Loss = 0.3793
Batch 300: Loss = 0.2292
Batch 360: Loss = 0.4039
Batch 420: Loss = 0.3863
Batch 480: Loss = 0.2847
Batch 540: Loss = 0.3270
Batch 600: Loss = 0.3551
Batch 660: Loss = 0.4514
Batch 720: Loss = 0.4087
Batch 780: Loss = 0.3147
Batch 840: Loss = 0.2873
Batch 900: Loss = 0.2410
Batch 960: Loss = 0.5140
Batch 1020: Loss = 0.4350
Batch 1080: Loss = 0.5900
  Train Loss: 0.3492 | Train Acc: 87.60% | Val Loss: 1.7132 | Val Acc: 55.76%
Final Test Loss: 1.2335, Test Acc: 56.70%
Epoch 1/10 LR 0.0001 and batch_size 100
Batch 60: Loss = 1.9530
Batch 120: Loss = 1.6275
Batch 180: Loss = 1.6537
Batch 240: Loss = 1.6754
Batch 300: Loss = 1.7777
Batch 360: Loss = 1.5961
Batch 420: Loss = 1.5495
Batch 480: Loss = 1.5448
Batch 540: Loss = 1.4509
Batch 600: Loss = 1.5171
Batch 660: Loss = 1.4486
Batch 720: Loss = 1.6392
Batch 780: Loss = 1.5862
Batch 840: Loss = 1.4562
Batch 900: Loss = 1.3059
  Train Loss: 1.6015 | Train Acc: 41.32% | Val Loss: 1.4525 | Val Acc: 46.56%
Epoch 2/10 LR 0.0001 and batch_size 100
Batch 60: Loss = 1.3683
Batch 120: Loss = 1.3966
Batch 180: Loss = 1.2915
Batch 240: Loss = 1.2261
Batch 300: Loss = 1.2639
Batch 360: Loss = 1.2668
Batch 420: Loss = 1.2324
Batch 480: Loss = 1.1840
Batch 540: Loss = 1.3819
Batch 600: Loss = 1.3375
Batch 660: Loss = 1.3530
Batch 720: Loss = 1.2404
Batch 780: Loss = 1.2058
Batch 840: Loss = 1.3503
Batch 900: Loss = 1.4143
  Train Loss: 1.3245 | Train Acc: 51.61% | Val Loss: 1.3427 | Val Acc: 51.29%
Epoch 3/10 LR 0.0001 and batch_size 100
Batch 60: Loss = 1.2059
Batch 120: Loss = 1.1613
Batch 180: Loss = 0.9532
Batch 240: Loss = 1.0462
Batch 300: Loss = 1.0508
Batch 360: Loss = 1.0885
Batch 420: Loss = 1.0366
Batch 480: Loss = 1.0828
Batch 540: Loss = 1.3551
Batch 600: Loss = 1.3053
Batch 660: Loss = 1.1921
Batch 720: Loss = 1.1713
Batch 780: Loss = 1.3979
Batch 840: Loss = 1.2299
Batch 900: Loss = 1.1940
  Train Loss: 1.1784 | Train Acc: 57.42% | Val Loss: 1.3075 | Val Acc: 52.99%
Epoch 4/10 LR 0.0001 and batch_size 100
Batch 60: Loss = 1.0288
Batch 120: Loss = 0.9926
Batch 180: Loss = 0.9088
Batch 240: Loss = 1.3060
Batch 300: Loss = 0.8702
Batch 360: Loss = 0.9141
Batch 420: Loss = 1.1111
Batch 480: Loss = 1.1242
Batch 540: Loss = 0.8560
Batch 600: Loss = 0.8809
Batch 660: Loss = 1.1544
Batch 720: Loss = 1.1777
Batch 780: Loss = 1.2556
Batch 840: Loss = 1.1954
Batch 900: Loss = 0.9519
  Train Loss: 1.0536 | Train Acc: 62.08% | Val Loss: 1.3077 | Val Acc: 53.24%
Epoch 5/10 LR 0.0001 and batch_size 100
Batch 60: Loss = 0.7766
Batch 120: Loss = 0.8656
Batch 180: Loss = 0.9200
Batch 240: Loss = 0.8046
Batch 300: Loss = 0.9626
Batch 360: Loss = 0.7939
Batch 420: Loss = 0.9116
Batch 480: Loss = 0.9326
Batch 540: Loss = 0.8406
Batch 600: Loss = 1.0160
Batch 660: Loss = 0.9002
Batch 720: Loss = 0.9446
Batch 780: Loss = 0.8752
Batch 840: Loss = 0.9303
Batch 900: Loss = 0.9881
  Train Loss: 0.9268 | Train Acc: 66.79% | Val Loss: 1.3590 | Val Acc: 53.79%
Epoch 6/10 LR 0.0001 and batch_size 100
Batch 60: Loss = 0.4875
Batch 120: Loss = 0.7671
Batch 180: Loss = 0.6886
Batch 240: Loss = 0.7616
Batch 300: Loss = 0.8159
Batch 360: Loss = 0.8897
Batch 420: Loss = 0.8114
Batch 480: Loss = 0.9726
Batch 540: Loss = 0.7857
Batch 600: Loss = 0.7722
Batch 660: Loss = 0.7411
Batch 720: Loss = 0.8469
Batch 780: Loss = 0.8357
Batch 840: Loss = 0.8954
Batch 900: Loss = 0.7140
  Train Loss: 0.7930 | Train Acc: 71.71% | Val Loss: 1.3203 | Val Acc: 55.71%
Epoch 7/10 LR 0.0001 and batch_size 100
Batch 60: Loss = 0.4942
Batch 120: Loss = 0.4486
Batch 180: Loss = 0.7479
Batch 240: Loss = 0.6053
Batch 300: Loss = 0.5781
Batch 360: Loss = 0.6186
Batch 420: Loss = 0.6863
Batch 480: Loss = 0.7738
Batch 540: Loss = 0.6718
Batch 600: Loss = 0.6586
Batch 660: Loss = 0.8295
Batch 720: Loss = 0.7459
Batch 780: Loss = 0.6502
Batch 840: Loss = 0.7221
Batch 900: Loss = 0.7381
  Train Loss: 0.6553 | Train Acc: 76.62% | Val Loss: 1.4167 | Val Acc: 55.39%
Epoch 8/10 LR 0.0001 and batch_size 100
Batch 60: Loss = 0.5337
Batch 120: Loss = 0.3682
Batch 180: Loss = 0.3128
Batch 240: Loss = 0.3497
Batch 300: Loss = 0.5122
Batch 360: Loss = 0.3938
Batch 420: Loss = 0.6013
Batch 480: Loss = 0.4437
Batch 540: Loss = 0.4948
Batch 600: Loss = 0.5242
Batch 660: Loss = 0.6029
Batch 720: Loss = 0.5767
Batch 780: Loss = 0.5354
Batch 840: Loss = 0.7780
Batch 900: Loss = 0.4302
  Train Loss: 0.5261 | Train Acc: 81.27% | Val Loss: 1.5354 | Val Acc: 54.83%
Epoch 9/10 LR 0.0001 and batch_size 100
Batch 60: Loss = 0.2869
Batch 120: Loss = 0.4767
Batch 180: Loss = 0.4423
Batch 240: Loss = 0.2971
Batch 300: Loss = 0.4243
Batch 360: Loss = 0.5040
Batch 420: Loss = 0.4394
Batch 480: Loss = 0.5251
Batch 540: Loss = 0.4568
Batch 600: Loss = 0.3917
Batch 660: Loss = 0.3122
Batch 720: Loss = 0.4670
Batch 780: Loss = 0.5019
Batch 840: Loss = 0.4480
Batch 900: Loss = 0.4760
  Train Loss: 0.4162 | Train Acc: 85.21% | Val Loss: 1.6875 | Val Acc: 54.83%
Epoch 10/10 LR 0.0001 and batch_size 100
Batch 60: Loss = 0.1388
Batch 120: Loss = 0.4656
Batch 180: Loss = 0.3540
Batch 240: Loss = 0.3859
Batch 300: Loss = 0.4956
Batch 360: Loss = 0.2984
Batch 420: Loss = 0.2882
Batch 480: Loss = 0.2978
Batch 540: Loss = 0.5506
Batch 600: Loss = 0.3274
Batch 660: Loss = 0.3908
Batch 720: Loss = 0.3611
Batch 780: Loss = 0.3260
Batch 840: Loss = 0.2823
Batch 900: Loss = 0.2648
  Train Loss: 0.3293 | Train Acc: 88.28% | Val Loss: 1.7636 | Val Acc: 55.08%
Final Test Loss: 1.3319, Test Acc: 55.62%
Epoch 1/10 LR 0.001 and batch_size 40
Batch 60: Loss = 1.8674
Batch 120: Loss = 1.8246
Batch 180: Loss = 1.6781
Batch 240: Loss = 1.8824
Batch 300: Loss = 1.7874
Batch 360: Loss = 1.3529
Batch 420: Loss = 1.6115
Batch 480: Loss = 1.9217
Batch 540: Loss = 1.6346
Batch 600: Loss = 1.5438
Batch 660: Loss = 1.5833
Batch 720: Loss = 1.6912
Batch 780: Loss = 1.6305
Batch 840: Loss = 1.5599
Batch 900: Loss = 1.4131
Batch 960: Loss = 1.6322
Batch 1020: Loss = 1.6080
Batch 1080: Loss = 1.4517
Batch 1140: Loss = 1.5040
Batch 1200: Loss = 1.6684
Batch 1260: Loss = 1.2947
Batch 1320: Loss = 1.5349
Batch 1380: Loss = 1.4132
Batch 1440: Loss = 1.5083
Batch 1500: Loss = 1.3782
Batch 1560: Loss = 1.3857
Batch 1620: Loss = 1.4548
Batch 1680: Loss = 1.4127
Batch 1740: Loss = 1.3942
Batch 1800: Loss = 1.5674
Batch 1860: Loss = 1.3307
Batch 1920: Loss = 1.3352
Batch 1980: Loss = 1.4118
Batch 2040: Loss = 1.4316
Batch 2100: Loss = 1.4586
Batch 2160: Loss = 1.3834
Batch 2220: Loss = 1.4411
  Train Loss: 1.5771 | Train Acc: 42.47% | Val Loss: 1.5533 | Val Acc: 46.22%
Epoch 2/10 LR 0.001 and batch_size 40
Batch 60: Loss = 1.1771
Batch 120: Loss = 1.2895
Batch 180: Loss = 1.5791
Batch 240: Loss = 1.4741
Batch 300: Loss = 1.2374
Batch 360: Loss = 1.4088
Batch 420: Loss = 1.3373
Batch 480: Loss = 1.3399
Batch 540: Loss = 1.2967
Batch 600: Loss = 1.4506
Batch 660: Loss = 1.3794
Batch 720: Loss = 1.5805
Batch 780: Loss = 1.2351
Batch 840: Loss = 1.0736
Batch 900: Loss = 1.4120
Batch 960: Loss = 1.1485
Batch 1020: Loss = 1.0741
Batch 1080: Loss = 1.2939
Batch 1140: Loss = 1.0888
Batch 1200: Loss = 1.5319
Batch 1260: Loss = 1.3923
Batch 1320: Loss = 1.1548
Batch 1380: Loss = 1.2812
Batch 1440: Loss = 1.4087
Batch 1500: Loss = 1.5370
Batch 1560: Loss = 0.9428
Batch 1620: Loss = 1.2485
Batch 1680: Loss = 1.6562
Batch 1740: Loss = 1.4774
Batch 1800: Loss = 0.9845
Batch 1860: Loss = 1.2018
Batch 1920: Loss = 1.2163
Batch 1980: Loss = 1.0666
Batch 2040: Loss = 1.4030
Batch 2100: Loss = 1.4588
Batch 2160: Loss = 1.2896
Batch 2220: Loss = 1.2216
  Train Loss: 1.2889 | Train Acc: 53.65% | Val Loss: 1.3121 | Val Acc: 53.00%
Epoch 3/10 LR 0.001 and batch_size 40
Batch 60: Loss = 1.0491
Batch 120: Loss = 1.2792
Batch 180: Loss = 1.2938
Batch 240: Loss = 1.2689
Batch 300: Loss = 1.0488
Batch 360: Loss = 1.4299
Batch 420: Loss = 1.1359
Batch 480: Loss = 1.1135
Batch 540: Loss = 1.0095
Batch 600: Loss = 0.9371
Batch 660: Loss = 1.0886
Batch 720: Loss = 1.3153
Batch 780: Loss = 1.0777
Batch 840: Loss = 1.4676
Batch 900: Loss = 1.1877
Batch 960: Loss = 1.1673
Batch 1020: Loss = 1.2869
Batch 1080: Loss = 1.1433
Batch 1140: Loss = 1.0870
Batch 1200: Loss = 1.1488
Batch 1260: Loss = 0.9341
Batch 1320: Loss = 1.3317
Batch 1380: Loss = 0.9086
Batch 1440: Loss = 1.1535
Batch 1500: Loss = 1.0989
Batch 1560: Loss = 1.3152
Batch 1620: Loss = 1.2565
Batch 1680: Loss = 1.4371
Batch 1740: Loss = 1.5478
Batch 1800: Loss = 1.2576
Batch 1860: Loss = 1.0333
Batch 1920: Loss = 1.1038
Batch 1980: Loss = 1.0136
Batch 2040: Loss = 1.1939
Batch 2100: Loss = 0.9475
Batch 2160: Loss = 1.0370
Batch 2220: Loss = 1.1156
  Train Loss: 1.1527 | Train Acc: 58.83% | Val Loss: 1.1959 | Val Acc: 57.34%
Epoch 4/10 LR 0.001 and batch_size 40
Batch 60: Loss = 0.7901
Batch 120: Loss = 1.0595
Batch 180: Loss = 1.0194
Batch 240: Loss = 1.0996
Batch 300: Loss = 1.3419
Batch 360: Loss = 0.9185
Batch 420: Loss = 0.9259
Batch 480: Loss = 1.0863
Batch 540: Loss = 0.9464
Batch 600: Loss = 0.9666
Batch 660: Loss = 1.3030
Batch 720: Loss = 0.8180
Batch 780: Loss = 1.0255
Batch 840: Loss = 1.2557
Batch 900: Loss = 0.9417
Batch 960: Loss = 1.0816
Batch 1020: Loss = 1.1075
Batch 1080: Loss = 0.9619
Batch 1140: Loss = 0.9585
Batch 1200: Loss = 1.1202
Batch 1260: Loss = 1.0306
Batch 1320: Loss = 1.0138
Batch 1380: Loss = 0.8468
Batch 1440: Loss = 0.9809
Batch 1500: Loss = 1.1862
Batch 1560: Loss = 1.0865
Batch 1620: Loss = 1.1386
Batch 1680: Loss = 1.0867
Batch 1740: Loss = 1.3358
Batch 1800: Loss = 0.7474
Batch 1860: Loss = 0.8624
Batch 1920: Loss = 1.0102
Batch 1980: Loss = 1.1044
Batch 2040: Loss = 0.9129
Batch 2100: Loss = 0.9276
Batch 2160: Loss = 0.7160
Batch 2220: Loss = 0.9122
  Train Loss: 1.0431 | Train Acc: 62.94% | Val Loss: 1.0776 | Val Acc: 61.53%
Epoch 5/10 LR 0.001 and batch_size 40
Batch 60: Loss = 1.1066
Batch 120: Loss = 1.1472
Batch 180: Loss = 0.9071
Batch 240: Loss = 1.0748
Batch 300: Loss = 0.8791
Batch 360: Loss = 1.1712
Batch 420: Loss = 0.9971
Batch 480: Loss = 0.8586
Batch 540: Loss = 1.0921
Batch 600: Loss = 1.0532
Batch 660: Loss = 0.9699
Batch 720: Loss = 0.9988
Batch 780: Loss = 0.8415
Batch 840: Loss = 0.6777
Batch 900: Loss = 1.1072
Batch 960: Loss = 0.7636
Batch 1020: Loss = 0.9993
Batch 1080: Loss = 0.9240
Batch 1140: Loss = 1.0493
Batch 1200: Loss = 0.8665
Batch 1260: Loss = 0.9592
Batch 1320: Loss = 0.8250
Batch 1380: Loss = 0.7445
Batch 1440: Loss = 0.8278
Batch 1500: Loss = 1.1351
Batch 1560: Loss = 1.0634
Batch 1620: Loss = 1.0020
Batch 1680: Loss = 0.9148
Batch 1740: Loss = 1.2259
Batch 1800: Loss = 0.6537
Batch 1860: Loss = 0.7829
Batch 1920: Loss = 1.0286
Batch 1980: Loss = 1.1039
Batch 2040: Loss = 1.1406
Batch 2100: Loss = 1.1623
Batch 2160: Loss = 0.9488
Batch 2220: Loss = 0.6722
  Train Loss: 0.9470 | Train Acc: 66.37% | Val Loss: 1.1622 | Val Acc: 59.55%
Epoch 6/10 LR 0.001 and batch_size 40
Batch 60: Loss = 0.5609
Batch 120: Loss = 0.5994
Batch 180: Loss = 0.5936
Batch 240: Loss = 0.7793
Batch 300: Loss = 0.8719
Batch 360: Loss = 1.0516
Batch 420: Loss = 1.1529
Batch 480: Loss = 0.7090
Batch 540: Loss = 0.9480
Batch 600: Loss = 0.9690
Batch 660: Loss = 0.7581
Batch 720: Loss = 0.9606
Batch 780: Loss = 0.8837
Batch 840: Loss = 1.0415
Batch 900: Loss = 0.9011
Batch 960: Loss = 0.8626
Batch 1020: Loss = 0.6394
Batch 1080: Loss = 0.6255
Batch 1140: Loss = 0.8266
Batch 1200: Loss = 0.6035
Batch 1260: Loss = 0.5115
Batch 1320: Loss = 0.7729
Batch 1380: Loss = 1.2347
Batch 1440: Loss = 0.9393
Batch 1500: Loss = 0.7925
Batch 1560: Loss = 1.0058
Batch 1620: Loss = 1.1558
Batch 1680: Loss = 0.9809
Batch 1740: Loss = 0.7745
Batch 1800: Loss = 1.1171
Batch 1860: Loss = 1.2272
Batch 1920: Loss = 0.8378
Batch 1980: Loss = 1.1193
Batch 2040: Loss = 0.7493
Batch 2100: Loss = 0.8885
Batch 2160: Loss = 0.6576
Batch 2220: Loss = 0.8485
  Train Loss: 0.8552 | Train Acc: 69.69% | Val Loss: 1.0395 | Val Acc: 63.15%
Epoch 7/10 LR 0.001 and batch_size 40
Batch 60: Loss = 0.8558
Batch 120: Loss = 0.9585
Batch 180: Loss = 0.8200
Batch 240: Loss = 0.6757
Batch 300: Loss = 0.6410
Batch 360: Loss = 0.6782
Batch 420: Loss = 0.6702
Batch 480: Loss = 0.7357
Batch 540: Loss = 0.5748
Batch 600: Loss = 0.8394
Batch 660: Loss = 0.6553
Batch 720: Loss = 0.5736
Batch 780: Loss = 1.0784
Batch 840: Loss = 0.6183
Batch 900: Loss = 0.9735
Batch 960: Loss = 0.9375
Batch 1020: Loss = 0.6907
Batch 1080: Loss = 0.8067
Batch 1140: Loss = 0.7634
Batch 1200: Loss = 0.7985
Batch 1260: Loss = 0.9424
Batch 1320: Loss = 0.6104
Batch 1380: Loss = 0.7045
Batch 1440: Loss = 0.7639
Batch 1500: Loss = 0.9013
Batch 1560: Loss = 0.8572
Batch 1620: Loss = 0.9521
Batch 1680: Loss = 0.7726
Batch 1740: Loss = 0.5345
Batch 1800: Loss = 0.7496
Batch 1860: Loss = 0.4149
Batch 1920: Loss = 0.8554
Batch 1980: Loss = 0.7527
Batch 2040: Loss = 0.9030
Batch 2100: Loss = 1.0648
Batch 2160: Loss = 0.6465
Batch 2220: Loss = 0.6655
  Train Loss: 0.7645 | Train Acc: 72.83% | Val Loss: 1.1139 | Val Acc: 62.25%
Epoch 8/10 LR 0.001 and batch_size 40
Batch 60: Loss = 0.6994
Batch 120: Loss = 1.0025
Batch 180: Loss = 0.8207
Batch 240: Loss = 0.5792
Batch 300: Loss = 0.7947
Batch 360: Loss = 0.7391
Batch 420: Loss = 0.3547
Batch 480: Loss = 0.7485
Batch 540: Loss = 0.4958
Batch 600: Loss = 0.7490
Batch 660: Loss = 0.6624
Batch 720: Loss = 0.7429
Batch 780: Loss = 0.5028
Batch 840: Loss = 0.7838
Batch 900: Loss = 0.6001
Batch 960: Loss = 0.6502
Batch 1020: Loss = 0.5115
Batch 1080: Loss = 0.4428
Batch 1140: Loss = 0.5788
Batch 1200: Loss = 0.4687
Batch 1260: Loss = 0.6636
Batch 1320: Loss = 0.6802
Batch 1380: Loss = 0.3811
Batch 1440: Loss = 0.6282
Batch 1500: Loss = 0.8216
Batch 1560: Loss = 0.8824
Batch 1620: Loss = 0.7731
Batch 1680: Loss = 0.5329
Batch 1740: Loss = 0.5471
Batch 1800: Loss = 0.8056
Batch 1860: Loss = 0.5856
Batch 1920: Loss = 0.5949
Batch 1980: Loss = 0.7319
Batch 2040: Loss = 0.8095
Batch 2100: Loss = 0.6460
Batch 2160: Loss = 0.8828
Batch 2220: Loss = 0.5262
  Train Loss: 0.6683 | Train Acc: 76.22% | Val Loss: 1.0373 | Val Acc: 64.35%
Epoch 9/10 LR 0.001 and batch_size 40
Batch 60: Loss = 0.3710
Batch 120: Loss = 0.4789
Batch 180: Loss = 0.6242
Batch 240: Loss = 0.5816
Batch 300: Loss = 0.6020
Batch 360: Loss = 0.5220
Batch 420: Loss = 0.2966
Batch 480: Loss = 0.6582
Batch 540: Loss = 0.6975
Batch 600: Loss = 0.7106
Batch 660: Loss = 0.7028
Batch 720: Loss = 0.9381
Batch 780: Loss = 0.5339
Batch 840: Loss = 0.6665
Batch 900: Loss = 0.5520
Batch 960: Loss = 0.6450
Batch 1020: Loss = 0.6316
Batch 1080: Loss = 0.5934
Batch 1140: Loss = 0.4726
Batch 1200: Loss = 0.7019
Batch 1260: Loss = 0.3777
Batch 1320: Loss = 0.4683
Batch 1380: Loss = 0.5689
Batch 1440: Loss = 0.5327
Batch 1500: Loss = 0.5079
Batch 1560: Loss = 0.4915
Batch 1620: Loss = 0.6374
Batch 1680: Loss = 0.9849
Batch 1740: Loss = 0.6593
Batch 1800: Loss = 0.4721
Batch 1860: Loss = 0.7125
Batch 1920: Loss = 0.4396
Batch 1980: Loss = 0.4729
Batch 2040: Loss = 0.8058
Batch 2100: Loss = 0.5512
Batch 2160: Loss = 0.5672
Batch 2220: Loss = 0.4774
  Train Loss: 0.5753 | Train Acc: 79.53% | Val Loss: 1.1035 | Val Acc: 64.04%
Epoch 10/10 LR 0.001 and batch_size 40
Batch 60: Loss = 0.3053
Batch 120: Loss = 0.3672
Batch 180: Loss = 0.6371
Batch 240: Loss = 0.2990
Batch 300: Loss = 0.4289
Batch 360: Loss = 0.6506
Batch 420: Loss = 0.3559
Batch 480: Loss = 0.3766
Batch 540: Loss = 0.4099
Batch 600: Loss = 0.5484
Batch 660: Loss = 0.3068
Batch 720: Loss = 0.3235
Batch 780: Loss = 0.5034
Batch 840: Loss = 0.5649
Batch 900: Loss = 0.4930
Batch 960: Loss = 0.4325
Batch 1020: Loss = 0.6281
Batch 1080: Loss = 0.5425
Batch 1140: Loss = 0.3993
Batch 1200: Loss = 0.6774
Batch 1260: Loss = 0.3956
Batch 1320: Loss = 0.3527
Batch 1380: Loss = 0.5137
Batch 1440: Loss = 0.4956
Batch 1500: Loss = 0.5937
Batch 1560: Loss = 0.5563
Batch 1620: Loss = 0.3920
Batch 1680: Loss = 0.4758
Batch 1740: Loss = 0.4717
Batch 1800: Loss = 0.7926
Batch 1860: Loss = 0.3823
Batch 1920: Loss = 0.7258
Batch 1980: Loss = 0.4561
Batch 2040: Loss = 0.6105
Batch 2100: Loss = 0.3930
Batch 2160: Loss = 0.3093
Batch 2220: Loss = 0.4837
  Train Loss: 0.4788 | Train Acc: 82.89% | Val Loss: 1.1749 | Val Acc: 63.86%
Final Test Loss: 1.0471, Test Acc: 64.28%
Epoch 1/10 LR 0.001 and batch_size 60
Batch 60: Loss = 1.8821
Batch 120: Loss = 2.0041
Batch 180: Loss = 1.6568
Batch 240: Loss = 1.5488
Batch 300: Loss = 1.8234
Batch 360: Loss = 1.7125
Batch 420: Loss = 1.6160
Batch 480: Loss = 1.8463
Batch 540: Loss = 1.4442
Batch 600: Loss = 1.5906
Batch 660: Loss = 1.3348
Batch 720: Loss = 1.7811
Batch 780: Loss = 1.6134
Batch 840: Loss = 1.6958
Batch 900: Loss = 1.3467
Batch 960: Loss = 1.4240
Batch 1020: Loss = 1.5469
Batch 1080: Loss = 1.6236
Batch 1140: Loss = 1.3697
Batch 1200: Loss = 1.5363
Batch 1260: Loss = 1.4281
Batch 1320: Loss = 1.3533
Batch 1380: Loss = 1.5890
Batch 1440: Loss = 1.4396
Batch 1500: Loss = 1.3608
  Train Loss: 1.5512 | Train Acc: 43.41% | Val Loss: 1.5242 | Val Acc: 45.55%
Epoch 2/10 LR 0.001 and batch_size 60
Batch 60: Loss = 1.4927
Batch 120: Loss = 1.3948
Batch 180: Loss = 1.2891
Batch 240: Loss = 1.6279
Batch 300: Loss = 1.4887
Batch 360: Loss = 1.1650
Batch 420: Loss = 1.1604
Batch 480: Loss = 1.4119
Batch 540: Loss = 1.3941
Batch 600: Loss = 1.5174
Batch 660: Loss = 1.5342
Batch 720: Loss = 1.3049
Batch 780: Loss = 1.1894
Batch 840: Loss = 1.4546
Batch 900: Loss = 1.1502
Batch 960: Loss = 1.1958
Batch 1020: Loss = 1.1574
Batch 1080: Loss = 1.1932
Batch 1140: Loss = 0.9655
Batch 1200: Loss = 1.2308
Batch 1260: Loss = 1.2889
Batch 1320: Loss = 1.4561
Batch 1380: Loss = 1.3513
Batch 1440: Loss = 1.0158
Batch 1500: Loss = 1.4277
  Train Loss: 1.2736 | Train Acc: 54.12% | Val Loss: 1.3213 | Val Acc: 53.30%
Epoch 3/10 LR 0.001 and batch_size 60
Batch 60: Loss = 1.0597
Batch 120: Loss = 1.1845
Batch 180: Loss = 1.0264
Batch 240: Loss = 0.9282
Batch 300: Loss = 0.8465
Batch 360: Loss = 1.1262
Batch 420: Loss = 1.0683
Batch 480: Loss = 1.2865
Batch 540: Loss = 1.1608
Batch 600: Loss = 1.0480
Batch 660: Loss = 1.0074
Batch 720: Loss = 1.1019
Batch 780: Loss = 1.4263
Batch 840: Loss = 1.0247
Batch 900: Loss = 1.1422
Batch 960: Loss = 1.5007
Batch 1020: Loss = 1.2506
Batch 1080: Loss = 1.2959
Batch 1140: Loss = 1.3364
Batch 1200: Loss = 1.1081
Batch 1260: Loss = 1.0543
Batch 1320: Loss = 1.2784
Batch 1380: Loss = 1.1286
Batch 1440: Loss = 1.0709
Batch 1500: Loss = 1.1084
  Train Loss: 1.1428 | Train Acc: 58.93% | Val Loss: 1.2936 | Val Acc: 54.46%
Epoch 4/10 LR 0.001 and batch_size 60
Batch 60: Loss = 1.2320
Batch 120: Loss = 1.1358
Batch 180: Loss = 0.9613
Batch 240: Loss = 1.0688
Batch 300: Loss = 1.1902
Batch 360: Loss = 0.7781
Batch 420: Loss = 1.1126
Batch 480: Loss = 1.0761
Batch 540: Loss = 1.0483
Batch 600: Loss = 1.2059
Batch 660: Loss = 1.0176
Batch 720: Loss = 0.9328
Batch 780: Loss = 0.8777
Batch 840: Loss = 1.0627
Batch 900: Loss = 0.8937
Batch 960: Loss = 0.7745
Batch 1020: Loss = 0.9474
Batch 1080: Loss = 0.8962
Batch 1140: Loss = 0.9752
Batch 1200: Loss = 0.8290
Batch 1260: Loss = 1.3550
Batch 1320: Loss = 0.8336
Batch 1380: Loss = 0.7806
Batch 1440: Loss = 0.9787
Batch 1500: Loss = 0.8811
  Train Loss: 1.0412 | Train Acc: 63.03% | Val Loss: 1.4368 | Val Acc: 51.77%
Epoch 5/10 LR 0.001 and batch_size 60
Batch 60: Loss = 0.9584
Batch 120: Loss = 1.0416
Batch 180: Loss = 0.7877
Batch 240: Loss = 0.8540
Batch 300: Loss = 1.1791
Batch 360: Loss = 1.0347
Batch 420: Loss = 0.9902
Batch 480: Loss = 0.7734
Batch 540: Loss = 0.8006
Batch 600: Loss = 0.7739
Batch 660: Loss = 0.7918
Batch 720: Loss = 1.0475
Batch 780: Loss = 0.9096
Batch 840: Loss = 0.8658
Batch 900: Loss = 0.9320
Batch 960: Loss = 0.9146
Batch 1020: Loss = 0.8452
Batch 1080: Loss = 1.1439
Batch 1140: Loss = 0.7839
Batch 1200: Loss = 1.0124
Batch 1260: Loss = 0.9561
Batch 1320: Loss = 0.9246
Batch 1380: Loss = 0.9014
Batch 1440: Loss = 1.1476
Batch 1500: Loss = 1.0212
  Train Loss: 0.9504 | Train Acc: 66.16% | Val Loss: 1.1616 | Val Acc: 59.76%
Epoch 6/10 LR 0.001 and batch_size 60
Batch 60: Loss = 0.6680
Batch 120: Loss = 0.6100
Batch 180: Loss = 0.7878
Batch 240: Loss = 0.8488
Batch 300: Loss = 0.8330
Batch 360: Loss = 0.9157
Batch 420: Loss = 0.7650
Batch 480: Loss = 0.9716
Batch 540: Loss = 0.9132
Batch 600: Loss = 0.8090
Batch 660: Loss = 0.9859
Batch 720: Loss = 0.8475
Batch 780: Loss = 0.8552
Batch 840: Loss = 0.9701
Batch 900: Loss = 0.8121
Batch 960: Loss = 0.8657
Batch 1020: Loss = 0.9414
Batch 1080: Loss = 1.0794
Batch 1140: Loss = 0.6962
Batch 1200: Loss = 0.7652
Batch 1260: Loss = 1.1588
Batch 1320: Loss = 0.8572
Batch 1380: Loss = 0.7230
Batch 1440: Loss = 0.5604
Batch 1500: Loss = 0.7150
  Train Loss: 0.8566 | Train Acc: 69.34% | Val Loss: 1.2763 | Val Acc: 55.26%
Epoch 7/10 LR 0.001 and batch_size 60
Batch 60: Loss = 0.8823
Batch 120: Loss = 0.9920
Batch 180: Loss = 0.6569
Batch 240: Loss = 0.6014
Batch 300: Loss = 0.9428
Batch 360: Loss = 0.6923
Batch 420: Loss = 0.7473
Batch 480: Loss = 0.6238
Batch 540: Loss = 0.7399
Batch 600: Loss = 0.7921
Batch 660: Loss = 0.5741
Batch 720: Loss = 0.8883
Batch 780: Loss = 0.8975
Batch 840: Loss = 0.8669
Batch 900: Loss = 0.6503
Batch 960: Loss = 0.8264
Batch 1020: Loss = 1.0668
Batch 1080: Loss = 0.8580
Batch 1140: Loss = 0.8716
Batch 1200: Loss = 0.5699
Batch 1260: Loss = 0.9007
Batch 1320: Loss = 1.1071
Batch 1380: Loss = 0.8482
Batch 1440: Loss = 0.7411
Batch 1500: Loss = 0.7788
  Train Loss: 0.7684 | Train Acc: 72.51% | Val Loss: 1.0501 | Val Acc: 64.17%
Epoch 8/10 LR 0.001 and batch_size 60
Batch 60: Loss = 0.6252
Batch 120: Loss = 0.6108
Batch 180: Loss = 0.7823
Batch 240: Loss = 0.6915
Batch 300: Loss = 0.7052
Batch 360: Loss = 0.7792
Batch 420: Loss = 0.9692
Batch 480: Loss = 0.5694
Batch 540: Loss = 0.5322
Batch 600: Loss = 0.8122
Batch 660: Loss = 0.6345
Batch 720: Loss = 0.6889
Batch 780: Loss = 0.9420
Batch 840: Loss = 0.6795
Batch 900: Loss = 0.6301
Batch 960: Loss = 0.7238
Batch 1020: Loss = 0.4999
Batch 1080: Loss = 0.4356
Batch 1140: Loss = 0.5986
Batch 1200: Loss = 0.5203
Batch 1260: Loss = 0.9501
Batch 1320: Loss = 0.7269
Batch 1380: Loss = 0.6189
Batch 1440: Loss = 0.7022
Batch 1500: Loss = 0.6388
  Train Loss: 0.6758 | Train Acc: 75.82% | Val Loss: 1.1765 | Val Acc: 61.62%
Epoch 9/10 LR 0.001 and batch_size 60
Batch 60: Loss = 0.3707
Batch 120: Loss = 0.5136
Batch 180: Loss = 0.3947
Batch 240: Loss = 0.4133
Batch 300: Loss = 0.5452
Batch 360: Loss = 0.6465
Batch 420: Loss = 0.5805
Batch 480: Loss = 0.6864
Batch 540: Loss = 0.2938
Batch 600: Loss = 0.6497
Batch 660: Loss = 0.7961
Batch 720: Loss = 0.4811
Batch 780: Loss = 0.7023
Batch 840: Loss = 0.5833
Batch 900: Loss = 0.5212
Batch 960: Loss = 0.6749
Batch 1020: Loss = 0.7049
Batch 1080: Loss = 0.7716
Batch 1140: Loss = 0.4790
Batch 1200: Loss = 0.4873
Batch 1260: Loss = 0.5540
Batch 1320: Loss = 0.8733
Batch 1380: Loss = 0.7060
Batch 1440: Loss = 0.4621
Batch 1500: Loss = 0.5252
  Train Loss: 0.5765 | Train Acc: 79.34% | Val Loss: 1.1185 | Val Acc: 64.21%
Epoch 10/10 LR 0.001 and batch_size 60
Batch 60: Loss = 0.4499
Batch 120: Loss = 0.4924
Batch 180: Loss = 0.4868
Batch 240: Loss = 0.4924
Batch 300: Loss = 0.5344
Batch 360: Loss = 0.3636
Batch 420: Loss = 0.4356
Batch 480: Loss = 0.6016
Batch 540: Loss = 0.6016
Batch 600: Loss = 0.3971
Batch 660: Loss = 0.4753
Batch 720: Loss = 0.4497
Batch 780: Loss = 0.4882
Batch 840: Loss = 0.3598
Batch 900: Loss = 0.5126
Batch 960: Loss = 0.5605
Batch 1020: Loss = 0.6225
Batch 1080: Loss = 0.2529
Batch 1140: Loss = 0.4087
Batch 1200: Loss = 0.4256
Batch 1260: Loss = 0.2743
Batch 1320: Loss = 0.5256
Batch 1380: Loss = 0.4983
Batch 1440: Loss = 0.4198
Batch 1500: Loss = 0.4393
  Train Loss: 0.4793 | Train Acc: 82.81% | Val Loss: 1.2592 | Val Acc: 63.26%
Final Test Loss: 1.1314, Test Acc: 63.93%
Epoch 1/10 LR 0.001 and batch_size 80
Batch 60: Loss = 2.0782
Batch 120: Loss = 1.7688
Batch 180: Loss = 1.6486
Batch 240: Loss = 1.5637
Batch 300: Loss = 1.8368
Batch 360: Loss = 1.6279
Batch 420: Loss = 1.5388
Batch 480: Loss = 1.2325
Batch 540: Loss = 1.5124
Batch 600: Loss = 1.4233
Batch 660: Loss = 1.3451
Batch 720: Loss = 1.2511
Batch 780: Loss = 1.3310
Batch 840: Loss = 1.5868
Batch 900: Loss = 1.5172
Batch 960: Loss = 1.5881
Batch 1020: Loss = 1.4503
Batch 1080: Loss = 1.3461
  Train Loss: 1.5468 | Train Acc: 43.53% | Val Loss: 1.5618 | Val Acc: 44.25%
Epoch 2/10 LR 0.001 and batch_size 80
Batch 60: Loss = 1.0899
Batch 120: Loss = 1.2477
Batch 180: Loss = 1.3011
Batch 240: Loss = 1.3419
Batch 300: Loss = 1.3605
Batch 360: Loss = 1.1007
Batch 420: Loss = 1.4228
Batch 480: Loss = 1.2743
Batch 540: Loss = 1.3366
Batch 600: Loss = 1.2027
Batch 660: Loss = 1.1761
Batch 720: Loss = 1.4491
Batch 780: Loss = 1.0629
Batch 840: Loss = 1.3927
Batch 900: Loss = 1.0732
Batch 960: Loss = 1.3130
Batch 1020: Loss = 1.2938
Batch 1080: Loss = 1.3372
  Train Loss: 1.2596 | Train Acc: 54.78% | Val Loss: 1.6288 | Val Acc: 46.65%
Epoch 3/10 LR 0.001 and batch_size 80
Batch 60: Loss = 1.2197
Batch 120: Loss = 1.2247
Batch 180: Loss = 1.1582
Batch 240: Loss = 1.1373
Batch 300: Loss = 0.9332
Batch 360: Loss = 1.0964
Batch 420: Loss = 1.0247
Batch 480: Loss = 1.2708
Batch 540: Loss = 1.0416
Batch 600: Loss = 1.2356
Batch 660: Loss = 1.1878
Batch 720: Loss = 1.0425
Batch 780: Loss = 1.0677
Batch 840: Loss = 1.0560
Batch 900: Loss = 0.8939
Batch 960: Loss = 0.9351
Batch 1020: Loss = 1.0013
Batch 1080: Loss = 1.0723
  Train Loss: 1.1202 | Train Acc: 59.99% | Val Loss: 1.3592 | Val Acc: 51.59%
Epoch 4/10 LR 0.001 and batch_size 80
Batch 60: Loss = 1.2262
Batch 120: Loss = 0.8070
Batch 180: Loss = 1.1176
Batch 240: Loss = 1.1418
Batch 300: Loss = 0.8811
Batch 360: Loss = 0.8897
Batch 420: Loss = 1.0487
Batch 480: Loss = 0.9314
Batch 540: Loss = 1.0132
Batch 600: Loss = 1.0042
Batch 660: Loss = 0.9091
Batch 720: Loss = 1.3280
Batch 780: Loss = 1.0001
Batch 840: Loss = 1.0351
Batch 900: Loss = 1.0125
Batch 960: Loss = 0.8264
Batch 1020: Loss = 1.0483
Batch 1080: Loss = 0.9724
  Train Loss: 1.0199 | Train Acc: 63.75% | Val Loss: 1.1245 | Val Acc: 59.92%
Epoch 5/10 LR 0.001 and batch_size 80
Batch 60: Loss = 0.8076
Batch 120: Loss = 0.9815
Batch 180: Loss = 0.9539
Batch 240: Loss = 0.9157
Batch 300: Loss = 0.8582
Batch 360: Loss = 1.1012
Batch 420: Loss = 0.9532
Batch 480: Loss = 0.7803
Batch 540: Loss = 1.0424
Batch 600: Loss = 0.8198
Batch 660: Loss = 0.9086
Batch 720: Loss = 0.9746
Batch 780: Loss = 0.8980
Batch 840: Loss = 1.0211
Batch 900: Loss = 0.9729
Batch 960: Loss = 0.9532
Batch 1020: Loss = 0.9628
Batch 1080: Loss = 0.9628
  Train Loss: 0.9327 | Train Acc: 66.85% | Val Loss: 1.1714 | Val Acc: 59.16%
Epoch 6/10 LR 0.001 and batch_size 80
Batch 60: Loss = 0.7373
Batch 120: Loss = 0.8440
Batch 180: Loss = 0.6514
Batch 240: Loss = 0.9932
Batch 300: Loss = 0.7938
Batch 360: Loss = 0.8569
Batch 420: Loss = 1.0685
Batch 480: Loss = 0.7684
Batch 540: Loss = 0.6124
Batch 600: Loss = 0.8240
Batch 660: Loss = 0.6540
Batch 720: Loss = 1.0190
Batch 780: Loss = 0.9291
Batch 840: Loss = 1.0509
Batch 900: Loss = 1.0619
Batch 960: Loss = 0.8588
Batch 1020: Loss = 0.7596
Batch 1080: Loss = 0.9279
  Train Loss: 0.8490 | Train Acc: 69.84% | Val Loss: 1.0421 | Val Acc: 63.18%
Epoch 7/10 LR 0.001 and batch_size 80
Batch 60: Loss = 0.6627
Batch 120: Loss = 0.7075
Batch 180: Loss = 0.6834
Batch 240: Loss = 0.7429
Batch 300: Loss = 0.5647
Batch 360: Loss = 0.7185
Batch 420: Loss = 0.7472
Batch 480: Loss = 0.9678
Batch 540: Loss = 0.6990
Batch 600: Loss = 0.7702
Batch 660: Loss = 0.7870
Batch 720: Loss = 0.7125
Batch 780: Loss = 0.7973
Batch 840: Loss = 0.7915
Batch 900: Loss = 0.6739
Batch 960: Loss = 0.8113
Batch 1020: Loss = 0.8033
Batch 1080: Loss = 0.8365
  Train Loss: 0.7615 | Train Acc: 73.07% | Val Loss: 1.0534 | Val Acc: 63.69%
Epoch 8/10 LR 0.001 and batch_size 80
Batch 60: Loss = 0.5380
Batch 120: Loss = 0.6330
Batch 180: Loss = 0.8572
Batch 240: Loss = 0.5935
Batch 300: Loss = 0.5490
Batch 360: Loss = 0.6361
Batch 420: Loss = 0.5491
Batch 480: Loss = 0.6949
Batch 540: Loss = 0.8078
Batch 600: Loss = 0.6293
Batch 660: Loss = 0.6081
Batch 720: Loss = 0.5678
Batch 780: Loss = 0.7166
Batch 840: Loss = 0.6387
Batch 900: Loss = 0.7084
Batch 960: Loss = 0.6702
Batch 1020: Loss = 0.6384
Batch 1080: Loss = 0.5250
  Train Loss: 0.6684 | Train Acc: 76.14% | Val Loss: 1.0794 | Val Acc: 63.22%
Epoch 9/10 LR 0.001 and batch_size 80
Batch 60: Loss = 0.5749
Batch 120: Loss = 0.5753
Batch 180: Loss = 0.5591
Batch 240: Loss = 0.5235
Batch 300: Loss = 0.4489
Batch 360: Loss = 0.7679
Batch 420: Loss = 0.4555
Batch 480: Loss = 0.7033
Batch 540: Loss = 0.7038
Batch 600: Loss = 0.4768
Batch 660: Loss = 0.6781
Batch 720: Loss = 0.6318
Batch 780: Loss = 0.6158
Batch 840: Loss = 0.8382
Batch 900: Loss = 0.5406
Batch 960: Loss = 0.6135
Batch 1020: Loss = 0.5161
Batch 1080: Loss = 0.4326
  Train Loss: 0.5692 | Train Acc: 79.65% | Val Loss: 1.1465 | Val Acc: 62.73%
Epoch 10/10 LR 0.001 and batch_size 80
Batch 60: Loss = 0.3070
Batch 120: Loss = 0.3485
Batch 180: Loss = 0.4094
Batch 240: Loss = 0.4823
Batch 300: Loss = 0.7992
Batch 360: Loss = 0.4097
Batch 420: Loss = 0.4583
Batch 480: Loss = 0.6040
Batch 540: Loss = 0.3737
Batch 600: Loss = 0.4531
Batch 660: Loss = 0.6261
Batch 720: Loss = 0.7029
Batch 780: Loss = 0.5275
Batch 840: Loss = 0.5311
Batch 900: Loss = 0.5128
Batch 960: Loss = 0.5114
Batch 1020: Loss = 0.4209
Batch 1080: Loss = 0.4335
  Train Loss: 0.4743 | Train Acc: 83.14% | Val Loss: 1.1961 | Val Acc: 64.31%
Final Test Loss: 1.2113, Test Acc: 64.22%
Epoch 1/10 LR 0.001 and batch_size 100
Batch 60: Loss = 1.7092
Batch 120: Loss = 1.6893
Batch 180: Loss = 1.6338
Batch 240: Loss = 1.5513
Batch 300: Loss = 1.5831
Batch 360: Loss = 1.4897
Batch 420: Loss = 1.4879
Batch 480: Loss = 1.4654
Batch 540: Loss = 1.2156
Batch 600: Loss = 1.4420
Batch 660: Loss = 1.3675
Batch 720: Loss = 1.4326
Batch 780: Loss = 1.4860
Batch 840: Loss = 1.4272
Batch 900: Loss = 1.5217
  Train Loss: 1.5386 | Train Acc: 43.50% | Val Loss: 1.8367 | Val Acc: 39.57%
Epoch 2/10 LR 0.001 and batch_size 100
Batch 60: Loss = 1.3813
Batch 120: Loss = 1.1547
Batch 180: Loss = 1.4241
Batch 240: Loss = 1.3986
Batch 300: Loss = 1.2086
Batch 360: Loss = 1.2271
Batch 420: Loss = 1.1974
Batch 480: Loss = 1.1424
Batch 540: Loss = 1.2394
Batch 600: Loss = 1.1084
Batch 660: Loss = 1.3177
Batch 720: Loss = 1.4204
Batch 780: Loss = 1.0442
Batch 840: Loss = 1.0656
Batch 900: Loss = 1.4355
  Train Loss: 1.2544 | Train Acc: 54.67% | Val Loss: 1.3256 | Val Acc: 52.26%
Epoch 3/10 LR 0.001 and batch_size 100
Batch 60: Loss = 1.1291
Batch 120: Loss = 1.2759
Batch 180: Loss = 1.0954
Batch 240: Loss = 1.0558
Batch 300: Loss = 0.9677
Batch 360: Loss = 1.2585
Batch 420: Loss = 1.2307
Batch 480: Loss = 1.1317
Batch 540: Loss = 1.2050
Batch 600: Loss = 1.0233
Batch 660: Loss = 1.2330
Batch 720: Loss = 1.3001
Batch 780: Loss = 1.0464
Batch 840: Loss = 0.9452
Batch 900: Loss = 1.0126
  Train Loss: 1.1216 | Train Acc: 59.71% | Val Loss: 1.2606 | Val Acc: 55.06%
Epoch 4/10 LR 0.001 and batch_size 100
Batch 60: Loss = 1.1419
Batch 120: Loss = 1.0267
Batch 180: Loss = 0.9056
Batch 240: Loss = 0.9414
Batch 300: Loss = 0.7827
Batch 360: Loss = 0.9399
Batch 420: Loss = 1.3109
Batch 480: Loss = 1.1317
Batch 540: Loss = 0.9432
Batch 600: Loss = 0.9657
Batch 660: Loss = 1.0328
Batch 720: Loss = 1.1087
Batch 780: Loss = 1.0149
Batch 840: Loss = 1.0462
Batch 900: Loss = 1.1209
  Train Loss: 1.0211 | Train Acc: 63.40% | Val Loss: 1.3413 | Val Acc: 53.77%
Epoch 5/10 LR 0.001 and batch_size 100
Batch 60: Loss = 0.9973
Batch 120: Loss = 0.8903
Batch 180: Loss = 1.2183
Batch 240: Loss = 0.8529
Batch 300: Loss = 0.7198
Batch 360: Loss = 0.8162
Batch 420: Loss = 1.0190
Batch 480: Loss = 0.9583
Batch 540: Loss = 0.9185
Batch 600: Loss = 1.1281
Batch 660: Loss = 0.9822
Batch 720: Loss = 0.9314
Batch 780: Loss = 0.9625
Batch 840: Loss = 0.9094
Batch 900: Loss = 0.9264
  Train Loss: 0.9359 | Train Acc: 66.62% | Val Loss: 1.0650 | Val Acc: 62.00%
Epoch 6/10 LR 0.001 and batch_size 100
Batch 60: Loss = 0.8054
Batch 120: Loss = 1.1595
Batch 180: Loss = 0.8945
Batch 240: Loss = 0.8922
Batch 300: Loss = 0.8307
Batch 360: Loss = 0.9259
Batch 420: Loss = 0.7704
Batch 480: Loss = 0.7698
Batch 540: Loss = 0.7537
Batch 600: Loss = 0.7212
Batch 660: Loss = 0.9364
Batch 720: Loss = 1.0431
Batch 780: Loss = 0.8118
Batch 840: Loss = 0.8238
Batch 900: Loss = 0.8776
  Train Loss: 0.8520 | Train Acc: 69.58% | Val Loss: 1.0672 | Val Acc: 62.63%
Epoch 7/10 LR 0.001 and batch_size 100
Batch 60: Loss = 0.6299
Batch 120: Loss = 0.7790
Batch 180: Loss = 0.9998
Batch 240: Loss = 0.8317
Batch 300: Loss = 0.6093
Batch 360: Loss = 0.7532
Batch 420: Loss = 0.8595
Batch 480: Loss = 0.8573
Batch 540: Loss = 0.9226
Batch 600: Loss = 0.6465
Batch 660: Loss = 0.9256
Batch 720: Loss = 0.8402
Batch 780: Loss = 0.6661
Batch 840: Loss = 0.7574
Batch 900: Loss = 0.6939
  Train Loss: 0.7674 | Train Acc: 72.62% | Val Loss: 1.0659 | Val Acc: 63.00%
Epoch 8/10 LR 0.001 and batch_size 100
Batch 60: Loss = 0.6126
Batch 120: Loss = 0.6366
Batch 180: Loss = 0.7000
Batch 240: Loss = 0.6578
Batch 300: Loss = 0.5176
Batch 360: Loss = 0.4878
Batch 420: Loss = 0.7493
Batch 480: Loss = 0.6543
Batch 540: Loss = 0.5624
Batch 600: Loss = 0.5283
Batch 660: Loss = 0.8031
Batch 720: Loss = 0.6692
Batch 780: Loss = 0.5619
Batch 840: Loss = 0.5817
Batch 900: Loss = 0.7397
  Train Loss: 0.6761 | Train Acc: 75.91% | Val Loss: 1.1331 | Val Acc: 62.48%
Epoch 9/10 LR 0.001 and batch_size 100
Batch 60: Loss = 0.3554
Batch 120: Loss = 0.5546
Batch 180: Loss = 0.5587
Batch 240: Loss = 0.4827
Batch 300: Loss = 0.6169
Batch 360: Loss = 0.6294
Batch 420: Loss = 0.6354
Batch 480: Loss = 0.4786
Batch 540: Loss = 0.7428
Batch 600: Loss = 0.5431
Batch 660: Loss = 0.4783
Batch 720: Loss = 0.6748
Batch 780: Loss = 0.5591
Batch 840: Loss = 0.6469
Batch 900: Loss = 0.6088
  Train Loss: 0.5866 | Train Acc: 78.93% | Val Loss: 1.2143 | Val Acc: 62.18%
Epoch 10/10 LR 0.001 and batch_size 100
Batch 60: Loss = 0.5979
Batch 120: Loss = 0.3184
Batch 180: Loss = 0.2517
Batch 240: Loss = 0.5659
Batch 300: Loss = 0.5270
Batch 360: Loss = 0.4997
Batch 420: Loss = 0.4351
Batch 480: Loss = 0.4987
Batch 540: Loss = 0.5105
Batch 600: Loss = 0.4311
Batch 660: Loss = 0.4602
Batch 720: Loss = 0.4311
Batch 780: Loss = 0.7040
Batch 840: Loss = 0.4819
Batch 900: Loss = 0.5324
  Train Loss: 0.4911 | Train Acc: 82.37% | Val Loss: 1.1333 | Val Acc: 64.36%
Final Test Loss: 1.1520, Test Acc: 63.84%
Epoch 1/10 LR 0.01 and batch_size 40
Batch 60: Loss = 3.0092
Batch 120: Loss = 2.6124
Batch 180: Loss = 1.9622
Batch 240: Loss = 2.0031
Batch 300: Loss = 2.1824
Batch 360: Loss = 2.1218
Batch 420: Loss = 1.8287
Batch 480: Loss = 1.8051
Batch 540: Loss = 2.1001
Batch 600: Loss = 1.8218
Batch 660: Loss = 1.8695
Batch 720: Loss = 1.8910
Batch 780: Loss = 1.6566
Batch 840: Loss = 1.7292
Batch 900: Loss = 1.6602
Batch 960: Loss = 1.5502
Batch 1020: Loss = 2.0859
Batch 1080: Loss = 1.6398
Batch 1140: Loss = 1.4760
Batch 1200: Loss = 2.0187
Batch 1260: Loss = 1.8355
Batch 1320: Loss = 1.6092
Batch 1380: Loss = 1.6614
Batch 1440: Loss = 1.1874
Batch 1500: Loss = 1.6140
Batch 1560: Loss = 1.3621
Batch 1620: Loss = 1.7904
Batch 1680: Loss = 1.7950
Batch 1740: Loss = 1.5804
Batch 1800: Loss = 1.6394
Batch 1860: Loss = 1.4987
Batch 1920: Loss = 1.5894
Batch 1980: Loss = 1.6321
Batch 2040: Loss = 1.2455
Batch 2100: Loss = 1.6712
Batch 2160: Loss = 1.6310
Batch 2220: Loss = 1.6884
  Train Loss: 1.7551 | Train Acc: 35.62% | Val Loss: 1.5088 | Val Acc: 44.48%
Epoch 2/10 LR 0.01 and batch_size 40
Batch 60: Loss = 1.4956
Batch 120: Loss = 1.3576
Batch 180: Loss = 1.7367
Batch 240: Loss = 1.6370
Batch 300: Loss = 1.4458
Batch 360: Loss = 1.1645
Batch 420: Loss = 1.1367
Batch 480: Loss = 1.4341
Batch 540: Loss = 1.4115
Batch 600: Loss = 1.1695
Batch 660: Loss = 1.1474
Batch 720: Loss = 1.2761
Batch 780: Loss = 1.6331
Batch 840: Loss = 1.3218
Batch 900: Loss = 1.3816
Batch 960: Loss = 1.2723
Batch 1020: Loss = 1.6100
Batch 1080: Loss = 1.6390
Batch 1140: Loss = 1.3921
Batch 1200: Loss = 1.3585
Batch 1260: Loss = 1.5824
Batch 1320: Loss = 1.3301
Batch 1380: Loss = 1.5486
Batch 1440: Loss = 1.6381
Batch 1500: Loss = 1.7167
Batch 1560: Loss = 1.1954
Batch 1620: Loss = 1.6694
Batch 1680: Loss = 1.1969
Batch 1740: Loss = 1.2789
Batch 1800: Loss = 1.2851
Batch 1860: Loss = 1.3886
Batch 1920: Loss = 1.1572
Batch 1980: Loss = 1.4620
Batch 2040: Loss = 1.1571
Batch 2100: Loss = 1.3629
Batch 2160: Loss = 1.3237
Batch 2220: Loss = 1.0118
  Train Loss: 1.4044 | Train Acc: 48.96% | Val Loss: 1.5306 | Val Acc: 46.43%
Epoch 3/10 LR 0.01 and batch_size 40
Batch 60: Loss = 1.2011
Batch 120: Loss = 1.2946
Batch 180: Loss = 1.3326
Batch 240: Loss = 1.4202
Batch 300: Loss = 1.1189
Batch 360: Loss = 0.9816
Batch 420: Loss = 1.0437
Batch 480: Loss = 1.0612
Batch 540: Loss = 1.7020
Batch 600: Loss = 1.2321
Batch 660: Loss = 1.1030
Batch 720: Loss = 1.1295
Batch 780: Loss = 1.1916
Batch 840: Loss = 1.3984
Batch 900: Loss = 1.3941
Batch 960: Loss = 1.3196
Batch 1020: Loss = 1.4392
Batch 1080: Loss = 1.4128
Batch 1140: Loss = 1.1509
Batch 1200: Loss = 1.3317
Batch 1260: Loss = 1.0671
Batch 1320: Loss = 1.4234
Batch 1380: Loss = 1.0461
Batch 1440: Loss = 1.2957
Batch 1500: Loss = 1.2433
Batch 1560: Loss = 1.4294
Batch 1620: Loss = 0.9820
Batch 1680: Loss = 1.2597
Batch 1740: Loss = 1.4637
Batch 1800: Loss = 1.0636
Batch 1860: Loss = 1.4126
Batch 1920: Loss = 1.1069
Batch 1980: Loss = 1.4231
Batch 2040: Loss = 1.5348
Batch 2100: Loss = 1.1497
Batch 2160: Loss = 1.6289
Batch 2220: Loss = 1.4906
  Train Loss: 1.2567 | Train Acc: 54.94% | Val Loss: 1.2416 | Val Acc: 55.27%
Epoch 4/10 LR 0.01 and batch_size 40
Batch 60: Loss = 0.8441
Batch 120: Loss = 1.2899
Batch 180: Loss = 0.7842
Batch 240: Loss = 0.9775
Batch 300: Loss = 1.1649
Batch 360: Loss = 1.0480
Batch 420: Loss = 1.7757
Batch 480: Loss = 1.2908
Batch 540: Loss = 1.3060
Batch 600: Loss = 0.9681
Batch 660: Loss = 1.1778
Batch 720: Loss = 0.7805
Batch 780: Loss = 1.1248
Batch 840: Loss = 1.3203
Batch 900: Loss = 1.3534
Batch 960: Loss = 1.0911
Batch 1020: Loss = 1.0907
Batch 1080: Loss = 1.1651
Batch 1140: Loss = 0.9269
Batch 1200: Loss = 1.4098
Batch 1260: Loss = 1.2255
Batch 1320: Loss = 1.0818
Batch 1380: Loss = 1.1253
Batch 1440: Loss = 0.9830
Batch 1500: Loss = 1.1184
Batch 1560: Loss = 1.0291
Batch 1620: Loss = 0.8968
Batch 1680: Loss = 1.3560
Batch 1740: Loss = 1.1504
Batch 1800: Loss = 0.8465
Batch 1860: Loss = 1.1465
Batch 1920: Loss = 1.1077
Batch 1980: Loss = 1.1229
Batch 2040: Loss = 0.9555
Batch 2100: Loss = 1.2725
Batch 2160: Loss = 1.1891
Batch 2220: Loss = 1.1401
  Train Loss: 1.1463 | Train Acc: 59.10% | Val Loss: 1.1436 | Val Acc: 59.31%
Epoch 5/10 LR 0.01 and batch_size 40
Batch 60: Loss = 0.9777
Batch 120: Loss = 0.8945
Batch 180: Loss = 1.1616
Batch 240: Loss = 1.0474
Batch 300: Loss = 1.1750
Batch 360: Loss = 0.8483
Batch 420: Loss = 1.0179
Batch 480: Loss = 1.0656
Batch 540: Loss = 1.1763
Batch 600: Loss = 0.9318
Batch 660: Loss = 0.8714
Batch 720: Loss = 1.0675
Batch 780: Loss = 0.9279
Batch 840: Loss = 0.8721
Batch 900: Loss = 0.9144
Batch 960: Loss = 1.1040
Batch 1020: Loss = 0.8997
Batch 1080: Loss = 0.9670
Batch 1140: Loss = 1.0577
Batch 1200: Loss = 0.7639
Batch 1260: Loss = 1.0858
Batch 1320: Loss = 1.0348
Batch 1380: Loss = 1.2824
Batch 1440: Loss = 1.0198
Batch 1500: Loss = 0.8152
Batch 1560: Loss = 0.6623
Batch 1620: Loss = 0.8808
Batch 1680: Loss = 1.1941
Batch 1740: Loss = 1.0061
Batch 1800: Loss = 1.0165
Batch 1860: Loss = 0.9047
Batch 1920: Loss = 0.9497
Batch 1980: Loss = 0.9659
Batch 2040: Loss = 0.9484
Batch 2100: Loss = 1.3151
Batch 2160: Loss = 1.4473
Batch 2220: Loss = 1.5408
  Train Loss: 1.0519 | Train Acc: 62.71% | Val Loss: 1.1241 | Val Acc: 60.19%
Epoch 6/10 LR 0.01 and batch_size 40
Batch 60: Loss = 0.9210
Batch 120: Loss = 0.9222
Batch 180: Loss = 0.8931
Batch 240: Loss = 0.8102
Batch 300: Loss = 0.9491
Batch 360: Loss = 0.9643
Batch 420: Loss = 1.0998
Batch 480: Loss = 0.9616
Batch 540: Loss = 0.8239
Batch 600: Loss = 0.9861
Batch 660: Loss = 0.8154
Batch 720: Loss = 0.7511
Batch 780: Loss = 1.1552
Batch 840: Loss = 1.0640
Batch 900: Loss = 1.1044
Batch 960: Loss = 0.8385
Batch 1020: Loss = 0.7925
Batch 1080: Loss = 1.1774
Batch 1140: Loss = 1.1793
Batch 1200: Loss = 1.0277
Batch 1260: Loss = 1.1288
Batch 1320: Loss = 1.0842
Batch 1380: Loss = 0.6571
Batch 1440: Loss = 0.6257
Batch 1500: Loss = 1.1846
Batch 1560: Loss = 1.2552
Batch 1620: Loss = 1.2404
Batch 1680: Loss = 0.8150
Batch 1740: Loss = 0.8783
Batch 1800: Loss = 1.0385
Batch 1860: Loss = 1.0737
Batch 1920: Loss = 0.5179
Batch 1980: Loss = 0.8560
Batch 2040: Loss = 0.8698
Batch 2100: Loss = 0.9247
Batch 2160: Loss = 1.2719
Batch 2220: Loss = 1.2468
  Train Loss: 0.9619 | Train Acc: 66.04% | Val Loss: 1.1999 | Val Acc: 59.04%
Epoch 7/10 LR 0.01 and batch_size 40
Batch 60: Loss = 0.7331
Batch 120: Loss = 0.5620
Batch 180: Loss = 0.5593
Batch 240: Loss = 0.6162
Batch 300: Loss = 0.8939
Batch 360: Loss = 0.7022
Batch 420: Loss = 0.8621
Batch 480: Loss = 0.7121
Batch 540: Loss = 0.7829
Batch 600: Loss = 1.3149
Batch 660: Loss = 0.8667
Batch 720: Loss = 1.1561
Batch 780: Loss = 0.9717
Batch 840: Loss = 0.6465
Batch 900: Loss = 0.8083
Batch 960: Loss = 0.8548
Batch 1020: Loss = 0.5130
Batch 1080: Loss = 0.6481
Batch 1140: Loss = 1.1276
Batch 1200: Loss = 0.9826
Batch 1260: Loss = 0.8730
Batch 1320: Loss = 0.9416
Batch 1380: Loss = 1.1866
Batch 1440: Loss = 0.7408
Batch 1500: Loss = 0.7708
Batch 1560: Loss = 0.8559
Batch 1620: Loss = 0.7481
Batch 1680: Loss = 0.9793
Batch 1740: Loss = 0.9959
Batch 1800: Loss = 1.0894
Batch 1860: Loss = 0.8116
Batch 1920: Loss = 0.8687
Batch 1980: Loss = 0.9873
Batch 2040: Loss = 0.7845
Batch 2100: Loss = 0.8286
Batch 2160: Loss = 0.8471
Batch 2220: Loss = 1.2250
  Train Loss: 0.8710 | Train Acc: 69.21% | Val Loss: 1.1651 | Val Acc: 59.94%
Epoch 8/10 LR 0.01 and batch_size 40
Batch 60: Loss = 0.8102
Batch 120: Loss = 0.6484
Batch 180: Loss = 0.7512
Batch 240: Loss = 0.6329
Batch 300: Loss = 0.9225
Batch 360: Loss = 0.7771
Batch 420: Loss = 1.2677
Batch 480: Loss = 0.5602
Batch 540: Loss = 0.6187
Batch 600: Loss = 0.6654
Batch 660: Loss = 0.4313
Batch 720: Loss = 0.7759
Batch 780: Loss = 0.6053
Batch 840: Loss = 0.6244
Batch 900: Loss = 1.0679
Batch 960: Loss = 0.5998
Batch 1020: Loss = 0.5470
Batch 1080: Loss = 0.5129
Batch 1140: Loss = 0.5885
Batch 1200: Loss = 0.6928
Batch 1260: Loss = 0.6721
Batch 1320: Loss = 0.8814
Batch 1380: Loss = 1.0490
Batch 1440: Loss = 0.9059
Batch 1500: Loss = 0.7031
Batch 1560: Loss = 0.8492
Batch 1620: Loss = 0.9318
Batch 1680: Loss = 0.7394
Batch 1740: Loss = 0.6938
Batch 1800: Loss = 0.6302
Batch 1860: Loss = 0.8674
Batch 1920: Loss = 0.5594
Batch 1980: Loss = 0.7879
Batch 2040: Loss = 0.5713
Batch 2100: Loss = 0.7105
Batch 2160: Loss = 0.6226
Batch 2220: Loss = 0.5445
  Train Loss: 0.7804 | Train Acc: 72.47% | Val Loss: 1.1921 | Val Acc: 60.44%
Epoch 9/10 LR 0.01 and batch_size 40
Batch 60: Loss = 0.7786
Batch 120: Loss = 0.8982
Batch 180: Loss = 0.4450
Batch 240: Loss = 0.9893
Batch 300: Loss = 0.8529
Batch 360: Loss = 0.6259
Batch 420: Loss = 0.7724
Batch 480: Loss = 0.5296
Batch 540: Loss = 0.4947
Batch 600: Loss = 0.5177
Batch 660: Loss = 0.6327
Batch 720: Loss = 0.8670
Batch 780: Loss = 0.5933
Batch 840: Loss = 0.7217
Batch 900: Loss = 0.8614
Batch 960: Loss = 0.5881
Batch 1020: Loss = 0.6975
Batch 1080: Loss = 0.7124
Batch 1140: Loss = 0.6767
Batch 1200: Loss = 0.6623
Batch 1260: Loss = 0.4313
Batch 1320: Loss = 0.6603
Batch 1380: Loss = 0.9810
Batch 1440: Loss = 0.6539
Batch 1500: Loss = 0.9690
Batch 1560: Loss = 0.9779
Batch 1620: Loss = 0.6773
Batch 1680: Loss = 0.3739
Batch 1740: Loss = 0.9068
Batch 1800: Loss = 0.4726
Batch 1860: Loss = 0.5671
Batch 1920: Loss = 0.7573
Batch 1980: Loss = 0.9321
Batch 2040: Loss = 0.8474
Batch 2100: Loss = 0.7634
Batch 2160: Loss = 0.4035
Batch 2220: Loss = 0.7025
  Train Loss: 0.6929 | Train Acc: 75.58% | Val Loss: 1.1296 | Val Acc: 62.80%
Epoch 10/10 LR 0.01 and batch_size 40
Batch 60: Loss = 0.2631
Batch 120: Loss = 0.7422
Batch 180: Loss = 0.4927
Batch 240: Loss = 0.5361
Batch 300: Loss = 0.2674
Batch 360: Loss = 0.4415
Batch 420: Loss = 0.4799
Batch 480: Loss = 0.3833
Batch 540: Loss = 0.6828
Batch 600: Loss = 0.7816
Batch 660: Loss = 0.7792
Batch 720: Loss = 0.7816
Batch 780: Loss = 0.5701
Batch 840: Loss = 0.5031
Batch 900: Loss = 0.7684
Batch 960: Loss = 0.8768
Batch 1020: Loss = 0.5062
Batch 1080: Loss = 0.8366
Batch 1140: Loss = 0.7928
Batch 1200: Loss = 0.3800
Batch 1260: Loss = 0.5577
Batch 1320: Loss = 0.3120
Batch 1380: Loss = 0.9093
Batch 1440: Loss = 0.6303
Batch 1500: Loss = 0.5506
Batch 1560: Loss = 0.7342
Batch 1620: Loss = 0.7264
Batch 1680: Loss = 0.7883
Batch 1740: Loss = 0.3896
Batch 1800: Loss = 0.6825
Batch 1860: Loss = 0.9495
Batch 1920: Loss = 0.2853
Batch 1980: Loss = 0.5757
Batch 2040: Loss = 0.3603
Batch 2100: Loss = 0.9206
Batch 2160: Loss = 0.7865
Batch 2220: Loss = 0.5316
  Train Loss: 0.6080 | Train Acc: 78.45% | Val Loss: 1.3851 | Val Acc: 58.11%
Final Test Loss: 1.1368, Test Acc: 62.65%
Epoch 1/10 LR 0.01 and batch_size 60
Batch 60: Loss = 3.0002
Batch 120: Loss = 2.0164
Batch 180: Loss = 2.3691
Batch 240: Loss = 1.8348
Batch 300: Loss = 2.0171
Batch 360: Loss = 1.6837
Batch 420: Loss = 1.8789
Batch 480: Loss = 1.7356
Batch 540: Loss = 1.7990
Batch 600: Loss = 1.6362
Batch 660: Loss = 1.7454
Batch 720: Loss = 1.5871
Batch 780: Loss = 1.5497
Batch 840: Loss = 1.6610
Batch 900: Loss = 1.7792
Batch 960: Loss = 1.6779
Batch 1020: Loss = 1.7251
Batch 1080: Loss = 1.7200
Batch 1140: Loss = 1.5005
Batch 1200: Loss = 1.6672
Batch 1260: Loss = 1.5673
Batch 1320: Loss = 1.5379
Batch 1380: Loss = 1.5641
Batch 1440: Loss = 1.4363
Batch 1500: Loss = 1.2190
  Train Loss: 1.7494 | Train Acc: 36.13% | Val Loss: 1.5711 | Val Acc: 43.07%
Epoch 2/10 LR 0.01 and batch_size 60
Batch 60: Loss = 1.4255
Batch 120: Loss = 1.5486
Batch 180: Loss = 1.5751
Batch 240: Loss = 1.5231
Batch 300: Loss = 1.3318
Batch 360: Loss = 1.4007
Batch 420: Loss = 1.3945
Batch 480: Loss = 1.5052
Batch 540: Loss = 1.5392
Batch 600: Loss = 1.6419
Batch 660: Loss = 1.3990
Batch 720: Loss = 1.2192
Batch 780: Loss = 1.3249
Batch 840: Loss = 1.5419
Batch 900: Loss = 1.5325
Batch 960: Loss = 1.4413
Batch 1020: Loss = 1.2304
Batch 1080: Loss = 1.2326
Batch 1140: Loss = 1.4242
Batch 1200: Loss = 1.0895
Batch 1260: Loss = 1.3515
Batch 1320: Loss = 1.4806
Batch 1380: Loss = 1.3548
Batch 1440: Loss = 1.2723
Batch 1500: Loss = 1.3347
  Train Loss: 1.4067 | Train Acc: 48.90% | Val Loss: 1.4954 | Val Acc: 46.98%
Epoch 3/10 LR 0.01 and batch_size 60
Batch 60: Loss = 1.3709
Batch 120: Loss = 1.3834
Batch 180: Loss = 1.4172
Batch 240: Loss = 1.4942
Batch 300: Loss = 1.3430
Batch 360: Loss = 1.2450
Batch 420: Loss = 1.2132
Batch 480: Loss = 1.1342
Batch 540: Loss = 1.2238
Batch 600: Loss = 1.4534
Batch 660: Loss = 1.3688
Batch 720: Loss = 1.3496
Batch 780: Loss = 1.1583
Batch 840: Loss = 1.2024
Batch 900: Loss = 1.0822
Batch 960: Loss = 1.2121
Batch 1020: Loss = 1.2348
Batch 1080: Loss = 1.3015
Batch 1140: Loss = 1.2455
Batch 1200: Loss = 1.1357
Batch 1260: Loss = 1.5616
Batch 1320: Loss = 1.2569
Batch 1380: Loss = 0.9981
Batch 1440: Loss = 1.3507
Batch 1500: Loss = 1.2244
  Train Loss: 1.2582 | Train Acc: 54.73% | Val Loss: 1.3241 | Val Acc: 51.79%
Epoch 4/10 LR 0.01 and batch_size 60
Batch 60: Loss = 1.2562
Batch 120: Loss = 1.1102
Batch 180: Loss = 0.9701
Batch 240: Loss = 1.2189
Batch 300: Loss = 1.2576
Batch 360: Loss = 1.1791
Batch 420: Loss = 0.9463
Batch 480: Loss = 1.0023
Batch 540: Loss = 1.3279
Batch 600: Loss = 1.2765
Batch 660: Loss = 1.5030
Batch 720: Loss = 0.9059
Batch 780: Loss = 1.2613
Batch 840: Loss = 1.0582
Batch 900: Loss = 1.0667
Batch 960: Loss = 1.0539
Batch 1020: Loss = 1.1035
Batch 1080: Loss = 0.9990
Batch 1140: Loss = 1.0005
Batch 1200: Loss = 1.1186
Batch 1260: Loss = 1.0590
Batch 1320: Loss = 1.0909
Batch 1380: Loss = 1.0684
Batch 1440: Loss = 1.5934
Batch 1500: Loss = 0.9939
  Train Loss: 1.1476 | Train Acc: 58.95% | Val Loss: 1.2426 | Val Acc: 56.00%
Epoch 5/10 LR 0.01 and batch_size 60
Batch 60: Loss = 1.0034
Batch 120: Loss = 1.1624
Batch 180: Loss = 1.1664
Batch 240: Loss = 1.0434
Batch 300: Loss = 1.1454
Batch 360: Loss = 0.9126
Batch 420: Loss = 1.1790
Batch 480: Loss = 1.3265
Batch 540: Loss = 0.9470
Batch 600: Loss = 0.9174
Batch 660: Loss = 1.0082
Batch 720: Loss = 1.0212
Batch 780: Loss = 1.1510
Batch 840: Loss = 1.1256
Batch 900: Loss = 0.8507
Batch 960: Loss = 0.9750
Batch 1020: Loss = 1.0600
Batch 1080: Loss = 1.2477
Batch 1140: Loss = 1.0890
Batch 1200: Loss = 1.2579
Batch 1260: Loss = 1.0281
Batch 1320: Loss = 1.2175
Batch 1380: Loss = 1.1383
Batch 1440: Loss = 1.0552
Batch 1500: Loss = 1.2662
  Train Loss: 1.0558 | Train Acc: 62.30% | Val Loss: 1.1117 | Val Acc: 60.47%
Epoch 6/10 LR 0.01 and batch_size 60
Batch 60: Loss = 0.9489
Batch 120: Loss = 0.8335
Batch 180: Loss = 1.1882
Batch 240: Loss = 0.8059
Batch 300: Loss = 0.9495
Batch 360: Loss = 1.1140
Batch 420: Loss = 0.9270
Batch 480: Loss = 1.2709
Batch 540: Loss = 1.0212
Batch 600: Loss = 0.8566
Batch 660: Loss = 0.9648
Batch 720: Loss = 1.2566
Batch 780: Loss = 0.8446
Batch 840: Loss = 1.0573
Batch 900: Loss = 1.2490
Batch 960: Loss = 0.7543
Batch 1020: Loss = 1.0487
Batch 1080: Loss = 0.9635
Batch 1140: Loss = 0.7682
Batch 1200: Loss = 0.9050
Batch 1260: Loss = 1.0558
Batch 1320: Loss = 0.8671
Batch 1380: Loss = 1.0753
Batch 1440: Loss = 0.8913
Batch 1500: Loss = 0.7813
  Train Loss: 0.9737 | Train Acc: 65.47% | Val Loss: 1.3667 | Val Acc: 54.26%
Epoch 7/10 LR 0.01 and batch_size 60
Batch 60: Loss = 0.9614
Batch 120: Loss = 0.7918
Batch 180: Loss = 0.9903
Batch 240: Loss = 0.8613
Batch 300: Loss = 0.6941
Batch 360: Loss = 0.9320
Batch 420: Loss = 0.7664
Batch 480: Loss = 0.8385
Batch 540: Loss = 0.8068
Batch 600: Loss = 0.6841
Batch 660: Loss = 0.8758
Batch 720: Loss = 1.0308
Batch 780: Loss = 0.8291
Batch 840: Loss = 0.8628
Batch 900: Loss = 0.7063
Batch 960: Loss = 1.0439
Batch 1020: Loss = 0.9234
Batch 1080: Loss = 0.6141
Batch 1140: Loss = 0.8658
Batch 1200: Loss = 0.6910
Batch 1260: Loss = 0.7968
Batch 1320: Loss = 0.8128
Batch 1380: Loss = 0.9544
Batch 1440: Loss = 0.9268
Batch 1500: Loss = 0.8851
  Train Loss: 0.8895 | Train Acc: 68.56% | Val Loss: 1.0669 | Val Acc: 62.45%
Epoch 8/10 LR 0.01 and batch_size 60
Batch 60: Loss = 0.6094
Batch 120: Loss = 0.9477
Batch 180: Loss = 0.7221
Batch 240: Loss = 0.6144
Batch 300: Loss = 1.0162
Batch 360: Loss = 0.8184
Batch 420: Loss = 0.6044
Batch 480: Loss = 1.0109
Batch 540: Loss = 1.1816
Batch 600: Loss = 1.2538
Batch 660: Loss = 0.8503
Batch 720: Loss = 0.7762
Batch 780: Loss = 0.6820
Batch 840: Loss = 0.6435
Batch 900: Loss = 0.6459
Batch 960: Loss = 0.6805
Batch 1020: Loss = 0.9930
Batch 1080: Loss = 1.0260
Batch 1140: Loss = 0.7250
Batch 1200: Loss = 0.7870
Batch 1260: Loss = 0.7051
Batch 1320: Loss = 0.9905
Batch 1380: Loss = 0.9832
Batch 1440: Loss = 0.6632
Batch 1500: Loss = 1.0682
  Train Loss: 0.8087 | Train Acc: 71.18% | Val Loss: 1.0960 | Val Acc: 61.84%
Epoch 9/10 LR 0.01 and batch_size 60
Batch 60: Loss = 0.8276
Batch 120: Loss = 0.7468
Batch 180: Loss = 0.7783
Batch 240: Loss = 0.6480
Batch 300: Loss = 0.8309
Batch 360: Loss = 0.5888
Batch 420: Loss = 0.7599
Batch 480: Loss = 0.7366
Batch 540: Loss = 0.6086
Batch 600: Loss = 0.5005
Batch 660: Loss = 0.7429
Batch 720: Loss = 0.5425
Batch 780: Loss = 0.4863
Batch 840: Loss = 0.5971
Batch 900: Loss = 0.9383
Batch 960: Loss = 0.7762
Batch 1020: Loss = 0.8274
Batch 1080: Loss = 0.6520
Batch 1140: Loss = 0.5783
Batch 1200: Loss = 0.6319
Batch 1260: Loss = 1.0433
Batch 1320: Loss = 0.6497
Batch 1380: Loss = 0.8401
Batch 1440: Loss = 0.8080
Batch 1500: Loss = 0.8446
  Train Loss: 0.7259 | Train Acc: 74.20% | Val Loss: 1.1991 | Val Acc: 60.45%
Epoch 10/10 LR 0.01 and batch_size 60
Batch 60: Loss = 0.5037
Batch 120: Loss = 0.5802
Batch 180: Loss = 0.6751
Batch 240: Loss = 0.8644
Batch 300: Loss = 0.7152
Batch 360: Loss = 0.3687
Batch 420: Loss = 0.5666
Batch 480: Loss = 0.6942
Batch 540: Loss = 0.9003
Batch 600: Loss = 0.5302
Batch 660: Loss = 0.4686
Batch 720: Loss = 0.7140
Batch 780: Loss = 0.6822
Batch 840: Loss = 0.6440
Batch 900: Loss = 0.7101
Batch 960: Loss = 0.5785
Batch 1020: Loss = 0.7077
Batch 1080: Loss = 0.7308
Batch 1140: Loss = 1.0197
Batch 1200: Loss = 0.6275
Batch 1260: Loss = 0.5699
Batch 1320: Loss = 0.7249
Batch 1380: Loss = 0.7139
Batch 1440: Loss = 0.7330
Batch 1500: Loss = 0.5805
  Train Loss: 0.6425 | Train Acc: 77.03% | Val Loss: 1.2463 | Val Acc: 60.46%
Final Test Loss: 1.0779, Test Acc: 62.34%
Epoch 1/10 LR 0.01 and batch_size 80
Batch 60: Loss = 2.9860
Batch 120: Loss = 1.8769
Batch 180: Loss = 2.0077
Batch 240: Loss = 2.0656
Batch 300: Loss = 1.9019
Batch 360: Loss = 1.7422
Batch 420: Loss = 1.6800
Batch 480: Loss = 1.6984
Batch 540: Loss = 1.7859
Batch 600: Loss = 1.5385
Batch 660: Loss = 1.7378
Batch 720: Loss = 1.5286
Batch 780: Loss = 1.4822
Batch 840: Loss = 1.4875
Batch 900: Loss = 1.4514
Batch 960: Loss = 1.4456
Batch 1020: Loss = 1.5609
Batch 1080: Loss = 1.3542
  Train Loss: 1.7822 | Train Acc: 34.94% | Val Loss: 1.7169 | Val Acc: 37.81%
Epoch 2/10 LR 0.01 and batch_size 80
Batch 60: Loss = 1.2482
Batch 120: Loss = 1.4156
Batch 180: Loss = 1.5181
Batch 240: Loss = 1.2997
Batch 300: Loss = 1.4075
Batch 360: Loss = 1.7159
Batch 420: Loss = 1.7233
Batch 480: Loss = 1.5017
Batch 540: Loss = 1.3064
Batch 600: Loss = 1.2985
Batch 660: Loss = 1.2622
Batch 720: Loss = 1.2901
Batch 780: Loss = 1.2222
Batch 840: Loss = 1.2420
Batch 900: Loss = 1.3820
Batch 960: Loss = 1.1323
Batch 1020: Loss = 1.1669
Batch 1080: Loss = 1.1344
  Train Loss: 1.3858 | Train Acc: 49.39% | Val Loss: 1.3729 | Val Acc: 50.32%
Epoch 3/10 LR 0.01 and batch_size 80
Batch 60: Loss = 1.3024
Batch 120: Loss = 1.2690
Batch 180: Loss = 1.4007
Batch 240: Loss = 1.2942
Batch 300: Loss = 1.2125
Batch 360: Loss = 1.2234
Batch 420: Loss = 1.2602
Batch 480: Loss = 1.1368
Batch 540: Loss = 1.2268
Batch 600: Loss = 1.2879
Batch 660: Loss = 1.3990
Batch 720: Loss = 1.0500
Batch 780: Loss = 1.4123
Batch 840: Loss = 1.0674
Batch 900: Loss = 1.4170
Batch 960: Loss = 1.4317
Batch 1020: Loss = 1.0596
Batch 1080: Loss = 1.1981
  Train Loss: 1.2393 | Train Acc: 55.33% | Val Loss: 1.3456 | Val Acc: 52.29%
Epoch 4/10 LR 0.01 and batch_size 80
Batch 60: Loss = 1.2387
Batch 120: Loss = 1.0054
Batch 180: Loss = 1.1164
Batch 240: Loss = 1.2376
Batch 300: Loss = 0.9407
Batch 360: Loss = 1.2303
Batch 420: Loss = 1.0971
Batch 480: Loss = 1.0806
Batch 540: Loss = 1.0317
Batch 600: Loss = 1.2020
Batch 660: Loss = 1.0194
Batch 720: Loss = 1.0309
Batch 780: Loss = 1.3979
Batch 840: Loss = 1.3166
Batch 900: Loss = 1.0448
Batch 960: Loss = 1.2020
Batch 1020: Loss = 1.2217
Batch 1080: Loss = 1.1815
  Train Loss: 1.1336 | Train Acc: 59.38% | Val Loss: 1.3539 | Val Acc: 52.45%
Epoch 5/10 LR 0.01 and batch_size 80
Batch 60: Loss = 0.9813
Batch 120: Loss = 1.1518
Batch 180: Loss = 1.0741
Batch 240: Loss = 0.8292
Batch 300: Loss = 0.8787
Batch 360: Loss = 0.8439
Batch 420: Loss = 0.9017
Batch 480: Loss = 1.0090
Batch 540: Loss = 0.9770
Batch 600: Loss = 1.0803
Batch 660: Loss = 0.9426
Batch 720: Loss = 0.9862
Batch 780: Loss = 1.0509
Batch 840: Loss = 0.9268
Batch 900: Loss = 1.0652
Batch 960: Loss = 1.0999
Batch 1020: Loss = 1.0092
Batch 1080: Loss = 0.9808
  Train Loss: 1.0457 | Train Acc: 62.66% | Val Loss: 1.2141 | Val Acc: 57.18%
Epoch 6/10 LR 0.01 and batch_size 80
Batch 60: Loss = 0.8925
Batch 120: Loss = 1.0068
Batch 180: Loss = 1.0519
Batch 240: Loss = 0.9401
Batch 300: Loss = 0.9461
Batch 360: Loss = 0.9605
Batch 420: Loss = 0.8207
Batch 480: Loss = 0.8790
Batch 540: Loss = 0.8237
Batch 600: Loss = 1.1602
Batch 660: Loss = 1.0941
Batch 720: Loss = 0.9090
Batch 780: Loss = 1.0423
Batch 840: Loss = 1.0161
Batch 900: Loss = 0.8215
Batch 960: Loss = 0.8822
Batch 1020: Loss = 0.9361
Batch 1080: Loss = 0.9002
  Train Loss: 0.9628 | Train Acc: 65.72% | Val Loss: 1.2659 | Val Acc: 56.63%
Epoch 7/10 LR 0.01 and batch_size 80
Batch 60: Loss = 0.8117
Batch 120: Loss = 0.8981
Batch 180: Loss = 0.9282
Batch 240: Loss = 1.0592
Batch 300: Loss = 0.9896
Batch 360: Loss = 0.9035
Batch 420: Loss = 1.0079
Batch 480: Loss = 0.8179
Batch 540: Loss = 0.8717
Batch 600: Loss = 0.8928
Batch 660: Loss = 0.7589
Batch 720: Loss = 1.1009
Batch 780: Loss = 1.0145
Batch 840: Loss = 0.8909
Batch 900: Loss = 0.8700
Batch 960: Loss = 0.9035
Batch 1020: Loss = 0.7929
Batch 1080: Loss = 0.8560
  Train Loss: 0.8761 | Train Acc: 68.98% | Val Loss: 1.1420 | Val Acc: 59.92%
Epoch 8/10 LR 0.01 and batch_size 80
Batch 60: Loss = 0.9316
Batch 120: Loss = 0.7866
Batch 180: Loss = 0.8887
Batch 240: Loss = 0.5928
Batch 300: Loss = 0.5503
Batch 360: Loss = 0.8531
Batch 420: Loss = 0.7124
Batch 480: Loss = 0.5793
Batch 540: Loss = 0.9420
Batch 600: Loss = 1.0496
Batch 660: Loss = 0.7979
Batch 720: Loss = 0.7133
Batch 780: Loss = 0.8853
Batch 840: Loss = 0.9658
Batch 900: Loss = 0.7917
Batch 960: Loss = 0.6637
Batch 1020: Loss = 0.7835
Batch 1080: Loss = 0.7758
  Train Loss: 0.7934 | Train Acc: 71.89% | Val Loss: 1.1000 | Val Acc: 62.05%
Epoch 9/10 LR 0.01 and batch_size 80
Batch 60: Loss = 0.6067
Batch 120: Loss = 0.7962
Batch 180: Loss = 0.8069
Batch 240: Loss = 0.5615
Batch 300: Loss = 0.7771
Batch 360: Loss = 0.9011
Batch 420: Loss = 0.8384
Batch 480: Loss = 0.8650
Batch 540: Loss = 0.6021
Batch 600: Loss = 0.7933
Batch 660: Loss = 0.5945
Batch 720: Loss = 0.8354
Batch 780: Loss = 0.5954
Batch 840: Loss = 0.7203
Batch 900: Loss = 0.6527
Batch 960: Loss = 0.6747
Batch 1020: Loss = 0.8175
Batch 1080: Loss = 0.7345
  Train Loss: 0.6998 | Train Acc: 75.16% | Val Loss: 1.1563 | Val Acc: 61.42%
Epoch 10/10 LR 0.01 and batch_size 80
Batch 60: Loss = 0.6030
Batch 120: Loss = 0.7085
Batch 180: Loss = 0.5510
Batch 240: Loss = 0.5765
Batch 300: Loss = 0.6849
Batch 360: Loss = 0.5580
Batch 420: Loss = 0.5260
Batch 480: Loss = 0.7236
Batch 540: Loss = 0.6594
Batch 600: Loss = 0.7138
Batch 660: Loss = 0.7016
Batch 720: Loss = 0.6713
Batch 780: Loss = 0.6466
Batch 840: Loss = 0.5532
Batch 900: Loss = 0.6923
Batch 960: Loss = 0.6654
Batch 1020: Loss = 0.5666
Batch 1080: Loss = 0.6889
  Train Loss: 0.6108 | Train Acc: 78.22% | Val Loss: 1.4370 | Val Acc: 56.96%
Final Test Loss: 1.1097, Test Acc: 61.84%
Epoch 1/10 LR 0.01 and batch_size 100
Batch 60: Loss = 2.0888
Batch 120: Loss = 1.9939
Batch 180: Loss = 1.8417
Batch 240: Loss = 1.9485
Batch 300: Loss = 1.8726
Batch 360: Loss = 1.8388
Batch 420: Loss = 1.6867
Batch 480: Loss = 1.6737
Batch 540: Loss = 1.4880
Batch 600: Loss = 1.5934
Batch 660: Loss = 1.7473
Batch 720: Loss = 1.6707
Batch 780: Loss = 1.5455
Batch 840: Loss = 1.3991
Batch 900: Loss = 1.4969
  Train Loss: 1.7961 | Train Acc: 34.89% | Val Loss: 1.6671 | Val Acc: 39.63%
Epoch 2/10 LR 0.01 and batch_size 100
Batch 60: Loss = 1.3958
Batch 120: Loss = 1.4206
Batch 180: Loss = 1.5036
Batch 240: Loss = 1.3848
Batch 300: Loss = 1.3265
Batch 360: Loss = 1.4150
Batch 420: Loss = 1.4613
Batch 480: Loss = 1.3448
Batch 540: Loss = 1.4397
Batch 600: Loss = 1.3749
Batch 660: Loss = 1.3130
Batch 720: Loss = 1.2893
Batch 780: Loss = 1.3642
Batch 840: Loss = 1.2449
Batch 900: Loss = 1.0838
  Train Loss: 1.4192 | Train Acc: 47.94% | Val Loss: 1.4325 | Val Acc: 48.77%
Epoch 3/10 LR 0.01 and batch_size 100
Batch 60: Loss = 1.4528
Batch 120: Loss = 1.2181
Batch 180: Loss = 1.3325
Batch 240: Loss = 1.1465
Batch 300: Loss = 1.3356
Batch 360: Loss = 1.3169
Batch 420: Loss = 1.3477
Batch 480: Loss = 1.2833
Batch 540: Loss = 1.3120
Batch 600: Loss = 0.9947
Batch 660: Loss = 1.1480
Batch 720: Loss = 1.3959
Batch 780: Loss = 1.2935
Batch 840: Loss = 1.3633
Batch 900: Loss = 0.9662
  Train Loss: 1.2542 | Train Acc: 54.34% | Val Loss: 1.3327 | Val Acc: 52.81%
Epoch 4/10 LR 0.01 and batch_size 100
Batch 60: Loss = 1.1806
Batch 120: Loss = 1.2841
Batch 180: Loss = 1.1467
Batch 240: Loss = 1.2536
Batch 300: Loss = 1.1606
Batch 360: Loss = 0.9751
Batch 420: Loss = 1.0785
Batch 480: Loss = 1.2437
Batch 540: Loss = 1.2032
Batch 600: Loss = 0.8882
Batch 660: Loss = 1.0633
Batch 720: Loss = 0.9165
Batch 780: Loss = 1.1281
Batch 840: Loss = 1.0928
Batch 900: Loss = 0.9335
  Train Loss: 1.1472 | Train Acc: 58.68% | Val Loss: 1.2352 | Val Acc: 55.55%
Epoch 5/10 LR 0.01 and batch_size 100
Batch 60: Loss = 1.2657
Batch 120: Loss = 1.0949
Batch 180: Loss = 1.0032
Batch 240: Loss = 0.9824
Batch 300: Loss = 1.1148
Batch 360: Loss = 1.1129
Batch 420: Loss = 1.0902
Batch 480: Loss = 0.9577
Batch 540: Loss = 0.9349
Batch 600: Loss = 1.0777
Batch 660: Loss = 0.9690
Batch 720: Loss = 0.9752
Batch 780: Loss = 0.8658
Batch 840: Loss = 1.0149
Batch 900: Loss = 0.9906
  Train Loss: 1.0583 | Train Acc: 62.07% | Val Loss: 1.2915 | Val Acc: 54.25%
Epoch 6/10 LR 0.01 and batch_size 100
Batch 60: Loss = 1.1419
Batch 120: Loss = 0.8825
Batch 180: Loss = 1.0884
Batch 240: Loss = 0.9608
Batch 300: Loss = 1.1367
Batch 360: Loss = 1.0476
Batch 420: Loss = 0.9039
Batch 480: Loss = 0.9270
Batch 540: Loss = 1.0528
Batch 600: Loss = 1.0753
Batch 660: Loss = 1.0079
Batch 720: Loss = 1.0970
Batch 780: Loss = 0.8291
Batch 840: Loss = 0.8839
Batch 900: Loss = 0.7940
  Train Loss: 0.9707 | Train Acc: 65.33% | Val Loss: 1.1931 | Val Acc: 58.88%
Epoch 7/10 LR 0.01 and batch_size 100
Batch 60: Loss = 0.9144
Batch 120: Loss = 0.8251
Batch 180: Loss = 0.7626
Batch 240: Loss = 0.7554
Batch 300: Loss = 0.7871
Batch 360: Loss = 0.7432
Batch 420: Loss = 0.8617
Batch 480: Loss = 1.0020
Batch 540: Loss = 0.9413
Batch 600: Loss = 0.8808
Batch 660: Loss = 0.8876
Batch 720: Loss = 0.8067
Batch 780: Loss = 0.7975
Batch 840: Loss = 0.9662
Batch 900: Loss = 0.9509
  Train Loss: 0.8922 | Train Acc: 68.14% | Val Loss: 1.2176 | Val Acc: 57.85%
Epoch 8/10 LR 0.01 and batch_size 100
Batch 60: Loss = 0.6732
Batch 120: Loss = 0.7470
Batch 180: Loss = 0.6930
Batch 240: Loss = 0.8413
Batch 300: Loss = 0.6625
Batch 360: Loss = 0.9569
Batch 420: Loss = 0.8800
Batch 480: Loss = 0.6537
Batch 540: Loss = 0.6493
Batch 600: Loss = 0.9803
Batch 660: Loss = 0.9684
Batch 720: Loss = 0.6862
Batch 780: Loss = 0.8547
Batch 840: Loss = 1.0786
Batch 900: Loss = 0.8180
  Train Loss: 0.8105 | Train Acc: 71.26% | Val Loss: 1.1827 | Val Acc: 60.44%
Epoch 9/10 LR 0.01 and batch_size 100
Batch 60: Loss = 0.5411
Batch 120: Loss = 0.6609
Batch 180: Loss = 0.5057
Batch 240: Loss = 0.8310
Batch 300: Loss = 0.5901
Batch 360: Loss = 0.8803
Batch 420: Loss = 0.6067
Batch 480: Loss = 0.8086
Batch 540: Loss = 0.9479
Batch 600: Loss = 0.8484
Batch 660: Loss = 0.5880
Batch 720: Loss = 0.5745
Batch 780: Loss = 0.8643
Batch 840: Loss = 0.6987
Batch 900: Loss = 0.6763
  Train Loss: 0.7238 | Train Acc: 74.34% | Val Loss: 1.1437 | Val Acc: 61.78%
Epoch 10/10 LR 0.01 and batch_size 100
Batch 60: Loss = 0.5637
Batch 120: Loss = 0.5491
Batch 180: Loss = 0.5125
Batch 240: Loss = 0.6979
Batch 300: Loss = 0.7050
Batch 360: Loss = 0.6112
Batch 420: Loss = 0.7025
Batch 480: Loss = 0.7338
Batch 540: Loss = 0.5563
Batch 600: Loss = 0.6302
Batch 660: Loss = 0.5135
Batch 720: Loss = 0.7944
Batch 780: Loss = 0.6850
Batch 840: Loss = 0.7075
Batch 900: Loss = 0.6663
  Train Loss: 0.6335 | Train Acc: 77.48% | Val Loss: 1.1871 | Val Acc: 60.95%
Final Test Loss: 1.1514, Test Acc: 61.67%
Epoch 1/10 LR 0.1 and batch_size 40
Batch 60: Loss = 2.2981
Batch 120: Loss = 2.2966
Batch 180: Loss = 2.1788
Batch 240: Loss = 2.1431
Batch 300: Loss = 2.0578
Batch 360: Loss = 2.0425
Batch 420: Loss = 2.1486
Batch 480: Loss = 2.0793
Batch 540: Loss = 2.0314
Batch 600: Loss = 2.1613
Batch 660: Loss = 2.0714
Batch 720: Loss = 2.0715
Batch 780: Loss = 1.9419
Batch 840: Loss = 2.2762
Batch 900: Loss = 1.9201
Batch 960: Loss = 1.9360
Batch 1020: Loss = 2.1429
Batch 1080: Loss = 2.0299
Batch 1140: Loss = 1.9347
Batch 1200: Loss = 1.9883
Batch 1260: Loss = 1.8576
Batch 1320: Loss = 1.9558
Batch 1380: Loss = 2.1060
Batch 1440: Loss = 1.9707
Batch 1500: Loss = 2.0974
Batch 1560: Loss = 2.4406
Batch 1620: Loss = 2.0722
Batch 1680: Loss = 2.1408
Batch 1740: Loss = 1.8801
Batch 1800: Loss = 1.8300
Batch 1860: Loss = 2.0042
Batch 1920: Loss = 2.3310
Batch 1980: Loss = 1.9479
Batch 2040: Loss = 1.9953
Batch 2100: Loss = 2.2356
Batch 2160: Loss = 1.9335
Batch 2220: Loss = 1.9875
  Train Loss: 2.1292 | Train Acc: 17.23% | Val Loss: 2.0052 | Val Acc: 17.58%
Epoch 2/10 LR 0.1 and batch_size 40
Batch 60: Loss = 2.1974
Batch 120: Loss = 1.9841
Batch 180: Loss = 1.9986
Batch 240: Loss = 1.8968
Batch 300: Loss = 2.0801
Batch 360: Loss = 2.1328
Batch 420: Loss = 2.1879
Batch 480: Loss = 1.8862
Batch 540: Loss = 2.0054
Batch 600: Loss = 2.1147
Batch 660: Loss = 1.7715
Batch 720: Loss = 1.9899
Batch 780: Loss = 1.9904
Batch 840: Loss = 1.9458
Batch 900: Loss = 2.0392
Batch 960: Loss = 1.9952
Batch 1020: Loss = 1.9338
Batch 1080: Loss = 2.2046
Batch 1140: Loss = 1.9907
Batch 1200: Loss = 1.9696
Batch 1260: Loss = 2.0968
Batch 1320: Loss = 1.9333
Batch 1380: Loss = 1.9100
Batch 1440: Loss = 1.9734
Batch 1500: Loss = 2.0171
Batch 1560: Loss = 1.8805
Batch 1620: Loss = 1.9083
Batch 1680: Loss = 1.9447
Batch 1740: Loss = 2.2111
Batch 1800: Loss = 1.9420
Batch 1860: Loss = 2.0366
Batch 1920: Loss = 1.8843
Batch 1980: Loss = 2.0700
Batch 2040: Loss = 2.1432
Batch 2100: Loss = 1.8333
Batch 2160: Loss = 1.7985
Batch 2220: Loss = 1.9584
  Train Loss: 1.9833 | Train Acc: 18.99% | Val Loss: 2.0634 | Val Acc: 17.05%
Epoch 3/10 LR 0.1 and batch_size 40
Batch 60: Loss = 1.9844
Batch 120: Loss = 2.1037
Batch 180: Loss = 2.0204
Batch 240: Loss = 1.9429
Batch 300: Loss = 1.9218
Batch 360: Loss = 1.7757
Batch 420: Loss = 1.9899
Batch 480: Loss = 1.9096
Batch 540: Loss = 1.9689
Batch 600: Loss = 1.9984
Batch 660: Loss = 1.9144
Batch 720: Loss = 1.9684
Batch 780: Loss = 2.0171
Batch 840: Loss = 2.0776
Batch 900: Loss = 2.1352
Batch 960: Loss = 2.1632
Batch 1020: Loss = 1.9007
Batch 1080: Loss = 1.8640
Batch 1140: Loss = 1.9475
Batch 1200: Loss = 1.8339
Batch 1260: Loss = 1.6219
Batch 1320: Loss = 1.8837
Batch 1380: Loss = 1.9328
Batch 1440: Loss = 1.7854
Batch 1500: Loss = 1.9135
Batch 1560: Loss = 1.9327
Batch 1620: Loss = 1.9042
Batch 1680: Loss = 1.8108
Batch 1740: Loss = 1.8827
Batch 1800: Loss = 1.8597
Batch 1860: Loss = 1.8622
Batch 1920: Loss = 1.9098
Batch 1980: Loss = 1.9698
Batch 2040: Loss = 1.9476
Batch 2100: Loss = 1.8508
Batch 2160: Loss = 1.9870
Batch 2220: Loss = 1.9931
  Train Loss: 1.9365 | Train Acc: 22.03% | Val Loss: 1.9043 | Val Acc: 23.52%
Epoch 4/10 LR 0.1 and batch_size 40
Batch 60: Loss = 2.0211
Batch 120: Loss = 1.7906
Batch 180: Loss = 2.0337
Batch 240: Loss = 1.8755
Batch 300: Loss = 2.1918
Batch 360: Loss = 1.8806
Batch 420: Loss = 1.7055
Batch 480: Loss = 1.8533
Batch 540: Loss = 2.0383
Batch 600: Loss = 2.0054
Batch 660: Loss = 1.8530
Batch 720: Loss = 1.9657
Batch 780: Loss = 1.8432
Batch 840: Loss = 1.8561
Batch 900: Loss = 1.8751
Batch 960: Loss = 2.1540
Batch 1020: Loss = 1.9925
Batch 1080: Loss = 1.9711
Batch 1140: Loss = 1.7710
Batch 1200: Loss = 1.8796
Batch 1260: Loss = 1.7636
Batch 1320: Loss = 2.0004
Batch 1380: Loss = 1.9519
Batch 1440: Loss = 1.8528
Batch 1500: Loss = 1.8561
Batch 1560: Loss = 1.9254
Batch 1620: Loss = 1.9865
Batch 1680: Loss = 1.6521
Batch 1740: Loss = 1.9493
Batch 1800: Loss = 1.6503
Batch 1860: Loss = 1.7902
Batch 1920: Loss = 2.0937
Batch 1980: Loss = 1.6713
Batch 2040: Loss = 1.9012
Batch 2100: Loss = 1.7001
Batch 2160: Loss = 1.7494
Batch 2220: Loss = 1.7227
  Train Loss: 1.8950 | Train Acc: 23.90% | Val Loss: 1.8160 | Val Acc: 26.90%
Epoch 5/10 LR 0.1 and batch_size 40
Batch 60: Loss = 1.9964
Batch 120: Loss = 2.1373
Batch 180: Loss = 1.7166
Batch 240: Loss = 2.0935
Batch 300: Loss = 1.8706
Batch 360: Loss = 1.8021
Batch 420: Loss = 1.8061
Batch 480: Loss = 1.9102
Batch 540: Loss = 1.9230
Batch 600: Loss = 1.7575
Batch 660: Loss = 1.6886
Batch 720: Loss = 1.8022
Batch 780: Loss = 1.7484
Batch 840: Loss = 1.7647
Batch 900: Loss = 1.9132
Batch 960: Loss = 2.0986
Batch 1020: Loss = 1.8717
Batch 1080: Loss = 1.8575
Batch 1140: Loss = 1.9442
Batch 1200: Loss = 1.7900
Batch 1260: Loss = 1.7512
Batch 1320: Loss = 1.8384
Batch 1380: Loss = 1.9315
Batch 1440: Loss = 1.8469
Batch 1500: Loss = 1.7507
Batch 1560: Loss = 2.0288
Batch 1620: Loss = 1.7375
Batch 1680: Loss = 1.8090
Batch 1740: Loss = 1.7149
Batch 1800: Loss = 1.7362
Batch 1860: Loss = 2.0189
Batch 1920: Loss = 1.7524
Batch 1980: Loss = 1.8700
Batch 2040: Loss = 2.0718
Batch 2100: Loss = 2.0468
Batch 2160: Loss = 1.7484
Batch 2220: Loss = 1.6889
  Train Loss: 1.8525 | Train Acc: 26.31% | Val Loss: 1.8134 | Val Acc: 25.60%
Epoch 6/10 LR 0.1 and batch_size 40
Batch 60: Loss = 2.0075
Batch 120: Loss = 1.8499
Batch 180: Loss = 1.8718
Batch 240: Loss = 1.7785
Batch 300: Loss = 2.0617
Batch 360: Loss = 1.7393
Batch 420: Loss = 1.8235
Batch 480: Loss = 1.7308
Batch 540: Loss = 1.7556
Batch 600: Loss = 1.8725
Batch 660: Loss = 1.6927
Batch 720: Loss = 1.6491
Batch 780: Loss = 1.7721
Batch 840: Loss = 1.9691
Batch 900: Loss = 1.8376
Batch 960: Loss = 1.7566
Batch 1020: Loss = 1.5675
Batch 1080: Loss = 1.7519
Batch 1140: Loss = 1.8412
Batch 1200: Loss = 1.5782
Batch 1260: Loss = 1.6318
Batch 1320: Loss = 1.7567
Batch 1380: Loss = 1.7387
Batch 1440: Loss = 1.9273
Batch 1500: Loss = 1.6607
Batch 1560: Loss = 1.9124
Batch 1620: Loss = 1.8909
Batch 1680: Loss = 1.7388
Batch 1740: Loss = 1.7819
Batch 1800: Loss = 2.1074
Batch 1860: Loss = 1.6622
Batch 1920: Loss = 1.5383
Batch 1980: Loss = 1.7998
Batch 2040: Loss = 1.8874
Batch 2100: Loss = 2.1529
Batch 2160: Loss = 1.7053
Batch 2220: Loss = 1.9361
  Train Loss: 1.8158 | Train Acc: 27.52% | Val Loss: 1.7732 | Val Acc: 30.91%
Epoch 7/10 LR 0.1 and batch_size 40
Batch 60: Loss = 1.9739
Batch 120: Loss = 1.9764
Batch 180: Loss = 1.5775
Batch 240: Loss = 1.7635
Batch 300: Loss = 1.7405
Batch 360: Loss = 1.8316
Batch 420: Loss = 1.5753
Batch 480: Loss = 1.9314
Batch 540: Loss = 1.7162
Batch 600: Loss = 1.6292
Batch 660: Loss = 2.4116
Batch 720: Loss = 1.8717
Batch 780: Loss = 1.7230
Batch 840: Loss = 1.7172
Batch 900: Loss = 1.6897
Batch 960: Loss = 1.8124
Batch 1020: Loss = 1.8159
Batch 1080: Loss = 1.6467
Batch 1140: Loss = 1.9960
Batch 1200: Loss = 1.6190
Batch 1260: Loss = 2.0733
Batch 1320: Loss = 1.5012
Batch 1380: Loss = 1.6733
Batch 1440: Loss = 1.8195
Batch 1500: Loss = 1.7082
Batch 1560: Loss = 1.6393
Batch 1620: Loss = 1.5140
Batch 1680: Loss = 1.8625
Batch 1740: Loss = 1.6312
Batch 1800: Loss = 1.8039
Batch 1860: Loss = 1.8800
Batch 1920: Loss = 1.9405
Batch 1980: Loss = 1.7260
Batch 2040: Loss = 1.6642
Batch 2100: Loss = 1.6129
Batch 2160: Loss = 1.6556
Batch 2220: Loss = 1.8562
  Train Loss: 1.7801 | Train Acc: 28.91% | Val Loss: 1.9004 | Val Acc: 24.20%
Epoch 8/10 LR 0.1 and batch_size 40