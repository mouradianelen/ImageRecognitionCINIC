Batch 60: Loss = 0.9291
Batch 120: Loss = 0.4493
Batch 180: Loss = 0.5794
Batch 240: Loss = 1.3256
Batch 300: Loss = 0.8108
Batch 360: Loss = 1.5084
Batch 420: Loss = 1.0645
Batch 480: Loss = 1.0141
Batch 540: Loss = 0.9636
Batch 600: Loss = 0.6672
Batch 660: Loss = 1.0294
Batch 720: Loss = 1.2136
Batch 780: Loss = 1.1301
Batch 840: Loss = 1.1084
Batch 900: Loss = 0.7702
Batch 960: Loss = 1.0035
Batch 1020: Loss = 0.9241
Batch 1080: Loss = 0.8873
Batch 1140: Loss = 0.6199
Batch 1200: Loss = 0.9890
Batch 1260: Loss = 0.9104
Batch 1320: Loss = 0.8828
Batch 1380: Loss = 1.0071
Batch 1440: Loss = 0.5641
Batch 1500: Loss = 0.9216
Batch 1560: Loss = 0.8917
Batch 1620: Loss = 0.8933
Batch 1680: Loss = 0.6472
Batch 1740: Loss = 1.6805
Batch 1800: Loss = 0.4329
Batch 1860: Loss = 1.2646
Batch 1920: Loss = 1.0269
Batch 1980: Loss = 1.0142
Batch 2040: Loss = 1.5526
Batch 2100: Loss = 1.0324
Batch 2160: Loss = 1.0306
Batch 2220: Loss = 0.6095
Batch 2280: Loss = 0.7108
Batch 2340: Loss = 0.7981
Batch 2400: Loss = 0.7399
Batch 2460: Loss = 0.6576
Batch 2520: Loss = 1.0119
Batch 2580: Loss = 0.8848
Batch 2640: Loss = 0.6679
Batch 2700: Loss = 0.7953
Batch 2760: Loss = 0.8709
Batch 2820: Loss = 1.0189
Batch 2880: Loss = 1.0348
Batch 2940: Loss = 1.1829
Batch 3000: Loss = 0.7362
Batch 3060: Loss = 0.8797
Batch 3120: Loss = 0.5379
Batch 3180: Loss = 1.3743
Batch 3240: Loss = 0.8043
Batch 3300: Loss = 0.8331
Batch 3360: Loss = 0.9428
Batch 3420: Loss = 1.0530
Batch 3480: Loss = 0.9763
Batch 3540: Loss = 0.9830
Batch 3600: Loss = 1.0729
Batch 3660: Loss = 0.5771
Batch 3720: Loss = 1.3101
Batch 3780: Loss = 0.4451
Batch 3840: Loss = 0.4742
Batch 3900: Loss = 0.6080
Batch 3960: Loss = 1.0686
Batch 4020: Loss = 1.0666
Batch 4080: Loss = 0.7575
Batch 4140: Loss = 0.8971
Batch 4200: Loss = 0.7882
Batch 4260: Loss = 0.7103
Batch 4320: Loss = 0.4499
Batch 4380: Loss = 0.7419
Batch 4440: Loss = 1.5427
Batch 4500: Loss = 0.8309
Additional Epoch [1/8] Train Loss: 0.8942 | Train Acc: 68.27% Val Loss: 1.2404 | Val Acc: 57.16%
Batch 60: Loss = 1.0970
Batch 120: Loss = 1.1533
Batch 180: Loss = 0.9651
Batch 240: Loss = 0.4433
Batch 300: Loss = 0.6897
Batch 360: Loss = 0.9503
Batch 420: Loss = 0.9102
Batch 480: Loss = 0.5560
Batch 540: Loss = 0.4906
Batch 600: Loss = 0.6001
Batch 660: Loss = 0.6491
Batch 720: Loss = 0.9449
Batch 780: Loss = 0.3744
Batch 840: Loss = 0.9867
Batch 900: Loss = 0.5581
Batch 960: Loss = 0.8120
Batch 1020: Loss = 0.4872
Batch 1080: Loss = 0.9962
Batch 1140: Loss = 0.4594
Batch 1200: Loss = 0.6245
Batch 1260: Loss = 0.5170
Batch 1320: Loss = 0.9865
Batch 1380: Loss = 1.1255
Batch 1440: Loss = 0.7072
Batch 1500: Loss = 0.6841
Batch 1560: Loss = 0.6940
Batch 1620: Loss = 0.7634
Batch 1680: Loss = 0.7115
Batch 1740: Loss = 0.7670
Batch 1800: Loss = 0.7094
Batch 1860: Loss = 0.6031
Batch 1920: Loss = 1.0882
Batch 1980: Loss = 0.7627
Batch 2040: Loss = 1.1078
Batch 2100: Loss = 1.1116
Batch 2160: Loss = 0.9349
Batch 2220: Loss = 0.5436
Batch 2280: Loss = 0.9542
Batch 2340: Loss = 0.8203
Batch 2400: Loss = 0.4597
Batch 2460: Loss = 0.9512
Batch 2520: Loss = 0.3523
Batch 2580: Loss = 0.8458
Batch 2640: Loss = 0.6220
Batch 2700: Loss = 0.5744
Batch 2760: Loss = 0.7082
Batch 2820: Loss = 1.1086
Batch 2880: Loss = 0.5220
Batch 2940: Loss = 0.9845
Batch 3000: Loss = 0.5635
Batch 3060: Loss = 0.8405
Batch 3120: Loss = 0.6586
Batch 3180: Loss = 0.8650
Batch 3240: Loss = 0.6826
Batch 3300: Loss = 0.6661
Batch 3360: Loss = 0.7859
Batch 3420: Loss = 1.3331
Batch 3480: Loss = 0.5746
Batch 3540: Loss = 0.7921
Batch 3600: Loss = 0.6153
Batch 3660: Loss = 1.1295
Batch 3720: Loss = 0.4612
Batch 3780: Loss = 0.8596
Batch 3840: Loss = 1.1308
Batch 3900: Loss = 0.7470
Batch 3960: Loss = 0.5146
Batch 4020: Loss = 0.7939
Batch 4080: Loss = 1.1286
Batch 4140: Loss = 0.9314
Batch 4200: Loss = 0.2376
Batch 4260: Loss = 0.5203
Batch 4320: Loss = 0.9305
Batch 4380: Loss = 0.9654
Batch 4440: Loss = 0.5219
Batch 4500: Loss = 0.9503
Additional Epoch [2/8] Train Loss: 0.8107 | Train Acc: 71.20% Val Loss: 1.0489 | Val Acc: 63.47%
Batch 60: Loss = 0.6025
Batch 120: Loss = 0.8529
Batch 180: Loss = 0.4833
Batch 240: Loss = 0.6667
Batch 300: Loss = 0.5616
Batch 360: Loss = 0.8821
Batch 420: Loss = 0.6710
Batch 480: Loss = 0.4583
Batch 540: Loss = 0.7324
Batch 600: Loss = 0.8077
Batch 660: Loss = 0.9015
Batch 720: Loss = 0.5366
Batch 780: Loss = 0.7383
Batch 840: Loss = 1.1053
Batch 900: Loss = 0.2638
Batch 960: Loss = 0.9567
Batch 1020: Loss = 0.6859
Batch 1080: Loss = 0.7138
Batch 1140: Loss = 0.6478
Batch 1200: Loss = 0.5649
Batch 1260: Loss = 0.8840
Batch 1320: Loss = 0.8947
Batch 1380: Loss = 1.4851
Batch 1440: Loss = 0.4867
Batch 1500: Loss = 0.5210
Batch 1560: Loss = 1.1671
Batch 1620: Loss = 0.3549
Batch 1680: Loss = 0.3556
Batch 1740: Loss = 0.7759
Batch 1800: Loss = 0.3926
Batch 1860: Loss = 0.5040
Batch 1920: Loss = 0.7749
Batch 1980: Loss = 0.5124
Batch 2040: Loss = 0.5525
Batch 2100: Loss = 1.2947
Batch 2160: Loss = 0.7486
Batch 2220: Loss = 0.7197
Batch 2280: Loss = 0.5013
Batch 2340: Loss = 0.8568
Batch 2400: Loss = 0.7545
Batch 2460: Loss = 0.4964
Batch 2520: Loss = 0.3411
Batch 2580: Loss = 0.7018
Batch 2640: Loss = 0.4928
Batch 2700: Loss = 0.7611
Batch 2760: Loss = 1.0497
Batch 2820: Loss = 1.2287
Batch 2880: Loss = 0.7005
Batch 2940: Loss = 0.7844
Batch 3000: Loss = 0.6782
Batch 3060: Loss = 1.2054
Batch 3120: Loss = 0.4222
Batch 3180: Loss = 1.0835
Batch 3240: Loss = 0.7249
Batch 3300: Loss = 0.7878
Batch 3360: Loss = 0.2469
Batch 3420: Loss = 0.6107
Batch 3480: Loss = 0.8719
Batch 3540: Loss = 0.4946
Batch 3600: Loss = 0.4337
Batch 3660: Loss = 0.3303
Batch 3720: Loss = 1.2555
Batch 3780: Loss = 1.0359
Batch 3840: Loss = 0.6361
Batch 3900: Loss = 0.6910
Batch 3960: Loss = 0.4118
Batch 4020: Loss = 0.9022
Batch 4080: Loss = 0.8481
Batch 4140: Loss = 0.9449
Batch 4200: Loss = 0.6073
Batch 4260: Loss = 1.1342
Batch 4320: Loss = 0.4547
Batch 4380: Loss = 0.5627
Batch 4440: Loss = 0.7717
Batch 4500: Loss = 0.9988
Additional Epoch [3/8] Train Loss: 0.7200 | Train Acc: 74.57% Val Loss: 1.0660 | Val Acc: 63.87%
Batch 60: Loss = 0.8945
Batch 120: Loss = 0.4548
Batch 180: Loss = 0.4383
Batch 240: Loss = 0.4018
Batch 300: Loss = 0.4646
Batch 360: Loss = 0.6424
Batch 420: Loss = 0.4424
Batch 480: Loss = 0.5157
Batch 540: Loss = 0.8904
Batch 600: Loss = 1.0170
Batch 660: Loss = 0.7455
Batch 720: Loss = 0.6896
Batch 780: Loss = 0.6707
Batch 840: Loss = 0.5233
Batch 900: Loss = 0.6785
Batch 960: Loss = 0.5383
Batch 1020: Loss = 0.7913
Batch 1080: Loss = 0.9099
Batch 1140: Loss = 0.6898
Batch 1200: Loss = 0.3639
Batch 1260: Loss = 0.6148
Batch 1320: Loss = 0.7314
Batch 1380: Loss = 0.6951
Batch 1440: Loss = 0.3721
Batch 1500: Loss = 0.6102
Batch 1560: Loss = 0.4537
Batch 1620: Loss = 0.5377
Batch 1680: Loss = 0.3812
Batch 1740: Loss = 0.7455
Batch 1800: Loss = 0.3830
Batch 1860: Loss = 0.4407
Batch 1920: Loss = 0.3032
Batch 1980: Loss = 0.6754
Batch 2040: Loss = 0.8690
Batch 2100: Loss = 1.2007
Batch 2160: Loss = 0.4254
Batch 2220: Loss = 0.2987
Batch 2280: Loss = 0.5374
Batch 2340: Loss = 0.4478
Batch 2400: Loss = 0.4137
Batch 2460: Loss = 0.4843
Batch 2520: Loss = 0.6366
Batch 2580: Loss = 0.8768
Batch 2640: Loss = 0.4741
Batch 2700: Loss = 0.7805
Batch 2760: Loss = 0.6914
Batch 2820: Loss = 0.5345
Batch 2880: Loss = 0.6457
Batch 2940: Loss = 0.6955
Batch 3000: Loss = 0.9023
Batch 3060: Loss = 0.5624
Batch 3120: Loss = 0.5163
Batch 3180: Loss = 0.6299
Batch 3240: Loss = 0.7812
Batch 3300: Loss = 0.4809
Batch 3360: Loss = 0.6317
Batch 3420: Loss = 0.5179
Batch 3480: Loss = 0.8346
Batch 3540: Loss = 0.5300
Batch 3600: Loss = 0.3864
Batch 3660: Loss = 0.6500
Batch 3720: Loss = 0.9372
Batch 3780: Loss = 0.6508
Batch 3840: Loss = 0.9430
Batch 3900: Loss = 0.8743
Batch 3960: Loss = 0.5300
Batch 4020: Loss = 0.6066
Batch 4080: Loss = 0.4733
Batch 4140: Loss = 0.4653
Batch 4200: Loss = 1.0956
Batch 4260: Loss = 0.7013
Batch 4320: Loss = 0.8960
Batch 4380: Loss = 0.7833
Batch 4440: Loss = 0.9256
Batch 4500: Loss = 0.5783
Additional Epoch [4/8] Train Loss: 0.6366 | Train Acc: 77.42% Val Loss: 1.0822 | Val Acc: 64.13%
Batch 60: Loss = 0.4394
Batch 120: Loss = 1.1485
Batch 180: Loss = 0.2155
Batch 240: Loss = 0.4654
Batch 300: Loss = 0.5661
Batch 360: Loss = 0.2631
Batch 420: Loss = 0.7390
Batch 480: Loss = 0.4772
Batch 540: Loss = 0.4001
Batch 600: Loss = 0.5324
Batch 660: Loss = 0.4020
Batch 720: Loss = 0.4426
Batch 780: Loss = 0.3682
Batch 840: Loss = 0.8991
Batch 900: Loss = 0.4453
Batch 960: Loss = 0.4470
Batch 1020: Loss = 0.2539
Batch 1080: Loss = 0.6637
Batch 1140: Loss = 0.2144
Batch 1200: Loss = 0.6035
Batch 1260: Loss = 0.3757
Batch 1320: Loss = 0.3518
Batch 1380: Loss = 0.6322
Batch 1440: Loss = 0.5881
Batch 1500: Loss = 0.4711
Batch 1560: Loss = 0.4010
Batch 1620: Loss = 0.8366
Batch 1680: Loss = 0.6814
Batch 1740: Loss = 0.9387
Batch 1800: Loss = 0.8650
Batch 1860: Loss = 0.4678
Batch 1920: Loss = 0.9068
Batch 1980: Loss = 0.4785
Batch 2040: Loss = 0.6485
Batch 2100: Loss = 0.5417
Batch 2160: Loss = 0.4534
Batch 2220: Loss = 0.4629
Batch 2280: Loss = 0.3838
Batch 2340: Loss = 0.4416
Batch 2400: Loss = 0.7819
Batch 2460: Loss = 0.4337
Batch 2520: Loss = 0.3949
Batch 2580: Loss = 0.4043
Batch 2640: Loss = 0.5986
Batch 2700: Loss = 0.2857
Batch 2760: Loss = 0.7404
Batch 2820: Loss = 0.3636
Batch 2880: Loss = 0.5679
Batch 2940: Loss = 0.3228
Batch 3000: Loss = 0.4093
Batch 3060: Loss = 0.2342
Batch 3120: Loss = 0.3825
Batch 3180: Loss = 0.8624
Batch 3240: Loss = 0.6706
Batch 3300: Loss = 0.6275
Batch 3360: Loss = 0.4324
Batch 3420: Loss = 0.3092
Batch 3480: Loss = 0.7861
Batch 3540: Loss = 0.5679
Batch 3600: Loss = 0.5020
Batch 3660: Loss = 0.8686
Batch 3720: Loss = 0.4889
Batch 3780: Loss = 0.6328
Batch 3840: Loss = 0.4048
Batch 3900: Loss = 0.6696
Batch 3960: Loss = 0.5702
Batch 4020: Loss = 0.9001
Batch 4080: Loss = 0.6238
Batch 4140: Loss = 0.5547
Batch 4200: Loss = 1.2495
Batch 4260: Loss = 0.5718
Batch 4320: Loss = 1.2897
Batch 4380: Loss = 1.2077
Batch 4440: Loss = 0.9214
Batch 4500: Loss = 0.5368
Additional Epoch [5/8] Train Loss: 0.5451 | Train Acc: 80.61% Val Loss: 1.1762 | Val Acc: 62.87%
Batch 60: Loss = 0.3929
Batch 120: Loss = 0.5408
Batch 180: Loss = 0.2066
Batch 240: Loss = 0.5675
Batch 300: Loss = 0.2520
Batch 360: Loss = 0.2829
Batch 420: Loss = 0.7735
Batch 480: Loss = 0.2857
Batch 540: Loss = 0.6118
Batch 600: Loss = 0.5166
Batch 660: Loss = 0.4436
Batch 720: Loss = 0.3506
Batch 780: Loss = 0.4465
Batch 840: Loss = 0.5263
Batch 900: Loss = 0.6253
Batch 960: Loss = 0.4839
Batch 1020: Loss = 0.5140
Batch 1080: Loss = 0.3995
Batch 1140: Loss = 0.3114
Batch 1200: Loss = 0.2775
Batch 1260: Loss = 0.6675
Batch 1320: Loss = 0.3379
Batch 1380: Loss = 0.2668
Batch 1440: Loss = 0.3493
Batch 1500: Loss = 0.4906
Batch 1560: Loss = 0.2934
Batch 1620: Loss = 0.4991
Batch 1680: Loss = 0.4880
Batch 1740: Loss = 0.3760
Batch 1800: Loss = 0.4619
Batch 1860: Loss = 0.8612
Batch 1920: Loss = 0.7272
Batch 1980: Loss = 0.3070
Batch 2040: Loss = 0.3375
Batch 2100: Loss = 0.5899
Batch 2160: Loss = 0.4891
Batch 2220: Loss = 0.3888
Batch 2280: Loss = 0.5048
Batch 2340: Loss = 0.3956
Batch 2400: Loss = 0.4869
Batch 2460: Loss = 0.7046
Batch 2520: Loss = 0.6738
Batch 2580: Loss = 0.3289
Batch 2640: Loss = 0.2828
Batch 2700: Loss = 0.6579
Batch 2760: Loss = 0.7701
Batch 2820: Loss = 0.4031
Batch 2880: Loss = 0.7573
Batch 2940: Loss = 0.5771
Batch 3000: Loss = 0.6684
Batch 3060: Loss = 0.3180
Batch 3120: Loss = 0.4491
Batch 3180: Loss = 0.7787
Batch 3240: Loss = 0.6032
Batch 3300: Loss = 0.1715
Batch 3360: Loss = 0.4772
Batch 3420: Loss = 0.2391
Batch 3480: Loss = 0.2663
Batch 3540: Loss = 0.4491
Batch 3600: Loss = 0.3077
Batch 3660: Loss = 0.3570
Batch 3720: Loss = 0.8802
Batch 3780: Loss = 0.9649
Batch 3840: Loss = 0.4020
Batch 3900: Loss = 0.4107
Batch 3960: Loss = 0.3286
Batch 4020: Loss = 0.6892
Batch 4080: Loss = 0.3544
Batch 4140: Loss = 0.3777
Batch 4200: Loss = 0.2002
Batch 4260: Loss = 0.2048
Batch 4320: Loss = 0.6840
Batch 4380: Loss = 0.5478
Batch 4440: Loss = 0.2504
Batch 4500: Loss = 0.5986
Additional Epoch [6/8] Train Loss: 0.4701 | Train Acc: 83.28% Val Loss: 1.2087 | Val Acc: 63.82%
Batch 60: Loss = 0.4237
Batch 120: Loss = 0.1985
Batch 180: Loss = 0.3163
Batch 240: Loss = 0.3870
Batch 300: Loss = 0.2668
Batch 360: Loss = 0.3020
Batch 420: Loss = 0.6587
Batch 480: Loss = 0.2459
Batch 540: Loss = 0.3456
Batch 600: Loss = 0.2635
Batch 660: Loss = 0.5605
Batch 720: Loss = 0.6926
Batch 780: Loss = 0.3868
Batch 840: Loss = 0.2662
Batch 900: Loss = 0.3240
Batch 960: Loss = 0.3421
Batch 1020: Loss = 1.0054
Batch 1080: Loss = 0.2752
Batch 1140: Loss = 0.2692
Batch 1200: Loss = 0.1521
Batch 1260: Loss = 0.4793
Batch 1320: Loss = 0.4675
Batch 1380: Loss = 0.3572
Batch 1440: Loss = 0.1794
Batch 1500: Loss = 0.6623
Batch 1560: Loss = 0.3485
Batch 1620: Loss = 0.5137
Batch 1680: Loss = 0.4805
Batch 1740: Loss = 0.5255
Batch 1800: Loss = 0.6373
Batch 1860: Loss = 0.3243
Batch 1920: Loss = 0.4804
Batch 1980: Loss = 0.6091
Batch 2040: Loss = 0.3549
Batch 2100: Loss = 0.1605
Batch 2160: Loss = 0.3350
Batch 2220: Loss = 0.4066
Batch 2280: Loss = 0.4475
Batch 2340: Loss = 0.7717
Batch 2400: Loss = 0.3568
Batch 2460: Loss = 0.2512
Batch 2520: Loss = 0.5085
Batch 2580: Loss = 0.1376
Batch 2640: Loss = 0.3774
Batch 2700: Loss = 0.8050
Batch 2760: Loss = 0.7971
Batch 2820: Loss = 0.2160
Batch 2880: Loss = 0.5234
Batch 2940: Loss = 0.2613
Batch 3000: Loss = 0.2252
Batch 3060: Loss = 0.7551
Batch 3120: Loss = 0.5770
Batch 3180: Loss = 0.2012
Batch 3240: Loss = 0.8063
Batch 3300: Loss = 0.2281
Batch 3360: Loss = 0.4084
Batch 3420: Loss = 0.4311
Batch 3480: Loss = 0.7636
Batch 3540: Loss = 0.2019
Batch 3600: Loss = 0.4803
Batch 3660: Loss = 0.2055
Batch 3720: Loss = 0.7809
Batch 3780: Loss = 0.6807
Batch 3840: Loss = 0.6730
Batch 3900: Loss = 0.2674
Batch 3960: Loss = 0.1106
Batch 4020: Loss = 0.5940
Batch 4080: Loss = 0.5391
Batch 4140: Loss = 0.1646
Batch 4200: Loss = 0.4838
Batch 4260: Loss = 0.6354
Batch 4320: Loss = 0.4808
Batch 4380: Loss = 0.3558
Batch 4440: Loss = 0.3954
Batch 4500: Loss = 0.8574
Additional Epoch [7/8] Train Loss: 0.4080 | Train Acc: 85.39% Val Loss: 1.3006 | Val Acc: 64.03%
Batch 60: Loss = 0.3629
Batch 120: Loss = 0.4627
Batch 180: Loss = 0.4970
Batch 240: Loss = 0.1387
Batch 300: Loss = 0.2652
Batch 360: Loss = 0.2835
Batch 420: Loss = 0.2398
Batch 480: Loss = 0.5629
Batch 540: Loss = 0.2001
Batch 600: Loss = 0.4702
Batch 660: Loss = 0.2016
Batch 720: Loss = 0.0541
Batch 780: Loss = 0.1929
Batch 840: Loss = 0.1082
Batch 900: Loss = 0.4038
Batch 960: Loss = 0.6980
Batch 1020: Loss = 0.3049
Batch 1080: Loss = 0.2308
Batch 1140: Loss = 0.3780
Batch 1200: Loss = 0.4610
Batch 1260: Loss = 0.5168
Batch 1320: Loss = 0.1646
Batch 1380: Loss = 0.4202
Batch 1440: Loss = 0.3069
Batch 1500: Loss = 0.5035
Batch 1560: Loss = 0.1333
Batch 1620: Loss = 0.4371
Batch 1680: Loss = 0.3575
Batch 1740: Loss = 0.3448
Batch 1800: Loss = 0.3488
Batch 1860: Loss = 0.2294
Batch 1920: Loss = 0.1246
Batch 1980: Loss = 0.2075
Batch 2040: Loss = 0.2494
Batch 2100: Loss = 0.1965
Batch 2160: Loss = 0.2262
Batch 2220: Loss = 0.6776
Batch 2280: Loss = 0.8725
Batch 2340: Loss = 0.2500
Batch 2400: Loss = 0.3345
Batch 2460: Loss = 0.1805
Batch 2520: Loss = 0.2915
Batch 2580: Loss = 0.3034
Batch 2640: Loss = 0.2075
Batch 2700: Loss = 0.4281
Batch 2760: Loss = 0.3345
Batch 2820: Loss = 0.2245
Batch 2880: Loss = 0.7349
Batch 2940: Loss = 0.2856
Batch 3000: Loss = 0.2444
Batch 3060: Loss = 0.5672
Batch 3120: Loss = 0.1989
Batch 3180: Loss = 0.2154
Batch 3240: Loss = 0.4427
Batch 3300: Loss = 0.3728
Batch 3360: Loss = 0.5150
Batch 3420: Loss = 0.4763
Batch 3480: Loss = 0.2059
Batch 3540: Loss = 0.2169
Batch 3600: Loss = 0.2132
Batch 3660: Loss = 0.4037
Batch 3720: Loss = 0.1442
Batch 3780: Loss = 0.4566
Batch 3840: Loss = 0.2163
Batch 3900: Loss = 0.2071
Batch 3960: Loss = 0.6125
Batch 4020: Loss = 0.3747
Batch 4080: Loss = 0.4188
Batch 4140: Loss = 0.1577
Batch 4200: Loss = 0.3110
Batch 4260: Loss = 0.2939
Batch 4320: Loss = 0.5038
Batch 4380: Loss = 0.5569
Batch 4440: Loss = 0.2807
Batch 4500: Loss = 0.8202
Additional Epoch [8/8] Train Loss: 0.3495 | Train Acc: 87.59% Val Loss: 1.3235 | Val Acc: 63.27%


