{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "940e66af-3279-48fc-a846-6daadd85b09e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-28T02:06:50.229738Z",
     "start_time": "2025-03-28T02:05:24.526401Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\r\n",
      "Traceback (most recent call last):\r\n",
      "  File \"<frozen runpy>\", line 198, in _run_module_as_main\r\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\r\n",
      "  File \"/Users/surenmnatsakanyan/Desktop/Warsaw_Courses/Semester2/Deep Learning/CNN_1-Project1/venv/lib/python3.11/site-packages/pip/__main__.py\", line 29, in <module>\r\n",
      "    from pip._internal.cli.main import main as _main\r\n",
      "  File \"/Users/surenmnatsakanyan/Desktop/Warsaw_Courses/Semester2/Deep Learning/CNN_1-Project1/venv/lib/python3.11/site-packages/pip/_internal/cli/main.py\", line 9, in <module>\r\n",
      "    from pip._internal.cli.autocompletion import autocomplete\r\n",
      "  File \"/Users/surenmnatsakanyan/Desktop/Warsaw_Courses/Semester2/Deep Learning/CNN_1-Project1/venv/lib/python3.11/site-packages/pip/_internal/cli/autocompletion.py\", line 10, in <module>\r\n",
      "    from pip._internal.cli.main_parser import create_main_parser\r\n",
      "  File \"/Users/surenmnatsakanyan/Desktop/Warsaw_Courses/Semester2/Deep Learning/CNN_1-Project1/venv/lib/python3.11/site-packages/pip/_internal/cli/main_parser.py\", line 9, in <module>\r\n",
      "    from pip._internal.build_env import get_runnable_pip\r\n",
      "  File \"/Users/surenmnatsakanyan/Desktop/Warsaw_Courses/Semester2/Deep Learning/CNN_1-Project1/venv/lib/python3.11/site-packages/pip/_internal/build_env.py\", line 16, in <module>\r\n",
      "    from pip._vendor.packaging.requirements import Requirement\r\n",
      "  File \"/Users/surenmnatsakanyan/Desktop/Warsaw_Courses/Semester2/Deep Learning/CNN_1-Project1/venv/lib/python3.11/site-packages/pip/_vendor/packaging/requirements.py\", line 10, in <module>\r\n",
      "    from pip._vendor.pyparsing import (  # noqa\r\n",
      "  File \"/Users/surenmnatsakanyan/Desktop/Warsaw_Courses/Semester2/Deep Learning/CNN_1-Project1/venv/lib/python3.11/site-packages/pip/_vendor/pyparsing/__init__.py\", line 138, in <module>\r\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# You might be required to have numpy2 second version, so if any problems run bellow. Please do not run it now, but when you get numpy version issues.\n",
    "# pip install \"numpy<2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c202323e-2983-4e94-9118-7e848099b311",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-28T01:58:57.494561Z",
     "start_time": "2025-03-28T01:56:26.767832Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision import transforms\n",
    "import numpy as np\n",
    "from torch.utils.data import Subset\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c121f6b6-1078-4c78-9998-176efd822e9b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-28T02:04:09.346318Z",
     "start_time": "2025-03-28T02:04:09.327997Z"
    }
   },
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride=1):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride,\n",
    "                               padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1,\n",
    "                               padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        \n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_channels != out_channels:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(out_channels)\n",
    "            )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "# 2. Define the ResNet18 Architecture\n",
    "class ResNet18(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(ResNet18, self).__init__()\n",
    "        # Conv1: 7x7, stride=2, padding=3\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        \n",
    "        # MaxPool: 3x3, stride=2, padding=1\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        \n",
    "        # Residual layers\n",
    "        self.layer1 = self._make_layer(64, 64, 2, stride=1)\n",
    "        self.layer2 = self._make_layer(64, 128, 2, stride=2)\n",
    "        self.layer3 = self._make_layer(128, 256, 2, stride=2)\n",
    "        self.layer4 = self._make_layer(256, 512, 2, stride=2)\n",
    "        \n",
    "        # Global Average Pooling & Fully Connected\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(512, num_classes)\n",
    "    \n",
    "    def _make_layer(self, in_channels, out_channels, blocks, stride):\n",
    "        layers = []\n",
    "        # First block can downsample if stride != 1\n",
    "        layers.append(ResidualBlock(in_channels, out_channels, stride))\n",
    "        for _ in range(1, blocks):\n",
    "            layers.append(ResidualBlock(out_channels, out_channels))\n",
    "        return nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.bn1(self.conv1(x)))\n",
    "        x = self.maxpool(x)\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5dd86ae2-0642-4a65-b89a-150f33c96d3e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-28T02:04:19.918171Z",
     "start_time": "2025-03-28T02:04:16.944745Z"
    }
   },
   "outputs": [],
   "source": [
    "DATA_DIR = \"archive\"  # Should contain train/, valid/, test/\n",
    "TRAIN_DIR = os.path.join(DATA_DIR, \"train\")\n",
    "VAL_DIR = os.path.join(DATA_DIR, \"valid\")\n",
    "TEST_DIR = os.path.join(DATA_DIR, \"test\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "basic_transform = transforms.ToTensor()\n",
    "NUM_CLASSES = 10\n",
    "\n",
    "train_dataset = ImageFolder(TRAIN_DIR, transform=basic_transform)\n",
    "val_dataset   = ImageFolder(VAL_DIR, transform=basic_transform)\n",
    "test_dataset  = ImageFolder(TEST_DIR, transform=basic_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b8797d40-7b0b-4fd7-83bd-62f017fe5ca6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-28T02:04:25.350474Z",
     "start_time": "2025-03-28T02:04:25.333967Z"
    }
   },
   "outputs": [],
   "source": [
    "def train_one_epoch(model, loader, optimizer, criterion):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    # Counter for batch logging\n",
    "    batch_idx = 0\n",
    "    \n",
    "    for images, labels in loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        \n",
    "        # Forward\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Metrics\n",
    "        total_loss += loss.item() * images.size(0)\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "        \n",
    "        # Logging every 60 batches\n",
    "        batch_idx += 1\n",
    "        if batch_idx % 60 == 0:\n",
    "            current_loss = loss.item()\n",
    "            print(f\"Batch {batch_idx}: Loss = {current_loss:.4f}\")\n",
    "    \n",
    "    avg_loss = total_loss / total\n",
    "    accuracy = 100.0 * correct / total\n",
    "    return avg_loss, accuracy\n",
    "\n",
    "\n",
    "def evaluate(model, loader, criterion):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            \n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            total_loss += loss.item() * images.size(0)\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "    \n",
    "    avg_loss = total_loss / total\n",
    "    accuracy = 100.0 * correct / total\n",
    "    return avg_loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "be2198ad-b14e-486d-b6ed-0ef22c539a87",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 60: Loss = 1.8056\n",
      "Batch 120: Loss = 2.0977\n",
      "Batch 180: Loss = 1.7997\n",
      "Batch 240: Loss = 1.7087\n",
      "Batch 300: Loss = 1.6217\n",
      "Batch 360: Loss = 1.9710\n",
      "Batch 420: Loss = 2.0206\n",
      "Batch 480: Loss = 2.0764\n",
      "Batch 540: Loss = 2.1607\n",
      "Batch 600: Loss = 1.6640\n",
      "Batch 660: Loss = 1.5909\n",
      "Batch 720: Loss = 1.3208\n",
      "Batch 780: Loss = 2.5268\n",
      "Batch 840: Loss = 1.5386\n",
      "Batch 900: Loss = 1.6482\n",
      "Batch 960: Loss = 1.7658\n",
      "Batch 1020: Loss = 1.7659\n",
      "Batch 1080: Loss = 2.1486\n",
      "Batch 1140: Loss = 1.7515\n",
      "Batch 1200: Loss = 1.5361\n",
      "Batch 1260: Loss = 1.7780\n",
      "Batch 1320: Loss = 2.0902\n",
      "Batch 1380: Loss = 1.5508\n",
      "Batch 1440: Loss = 1.6111\n",
      "Batch 1500: Loss = 1.4598\n",
      "Batch 1560: Loss = 1.9385\n",
      "Batch 1620: Loss = 1.6070\n",
      "Batch 1680: Loss = 1.7043\n",
      "Batch 1740: Loss = 1.5222\n",
      "Batch 1800: Loss = 1.6921\n",
      "Batch 1860: Loss = 1.5958\n",
      "Batch 1920: Loss = 1.6879\n",
      "Batch 1980: Loss = 1.4940\n",
      "Batch 2040: Loss = 1.6723\n",
      "Batch 2100: Loss = 1.4558\n",
      "Batch 2160: Loss = 1.3828\n",
      "Batch 2220: Loss = 1.3518\n",
      "Batch 2280: Loss = 1.9867\n",
      "Batch 2340: Loss = 1.6502\n",
      "Batch 2400: Loss = 1.7180\n",
      "Batch 2460: Loss = 1.6064\n",
      "Batch 2520: Loss = 1.7465\n",
      "Batch 2580: Loss = 1.8801\n",
      "Batch 2640: Loss = 1.2831\n",
      "Batch 2700: Loss = 1.3586\n",
      "Batch 2760: Loss = 1.3070\n",
      "Batch 2820: Loss = 1.5092\n",
      "Batch 2880: Loss = 1.6981\n",
      "Batch 2940: Loss = 1.4272\n",
      "Batch 3000: Loss = 1.2879\n",
      "Batch 3060: Loss = 1.6116\n",
      "Batch 3120: Loss = 1.3039\n",
      "Batch 3180: Loss = 1.4352\n",
      "Batch 3240: Loss = 1.3551\n",
      "Batch 3300: Loss = 1.4117\n",
      "Batch 3360: Loss = 1.4548\n",
      "Batch 3420: Loss = 1.6549\n",
      "Batch 3480: Loss = 1.4645\n",
      "Batch 3540: Loss = 1.4968\n",
      "Batch 3600: Loss = 1.5525\n",
      "Batch 3660: Loss = 1.8029\n",
      "Batch 3720: Loss = 1.4071\n",
      "Batch 3780: Loss = 1.3634\n",
      "Batch 3840: Loss = 1.2032\n",
      "Batch 3900: Loss = 1.5148\n",
      "Batch 3960: Loss = 1.2316\n",
      "Batch 4020: Loss = 1.1916\n",
      "Batch 4080: Loss = 1.1122\n",
      "Batch 4140: Loss = 1.5452\n",
      "Batch 4200: Loss = 1.3094\n",
      "Batch 4260: Loss = 1.4150\n",
      "Batch 4320: Loss = 1.4311\n",
      "Batch 4380: Loss = 1.3913\n",
      "Batch 4440: Loss = 1.0943\n",
      "Batch 4500: Loss = 1.1873\n",
      "Epoch [1/5] Train Loss: 1.6470 | Train Acc: 40.05% Val Loss: 1.4741 | Val Acc: 46.62%\n",
      "Batch 60: Loss = 1.4562\n",
      "Batch 120: Loss = 1.1627\n",
      "Batch 180: Loss = 1.3227\n",
      "Batch 240: Loss = 1.3792\n",
      "Batch 300: Loss = 1.3357\n",
      "Batch 360: Loss = 0.9987\n",
      "Batch 420: Loss = 1.7592\n",
      "Batch 480: Loss = 1.1216\n",
      "Batch 540: Loss = 1.5160\n",
      "Batch 600: Loss = 1.8904\n",
      "Batch 660: Loss = 1.3029\n",
      "Batch 720: Loss = 1.3479\n",
      "Batch 780: Loss = 1.1990\n",
      "Batch 840: Loss = 1.5904\n",
      "Batch 900: Loss = 1.3239\n",
      "Batch 960: Loss = 1.1658\n",
      "Batch 1020: Loss = 1.1452\n",
      "Batch 1080: Loss = 1.4022\n",
      "Batch 1140: Loss = 1.2764\n",
      "Batch 1200: Loss = 1.2794\n",
      "Batch 1260: Loss = 1.3651\n",
      "Batch 1320: Loss = 1.6694\n",
      "Batch 1380: Loss = 1.0372\n",
      "Batch 1440: Loss = 1.3525\n",
      "Batch 1500: Loss = 1.1647\n",
      "Batch 1560: Loss = 1.5549\n",
      "Batch 1620: Loss = 1.1648\n",
      "Batch 1680: Loss = 1.3946\n",
      "Batch 1740: Loss = 1.1953\n",
      "Batch 1800: Loss = 1.1331\n",
      "Batch 1860: Loss = 1.2397\n",
      "Batch 1920: Loss = 1.0646\n",
      "Batch 1980: Loss = 1.1434\n",
      "Batch 2040: Loss = 1.5722\n",
      "Batch 2100: Loss = 1.4316\n",
      "Batch 2160: Loss = 1.8441\n",
      "Batch 2220: Loss = 1.0211\n",
      "Batch 2280: Loss = 0.7578\n",
      "Batch 2340: Loss = 1.0924\n",
      "Batch 2400: Loss = 1.4610\n",
      "Batch 2460: Loss = 1.4309\n",
      "Batch 2520: Loss = 1.4016\n",
      "Batch 2580: Loss = 1.1218\n",
      "Batch 2640: Loss = 1.1475\n",
      "Batch 2700: Loss = 1.0268\n",
      "Batch 2760: Loss = 1.1658\n",
      "Batch 2820: Loss = 1.1490\n",
      "Batch 2880: Loss = 1.1475\n",
      "Batch 2940: Loss = 1.2704\n",
      "Batch 3000: Loss = 1.4637\n",
      "Batch 3060: Loss = 0.7195\n",
      "Batch 3120: Loss = 0.9074\n",
      "Batch 3180: Loss = 1.3146\n",
      "Batch 3240: Loss = 1.5190\n",
      "Batch 3300: Loss = 1.5601\n",
      "Batch 3360: Loss = 1.1098\n",
      "Batch 3420: Loss = 0.9894\n",
      "Batch 3480: Loss = 1.4408\n",
      "Batch 3540: Loss = 1.4965\n",
      "Batch 3600: Loss = 1.7488\n",
      "Batch 3660: Loss = 0.9275\n",
      "Batch 3720: Loss = 1.3120\n",
      "Batch 3780: Loss = 1.5273\n",
      "Batch 3840: Loss = 1.5001\n",
      "Batch 3900: Loss = 1.1102\n",
      "Batch 3960: Loss = 1.1005\n",
      "Batch 4020: Loss = 1.0928\n",
      "Batch 4080: Loss = 1.3002\n",
      "Batch 4140: Loss = 1.0367\n",
      "Batch 4200: Loss = 1.3544\n",
      "Batch 4260: Loss = 1.4154\n",
      "Batch 4320: Loss = 1.0925\n",
      "Batch 4380: Loss = 1.3712\n",
      "Batch 4440: Loss = 1.2961\n",
      "Batch 4500: Loss = 0.7120\n",
      "Epoch [2/5] Train Loss: 1.3487 | Train Acc: 51.29% Val Loss: 1.4407 | Val Acc: 48.93%\n",
      "Batch 60: Loss = 1.0256\n",
      "Batch 120: Loss = 0.9440\n",
      "Batch 180: Loss = 1.0384\n",
      "Batch 240: Loss = 1.0647\n",
      "Batch 300: Loss = 1.7422\n",
      "Batch 360: Loss = 1.3135\n",
      "Batch 420: Loss = 1.8359\n",
      "Batch 480: Loss = 1.4736\n",
      "Batch 540: Loss = 1.3508\n",
      "Batch 600: Loss = 0.9627\n",
      "Batch 660: Loss = 1.0827\n",
      "Batch 720: Loss = 1.1862\n",
      "Batch 780: Loss = 1.5023\n",
      "Batch 840: Loss = 1.1919\n",
      "Batch 900: Loss = 1.5194\n",
      "Batch 960: Loss = 1.1423\n",
      "Batch 1020: Loss = 1.2172\n",
      "Batch 1080: Loss = 1.4972\n",
      "Batch 1140: Loss = 1.0689\n",
      "Batch 1200: Loss = 1.2947\n",
      "Batch 1260: Loss = 0.9711\n",
      "Batch 1320: Loss = 1.1435\n",
      "Batch 1380: Loss = 1.3848\n",
      "Batch 1440: Loss = 1.3523\n",
      "Batch 1500: Loss = 0.9489\n",
      "Batch 1560: Loss = 1.0713\n",
      "Batch 1620: Loss = 1.5111\n",
      "Batch 1680: Loss = 1.0455\n",
      "Batch 1740: Loss = 1.3246\n",
      "Batch 1800: Loss = 0.8463\n",
      "Batch 1860: Loss = 1.0608\n",
      "Batch 1920: Loss = 1.6666\n",
      "Batch 1980: Loss = 1.2668\n",
      "Batch 2040: Loss = 1.1155\n",
      "Batch 2100: Loss = 0.8339\n",
      "Batch 2160: Loss = 1.3568\n",
      "Batch 2220: Loss = 1.6953\n",
      "Batch 2280: Loss = 1.4624\n",
      "Batch 2340: Loss = 1.4906\n",
      "Batch 2400: Loss = 1.5785\n",
      "Batch 2460: Loss = 1.7660\n",
      "Batch 2520: Loss = 1.2345\n",
      "Batch 2580: Loss = 1.3228\n",
      "Batch 2640: Loss = 1.1285\n",
      "Batch 2700: Loss = 1.2540\n",
      "Batch 2760: Loss = 1.4450\n",
      "Batch 2820: Loss = 1.3789\n",
      "Batch 2880: Loss = 1.0459\n",
      "Batch 2940: Loss = 1.4419\n",
      "Batch 3000: Loss = 1.3737\n",
      "Batch 3060: Loss = 0.9154\n",
      "Batch 3120: Loss = 0.9992\n",
      "Batch 3180: Loss = 1.5674\n",
      "Batch 3240: Loss = 1.3808\n",
      "Batch 3300: Loss = 1.1241\n",
      "Batch 3360: Loss = 1.0157\n",
      "Batch 3420: Loss = 1.2998\n",
      "Batch 3480: Loss = 1.0899\n",
      "Batch 3540: Loss = 1.3265\n",
      "Batch 3600: Loss = 0.9738\n",
      "Batch 3660: Loss = 1.1851\n",
      "Batch 3720: Loss = 1.0816\n",
      "Batch 3780: Loss = 0.9699\n",
      "Batch 3840: Loss = 0.7619\n",
      "Batch 3900: Loss = 1.3923\n",
      "Batch 3960: Loss = 0.8552\n",
      "Batch 4020: Loss = 1.4864\n",
      "Batch 4080: Loss = 0.8445\n",
      "Batch 4140: Loss = 1.1744\n",
      "Batch 4200: Loss = 1.1661\n",
      "Batch 4260: Loss = 1.1237\n",
      "Batch 4320: Loss = 0.9325\n",
      "Batch 4380: Loss = 1.0702\n",
      "Batch 4440: Loss = 1.3769\n",
      "Batch 4500: Loss = 1.4187\n",
      "Epoch [3/5] Train Loss: 1.1931 | Train Acc: 57.37% Val Loss: 1.3032 | Val Acc: 54.21%\n",
      "Batch 60: Loss = 0.9822\n",
      "Batch 120: Loss = 1.0721\n",
      "Batch 180: Loss = 1.6073\n",
      "Batch 240: Loss = 1.2009\n",
      "Batch 300: Loss = 1.4290\n",
      "Batch 360: Loss = 1.2307\n",
      "Batch 420: Loss = 0.6277\n",
      "Batch 480: Loss = 1.4832\n",
      "Batch 540: Loss = 0.9182\n",
      "Batch 600: Loss = 0.8820\n",
      "Batch 660: Loss = 1.4712\n",
      "Batch 720: Loss = 0.6835\n",
      "Batch 780: Loss = 1.3605\n",
      "Batch 840: Loss = 1.0311\n",
      "Batch 900: Loss = 0.8635\n",
      "Batch 960: Loss = 1.1589\n",
      "Batch 1020: Loss = 1.2397\n",
      "Batch 1080: Loss = 1.5118\n",
      "Batch 1140: Loss = 1.4831\n",
      "Batch 1200: Loss = 1.4870\n",
      "Batch 1260: Loss = 0.8505\n",
      "Batch 1320: Loss = 0.9761\n",
      "Batch 1380: Loss = 1.1897\n",
      "Batch 1440: Loss = 1.4723\n",
      "Batch 1500: Loss = 0.9596\n",
      "Batch 1560: Loss = 0.9359\n",
      "Batch 1620: Loss = 0.9720\n",
      "Batch 1680: Loss = 0.8867\n",
      "Batch 1740: Loss = 0.9464\n",
      "Batch 1800: Loss = 1.3816\n",
      "Batch 1860: Loss = 1.0847\n",
      "Batch 1920: Loss = 0.9052\n",
      "Batch 1980: Loss = 1.3958\n",
      "Batch 2040: Loss = 1.1173\n",
      "Batch 2100: Loss = 1.2338\n",
      "Batch 2160: Loss = 1.1604\n",
      "Batch 2220: Loss = 0.9510\n",
      "Batch 2280: Loss = 1.3922\n",
      "Batch 2340: Loss = 0.9572\n",
      "Batch 2400: Loss = 1.3792\n",
      "Batch 2460: Loss = 1.0677\n",
      "Batch 2520: Loss = 0.9081\n",
      "Batch 2580: Loss = 0.8596\n",
      "Batch 2640: Loss = 0.8914\n",
      "Batch 2700: Loss = 0.9807\n",
      "Batch 2760: Loss = 1.0863\n",
      "Batch 2820: Loss = 1.4500\n",
      "Batch 2880: Loss = 1.0984\n",
      "Batch 2940: Loss = 1.5208\n",
      "Batch 3000: Loss = 1.4471\n",
      "Batch 3060: Loss = 0.7505\n",
      "Batch 3120: Loss = 1.2444\n",
      "Batch 3180: Loss = 0.7118\n",
      "Batch 3240: Loss = 0.6590\n",
      "Batch 3300: Loss = 0.9834\n",
      "Batch 3360: Loss = 0.9745\n",
      "Batch 3420: Loss = 1.4453\n",
      "Batch 3480: Loss = 1.1033\n",
      "Batch 3540: Loss = 1.1144\n",
      "Batch 3600: Loss = 0.5359\n",
      "Batch 3660: Loss = 1.3640\n",
      "Batch 3720: Loss = 0.9380\n",
      "Batch 3780: Loss = 0.8199\n",
      "Batch 3840: Loss = 0.6190\n",
      "Batch 3900: Loss = 1.4428\n",
      "Batch 3960: Loss = 0.9891\n",
      "Batch 4020: Loss = 0.8548\n",
      "Batch 4080: Loss = 0.8270\n",
      "Batch 4140: Loss = 1.2356\n",
      "Batch 4200: Loss = 0.6742\n",
      "Batch 4260: Loss = 1.2113\n",
      "Batch 4320: Loss = 1.7223\n",
      "Batch 4380: Loss = 0.8266\n",
      "Batch 4440: Loss = 0.9107\n",
      "Batch 4500: Loss = 1.0940\n",
      "Epoch [4/5] Train Loss: 1.0848 | Train Acc: 61.47% Val Loss: 1.1542 | Val Acc: 58.57%\n",
      "Batch 60: Loss = 1.2382\n",
      "Batch 120: Loss = 0.7543\n",
      "Batch 180: Loss = 0.7758\n",
      "Batch 240: Loss = 1.0219\n",
      "Batch 300: Loss = 0.6843\n",
      "Batch 360: Loss = 0.9356\n",
      "Batch 420: Loss = 0.5979\n",
      "Batch 480: Loss = 1.0419\n",
      "Batch 540: Loss = 0.6898\n",
      "Batch 600: Loss = 0.6866\n",
      "Batch 660: Loss = 0.5680\n",
      "Batch 720: Loss = 1.2181\n",
      "Batch 780: Loss = 1.7612\n",
      "Batch 840: Loss = 0.9034\n",
      "Batch 900: Loss = 0.9563\n",
      "Batch 960: Loss = 1.3825\n",
      "Batch 1020: Loss = 1.3198\n",
      "Batch 1080: Loss = 1.1272\n",
      "Batch 1140: Loss = 0.9330\n",
      "Batch 1200: Loss = 0.9068\n",
      "Batch 1260: Loss = 1.4688\n",
      "Batch 1320: Loss = 0.9536\n",
      "Batch 1380: Loss = 0.7270\n",
      "Batch 1440: Loss = 0.9903\n",
      "Batch 1500: Loss = 1.0750\n",
      "Batch 1560: Loss = 0.6054\n",
      "Batch 1620: Loss = 0.6617\n",
      "Batch 1680: Loss = 1.1201\n",
      "Batch 1740: Loss = 0.8848\n",
      "Batch 1800: Loss = 0.9191\n",
      "Batch 1860: Loss = 0.7863\n",
      "Batch 1920: Loss = 0.8451\n",
      "Batch 1980: Loss = 1.0753\n",
      "Batch 2040: Loss = 0.4282\n",
      "Batch 2100: Loss = 0.9432\n",
      "Batch 2160: Loss = 1.0495\n",
      "Batch 2220: Loss = 0.9610\n",
      "Batch 2280: Loss = 0.6962\n",
      "Batch 2340: Loss = 1.2156\n",
      "Batch 2400: Loss = 1.0260\n",
      "Batch 2460: Loss = 0.8002\n",
      "Batch 2520: Loss = 1.0005\n",
      "Batch 2580: Loss = 1.4439\n",
      "Batch 2640: Loss = 0.3942\n",
      "Batch 2700: Loss = 0.7073\n",
      "Batch 2760: Loss = 0.6593\n",
      "Batch 2820: Loss = 0.8692\n",
      "Batch 2880: Loss = 1.1103\n",
      "Batch 2940: Loss = 1.0651\n",
      "Batch 3000: Loss = 1.2538\n",
      "Batch 3060: Loss = 1.3708\n",
      "Batch 3120: Loss = 1.3601\n",
      "Batch 3180: Loss = 1.3272\n",
      "Batch 3240: Loss = 1.0021\n",
      "Batch 3300: Loss = 0.9262\n",
      "Batch 3360: Loss = 1.2014\n",
      "Batch 3420: Loss = 0.9481\n",
      "Batch 3480: Loss = 0.8860\n",
      "Batch 3540: Loss = 1.4273\n",
      "Batch 3600: Loss = 0.8782\n",
      "Batch 3660: Loss = 0.9041\n",
      "Batch 3720: Loss = 1.0130\n",
      "Batch 3780: Loss = 1.1777\n",
      "Batch 3840: Loss = 0.6088\n",
      "Batch 3900: Loss = 0.8457\n",
      "Batch 3960: Loss = 0.9544\n",
      "Batch 4020: Loss = 0.7575\n",
      "Batch 4080: Loss = 1.3521\n",
      "Batch 4140: Loss = 1.0096\n",
      "Batch 4200: Loss = 0.6971\n",
      "Batch 4260: Loss = 1.2413\n",
      "Batch 4320: Loss = 0.8529\n",
      "Batch 4380: Loss = 1.2918\n",
      "Batch 4440: Loss = 1.2541\n",
      "Batch 4500: Loss = 0.8093\n",
      "Epoch [5/5] Train Loss: 0.9861 | Train Acc: 64.96% Val Loss: 1.1053 | Val Acc: 60.64%\n"
     ]
    }
   ],
   "source": [
    "# 3. Hyper-Parameters\n",
    "BATCH_SIZE = 20\n",
    "LR = 0.001\n",
    "EPOCHS = 5\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n",
    "val_loader   = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n",
    "test_loader  = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n",
    "model = ResNet18(num_classes=NUM_CLASSES).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LR)\n",
    "\n",
    "best_val_acc = 0.0\n",
    "for epoch in range(EPOCHS):\n",
    "    train_loss, train_acc = train_one_epoch(model, train_loader, optimizer, criterion)\n",
    "    val_loss, val_acc = evaluate(model, val_loader, criterion)\n",
    "    \n",
    "    print(f\"Epoch [{epoch+1}/{EPOCHS}] \"\n",
    "          f\"Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.2f}% | \"\n",
    "          f\"Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.2f}%\")\n",
    "    \n",
    "    # Save best model\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        torch.save(model.state_dict(), \"model_bs_20_lr_0001/best_resnet_model_bs_20_lr_0001_5epoch.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "918c1633-8146-4d61-8d55-e7b3418df9d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final Test Loss: 1.1093, Test Acc: 60.79%\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(\"model_bs_20_lr_0001/best_resnet_model_bs_20_lr_0001_5epoch.pth\"))\n",
    "test_loss, test_acc = evaluate(model, test_loader, criterion)\n",
    "print(f\"\\nFinal Test Loss: {test_loss:.4f}, Test Acc: {test_acc:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0c2ad334-99f0-4501-afbf-24a545c319b5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 60: Loss = 0.4394\n",
      "Batch 120: Loss = 1.1485\n",
      "Batch 180: Loss = 0.2155\n",
      "Batch 240: Loss = 0.4654\n",
      "Batch 300: Loss = 0.5661\n",
      "Batch 360: Loss = 0.2631\n",
      "Batch 420: Loss = 0.7390\n",
      "Batch 480: Loss = 0.4772\n",
      "Batch 540: Loss = 0.4001\n",
      "Batch 600: Loss = 0.5324\n",
      "Batch 660: Loss = 0.4020\n",
      "Batch 720: Loss = 0.4426\n",
      "Batch 780: Loss = 0.3682\n",
      "Batch 840: Loss = 0.8991\n",
      "Batch 900: Loss = 0.4453\n",
      "Batch 960: Loss = 0.4470\n",
      "Batch 1020: Loss = 0.2539\n",
      "Batch 1080: Loss = 0.6637\n",
      "Batch 1140: Loss = 0.2144\n",
      "Batch 1200: Loss = 0.6035\n",
      "Batch 1260: Loss = 0.3757\n",
      "Batch 1320: Loss = 0.3518\n",
      "Batch 1380: Loss = 0.6322\n",
      "Batch 1440: Loss = 0.5881\n",
      "Batch 1500: Loss = 0.4711\n",
      "Batch 1560: Loss = 0.4010\n",
      "Batch 1620: Loss = 0.8366\n",
      "Batch 1680: Loss = 0.6814\n",
      "Batch 1740: Loss = 0.9387\n",
      "Batch 1800: Loss = 0.8650\n",
      "Batch 1860: Loss = 0.4678\n",
      "Batch 1920: Loss = 0.9068\n",
      "Batch 1980: Loss = 0.4785\n",
      "Batch 2040: Loss = 0.6485\n",
      "Batch 2100: Loss = 0.5417\n",
      "Batch 2160: Loss = 0.4534\n",
      "Batch 2220: Loss = 0.4629\n",
      "Batch 2280: Loss = 0.3838\n",
      "Batch 2340: Loss = 0.4416\n",
      "Batch 2400: Loss = 0.7819\n",
      "Batch 2460: Loss = 0.4337\n",
      "Batch 2520: Loss = 0.3949\n",
      "Batch 2580: Loss = 0.4043\n",
      "Batch 2640: Loss = 0.5986\n",
      "Batch 2700: Loss = 0.2857\n",
      "Batch 2760: Loss = 0.7404\n",
      "Batch 2820: Loss = 0.3636\n",
      "Batch 2880: Loss = 0.5679\n",
      "Batch 2940: Loss = 0.3228\n",
      "Batch 3000: Loss = 0.4093\n",
      "Batch 3060: Loss = 0.2342\n",
      "Batch 3120: Loss = 0.3825\n",
      "Batch 3180: Loss = 0.8624\n",
      "Batch 3240: Loss = 0.6706\n",
      "Batch 3300: Loss = 0.6275\n",
      "Batch 3360: Loss = 0.4324\n",
      "Batch 3420: Loss = 0.3092\n",
      "Batch 3480: Loss = 0.7861\n",
      "Batch 3540: Loss = 0.5679\n",
      "Batch 3600: Loss = 0.5020\n",
      "Batch 3660: Loss = 0.8686\n",
      "Batch 3720: Loss = 0.4889\n",
      "Batch 3780: Loss = 0.6328\n",
      "Batch 3840: Loss = 0.4048\n",
      "Batch 3900: Loss = 0.6696\n",
      "Batch 3960: Loss = 0.5702\n",
      "Batch 4020: Loss = 0.9001\n",
      "Batch 4080: Loss = 0.6238\n",
      "Batch 4140: Loss = 0.5547\n",
      "Batch 4200: Loss = 1.2495\n",
      "Batch 4260: Loss = 0.5718\n",
      "Batch 4320: Loss = 1.2897\n",
      "Batch 4380: Loss = 1.2077\n",
      "Batch 4440: Loss = 0.9214\n",
      "Batch 4500: Loss = 0.5368\n",
      "Additional Epoch [1/5] Train Loss: 0.5451 | Train Acc: 80.61% Val Loss: 1.1762 | Val Acc: 62.87%\n",
      "Batch 60: Loss = 0.3929\n",
      "Batch 120: Loss = 0.5408\n",
      "Batch 180: Loss = 0.2066\n",
      "Batch 240: Loss = 0.5675\n",
      "Batch 300: Loss = 0.2520\n",
      "Batch 360: Loss = 0.2829\n",
      "Batch 420: Loss = 0.7735\n",
      "Batch 480: Loss = 0.2857\n",
      "Batch 540: Loss = 0.6118\n",
      "Batch 600: Loss = 0.5166\n",
      "Batch 660: Loss = 0.4436\n",
      "Batch 720: Loss = 0.3506\n",
      "Batch 780: Loss = 0.4465\n",
      "Batch 840: Loss = 0.5263\n",
      "Batch 900: Loss = 0.6253\n",
      "Batch 960: Loss = 0.4839\n",
      "Batch 1020: Loss = 0.5140\n",
      "Batch 1080: Loss = 0.3995\n",
      "Batch 1140: Loss = 0.3114\n",
      "Batch 1200: Loss = 0.2775\n",
      "Batch 1260: Loss = 0.6675\n",
      "Batch 1320: Loss = 0.3379\n",
      "Batch 1380: Loss = 0.2668\n",
      "Batch 1440: Loss = 0.3493\n",
      "Batch 1500: Loss = 0.4906\n",
      "Batch 1560: Loss = 0.2934\n",
      "Batch 1620: Loss = 0.4991\n",
      "Batch 1680: Loss = 0.4880\n",
      "Batch 1740: Loss = 0.3760\n",
      "Batch 1800: Loss = 0.4619\n",
      "Batch 1860: Loss = 0.8612\n",
      "Batch 1920: Loss = 0.7272\n",
      "Batch 1980: Loss = 0.3070\n",
      "Batch 2040: Loss = 0.3375\n",
      "Batch 2100: Loss = 0.5899\n",
      "Batch 2160: Loss = 0.4891\n",
      "Batch 2220: Loss = 0.3888\n",
      "Batch 2280: Loss = 0.5048\n",
      "Batch 2340: Loss = 0.3956\n",
      "Batch 2400: Loss = 0.4869\n",
      "Batch 2460: Loss = 0.7046\n",
      "Batch 2520: Loss = 0.6738\n",
      "Batch 2580: Loss = 0.3289\n",
      "Batch 2640: Loss = 0.2828\n",
      "Batch 2700: Loss = 0.6579\n",
      "Batch 2760: Loss = 0.7701\n",
      "Batch 2820: Loss = 0.4031\n",
      "Batch 2880: Loss = 0.7573\n",
      "Batch 2940: Loss = 0.5771\n",
      "Batch 3000: Loss = 0.6684\n",
      "Batch 3060: Loss = 0.3180\n",
      "Batch 3120: Loss = 0.4491\n",
      "Batch 3180: Loss = 0.7787\n",
      "Batch 3240: Loss = 0.6032\n",
      "Batch 3300: Loss = 0.1715\n",
      "Batch 3360: Loss = 0.4772\n",
      "Batch 3420: Loss = 0.2391\n",
      "Batch 3480: Loss = 0.2663\n",
      "Batch 3540: Loss = 0.4491\n",
      "Batch 3600: Loss = 0.3077\n",
      "Batch 3660: Loss = 0.3570\n",
      "Batch 3720: Loss = 0.8802\n",
      "Batch 3780: Loss = 0.9649\n",
      "Batch 3840: Loss = 0.4020\n",
      "Batch 3900: Loss = 0.4107\n",
      "Batch 3960: Loss = 0.3286\n",
      "Batch 4020: Loss = 0.6892\n",
      "Batch 4080: Loss = 0.3544\n",
      "Batch 4140: Loss = 0.3777\n",
      "Batch 4200: Loss = 0.2002\n",
      "Batch 4260: Loss = 0.2048\n",
      "Batch 4320: Loss = 0.6840\n",
      "Batch 4380: Loss = 0.5478\n",
      "Batch 4440: Loss = 0.2504\n",
      "Batch 4500: Loss = 0.5986\n",
      "Additional Epoch [2/5] Train Loss: 0.4701 | Train Acc: 83.28% Val Loss: 1.2087 | Val Acc: 63.82%\n",
      "Batch 60: Loss = 0.4237\n",
      "Batch 120: Loss = 0.1985\n",
      "Batch 180: Loss = 0.3163\n",
      "Batch 240: Loss = 0.3870\n",
      "Batch 300: Loss = 0.2668\n",
      "Batch 360: Loss = 0.3020\n",
      "Batch 420: Loss = 0.6587\n",
      "Batch 480: Loss = 0.2459\n",
      "Batch 540: Loss = 0.3456\n",
      "Batch 600: Loss = 0.2635\n",
      "Batch 660: Loss = 0.5605\n",
      "Batch 720: Loss = 0.6926\n",
      "Batch 780: Loss = 0.3868\n",
      "Batch 840: Loss = 0.2662\n",
      "Batch 900: Loss = 0.3240\n",
      "Batch 960: Loss = 0.3421\n",
      "Batch 1020: Loss = 1.0054\n",
      "Batch 1080: Loss = 0.2752\n",
      "Batch 1140: Loss = 0.2692\n",
      "Batch 1200: Loss = 0.1521\n",
      "Batch 1260: Loss = 0.4793\n",
      "Batch 1320: Loss = 0.4675\n",
      "Batch 1380: Loss = 0.3572\n",
      "Batch 1440: Loss = 0.1794\n",
      "Batch 1500: Loss = 0.6623\n",
      "Batch 1560: Loss = 0.3485\n",
      "Batch 1620: Loss = 0.5137\n",
      "Batch 1680: Loss = 0.4805\n",
      "Batch 1740: Loss = 0.5255\n",
      "Batch 1800: Loss = 0.6373\n",
      "Batch 1860: Loss = 0.3243\n",
      "Batch 1920: Loss = 0.4804\n",
      "Batch 1980: Loss = 0.6091\n",
      "Batch 2040: Loss = 0.3549\n",
      "Batch 2100: Loss = 0.1605\n",
      "Batch 2160: Loss = 0.3350\n",
      "Batch 2220: Loss = 0.4066\n",
      "Batch 2280: Loss = 0.4475\n",
      "Batch 2340: Loss = 0.7717\n",
      "Batch 2400: Loss = 0.3568\n",
      "Batch 2460: Loss = 0.2512\n",
      "Batch 2520: Loss = 0.5085\n",
      "Batch 2580: Loss = 0.1376\n",
      "Batch 2640: Loss = 0.3774\n",
      "Batch 2700: Loss = 0.8050\n",
      "Batch 2760: Loss = 0.7971\n",
      "Batch 2820: Loss = 0.2160\n",
      "Batch 2880: Loss = 0.5234\n",
      "Batch 2940: Loss = 0.2613\n",
      "Batch 3000: Loss = 0.2252\n",
      "Batch 3060: Loss = 0.7551\n",
      "Batch 3120: Loss = 0.5770\n",
      "Batch 3180: Loss = 0.2012\n",
      "Batch 3240: Loss = 0.8063\n",
      "Batch 3300: Loss = 0.2281\n",
      "Batch 3360: Loss = 0.4084\n",
      "Batch 3420: Loss = 0.4311\n",
      "Batch 3480: Loss = 0.7636\n",
      "Batch 3540: Loss = 0.2019\n",
      "Batch 3600: Loss = 0.4803\n",
      "Batch 3660: Loss = 0.2055\n",
      "Batch 3720: Loss = 0.7809\n",
      "Batch 3780: Loss = 0.6807\n",
      "Batch 3840: Loss = 0.6730\n",
      "Batch 3900: Loss = 0.2674\n",
      "Batch 3960: Loss = 0.1106\n",
      "Batch 4020: Loss = 0.5940\n",
      "Batch 4080: Loss = 0.5391\n",
      "Batch 4140: Loss = 0.1646\n",
      "Batch 4200: Loss = 0.4838\n",
      "Batch 4260: Loss = 0.6354\n",
      "Batch 4320: Loss = 0.4808\n",
      "Batch 4380: Loss = 0.3558\n",
      "Batch 4440: Loss = 0.3954\n",
      "Batch 4500: Loss = 0.8574\n",
      "Additional Epoch [3/5] Train Loss: 0.4080 | Train Acc: 85.39% Val Loss: 1.3006 | Val Acc: 64.03%\n",
      "Batch 60: Loss = 0.3629\n",
      "Batch 120: Loss = 0.4627\n",
      "Batch 180: Loss = 0.4970\n",
      "Batch 240: Loss = 0.1387\n",
      "Batch 300: Loss = 0.2652\n",
      "Batch 360: Loss = 0.2835\n",
      "Batch 420: Loss = 0.2398\n",
      "Batch 480: Loss = 0.5629\n",
      "Batch 540: Loss = 0.2001\n",
      "Batch 600: Loss = 0.4702\n",
      "Batch 660: Loss = 0.2016\n",
      "Batch 720: Loss = 0.0541\n",
      "Batch 780: Loss = 0.1929\n",
      "Batch 840: Loss = 0.1082\n",
      "Batch 900: Loss = 0.4038\n",
      "Batch 960: Loss = 0.6980\n",
      "Batch 1020: Loss = 0.3049\n",
      "Batch 1080: Loss = 0.2308\n",
      "Batch 1140: Loss = 0.3780\n",
      "Batch 1200: Loss = 0.4610\n",
      "Batch 1260: Loss = 0.5168\n",
      "Batch 1320: Loss = 0.1646\n",
      "Batch 1380: Loss = 0.4202\n",
      "Batch 1440: Loss = 0.3069\n",
      "Batch 1500: Loss = 0.5035\n",
      "Batch 1560: Loss = 0.1333\n",
      "Batch 1620: Loss = 0.4371\n",
      "Batch 1680: Loss = 0.3575\n",
      "Batch 1740: Loss = 0.3448\n",
      "Batch 1800: Loss = 0.3488\n",
      "Batch 1860: Loss = 0.2294\n",
      "Batch 1920: Loss = 0.1246\n",
      "Batch 1980: Loss = 0.2075\n",
      "Batch 2040: Loss = 0.2494\n",
      "Batch 2100: Loss = 0.1965\n",
      "Batch 2160: Loss = 0.2262\n",
      "Batch 2220: Loss = 0.6776\n",
      "Batch 2280: Loss = 0.8725\n",
      "Batch 2340: Loss = 0.2500\n",
      "Batch 2400: Loss = 0.3345\n",
      "Batch 2460: Loss = 0.1805\n",
      "Batch 2520: Loss = 0.2915\n",
      "Batch 2580: Loss = 0.3034\n",
      "Batch 2640: Loss = 0.2075\n",
      "Batch 2700: Loss = 0.4281\n",
      "Batch 2760: Loss = 0.3345\n",
      "Batch 2820: Loss = 0.2245\n",
      "Batch 2880: Loss = 0.7349\n",
      "Batch 2940: Loss = 0.2856\n",
      "Batch 3000: Loss = 0.2444\n",
      "Batch 3060: Loss = 0.5672\n",
      "Batch 3120: Loss = 0.1989\n",
      "Batch 3180: Loss = 0.2154\n",
      "Batch 3240: Loss = 0.4427\n",
      "Batch 3300: Loss = 0.3728\n",
      "Batch 3360: Loss = 0.5150\n",
      "Batch 3420: Loss = 0.4763\n",
      "Batch 3480: Loss = 0.2059\n",
      "Batch 3540: Loss = 0.2169\n",
      "Batch 3600: Loss = 0.2132\n",
      "Batch 3660: Loss = 0.4037\n",
      "Batch 3720: Loss = 0.1442\n",
      "Batch 3780: Loss = 0.4566\n",
      "Batch 3840: Loss = 0.2163\n",
      "Batch 3900: Loss = 0.2071\n",
      "Batch 3960: Loss = 0.6125\n",
      "Batch 4020: Loss = 0.3747\n",
      "Batch 4080: Loss = 0.4188\n",
      "Batch 4140: Loss = 0.1577\n",
      "Batch 4200: Loss = 0.3110\n",
      "Batch 4260: Loss = 0.2939\n",
      "Batch 4320: Loss = 0.5038\n",
      "Batch 4380: Loss = 0.5569\n",
      "Batch 4440: Loss = 0.2807\n",
      "Batch 4500: Loss = 0.8202\n",
      "Additional Epoch [4/5] Train Loss: 0.3495 | Train Acc: 87.59% Val Loss: 1.3235 | Val Acc: 63.27%\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(\"model_bs_20_lr_0001/best_resnet_model_bs_20_lr_0001_5epoch.pth\"))\n",
    "for epoch in range(8):\n",
    "    train_loss, train_acc = train_one_epoch(model, train_loader, optimizer, criterion)\n",
    "    val_loss, val_acc = evaluate(model, val_loader, criterion)\n",
    "    print(f\"Additional Epoch [{epoch+1}/8] \"\n",
    "          f\"Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.2f}% | \"\n",
    "          f\"Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.2f}%\")\n",
    "    torch.save(model.state_dict(), f\"best_resnet18_extended_epoch{epoch+1}.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "57d05dd944fee131",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-28T02:05:19.923214Z",
     "start_time": "2025-03-28T02:04:33.405857Z"
    },
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 LR 0.0001 and batch_size 40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.2.4 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"<string>\", line 1, in <module>\n",
      "  File \"/usr/local/Cellar/python@3.11/3.11.9_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/multiprocessing/spawn.py\", line 122, in spawn_main\n",
      "    exitcode = _main(fd, parent_sentinel)\n",
      "  File \"/usr/local/Cellar/python@3.11/3.11.9_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/multiprocessing/spawn.py\", line 132, in _main\n",
      "    self = reduction.pickle.load(from_parent)\n",
      "  File \"/Users/surenmnatsakanyan/Desktop/Warsaw_Courses/Semester2/Deep Learning/CNN_1-Project1/venv/lib/python3.11/site-packages/torch/__init__.py\", line 1477, in <module>\n",
      "    from .functional import *  # noqa: F403\n",
      "  File \"/Users/surenmnatsakanyan/Desktop/Warsaw_Courses/Semester2/Deep Learning/CNN_1-Project1/venv/lib/python3.11/site-packages/torch/functional.py\", line 9, in <module>\n",
      "    import torch.nn.functional as F\n",
      "  File \"/Users/surenmnatsakanyan/Desktop/Warsaw_Courses/Semester2/Deep Learning/CNN_1-Project1/venv/lib/python3.11/site-packages/torch/nn/__init__.py\", line 1, in <module>\n",
      "    from .modules import *  # noqa: F403\n",
      "  File \"/Users/surenmnatsakanyan/Desktop/Warsaw_Courses/Semester2/Deep Learning/CNN_1-Project1/venv/lib/python3.11/site-packages/torch/nn/modules/__init__.py\", line 35, in <module>\n",
      "    from .transformer import TransformerEncoder, TransformerDecoder, \\\n",
      "  File \"/Users/surenmnatsakanyan/Desktop/Warsaw_Courses/Semester2/Deep Learning/CNN_1-Project1/venv/lib/python3.11/site-packages/torch/nn/modules/transformer.py\", line 20, in <module>\n",
      "    device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\n",
      "/Users/surenmnatsakanyan/Desktop/Warsaw_Courses/Semester2/Deep Learning/CNN_1-Project1/venv/lib/python3.11/site-packages/torch/nn/modules/transformer.py:20: UserWarning: Failed to initialize NumPy: _ARRAY_API not found (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/utils/tensor_numpy.cpp:84.)\n",
      "  device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\n",
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.2.4 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"<string>\", line 1, in <module>\n",
      "  File \"/usr/local/Cellar/python@3.11/3.11.9_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/multiprocessing/spawn.py\", line 122, in spawn_main\n",
      "    exitcode = _main(fd, parent_sentinel)\n",
      "  File \"/usr/local/Cellar/python@3.11/3.11.9_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/multiprocessing/spawn.py\", line 132, in _main\n",
      "    self = reduction.pickle.load(from_parent)\n",
      "  File \"/Users/surenmnatsakanyan/Desktop/Warsaw_Courses/Semester2/Deep Learning/CNN_1-Project1/venv/lib/python3.11/site-packages/torch/__init__.py\", line 1477, in <module>\n",
      "    from .functional import *  # noqa: F403\n",
      "  File \"/Users/surenmnatsakanyan/Desktop/Warsaw_Courses/Semester2/Deep Learning/CNN_1-Project1/venv/lib/python3.11/site-packages/torch/functional.py\", line 9, in <module>\n",
      "    import torch.nn.functional as F\n",
      "  File \"/Users/surenmnatsakanyan/Desktop/Warsaw_Courses/Semester2/Deep Learning/CNN_1-Project1/venv/lib/python3.11/site-packages/torch/nn/__init__.py\", line 1, in <module>\n",
      "    from .modules import *  # noqa: F403\n",
      "  File \"/Users/surenmnatsakanyan/Desktop/Warsaw_Courses/Semester2/Deep Learning/CNN_1-Project1/venv/lib/python3.11/site-packages/torch/nn/modules/__init__.py\", line 35, in <module>\n",
      "    from .transformer import TransformerEncoder, TransformerDecoder, \\\n",
      "  File \"/Users/surenmnatsakanyan/Desktop/Warsaw_Courses/Semester2/Deep Learning/CNN_1-Project1/venv/lib/python3.11/site-packages/torch/nn/modules/transformer.py\", line 20, in <module>\n",
      "    device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\n",
      "/Users/surenmnatsakanyan/Desktop/Warsaw_Courses/Semester2/Deep Learning/CNN_1-Project1/venv/lib/python3.11/site-packages/torch/nn/modules/transformer.py:20: UserWarning: Failed to initialize NumPy: _ARRAY_API not found (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/utils/tensor_numpy.cpp:84.)\n",
      "  device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\n",
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "learning_rates = [0.0001,0.001, 0.01, 0.1]\n",
    "batch_sizes = [40, 60, 80, 100]\n",
    "EPOCHS = 10\n",
    "for lr in learning_rates:\n",
    "    for bs in batch_sizes:\n",
    "        train_loader = DataLoader(train_dataset, batch_size=bs, shuffle=True, num_workers=2)\n",
    "        val_loader   = DataLoader(val_dataset, batch_size=bs, shuffle=False, num_workers=2)\n",
    "        test_loader  = DataLoader(test_dataset, batch_size=bs, shuffle=False, num_workers=2)\n",
    "        \n",
    "        # Initialize model and optimizer for current hyper-parameter combination\n",
    "        model = ResNet18(num_classes=NUM_CLASSES).to(device)\n",
    "        optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        \n",
    "        best_val_acc = 0.0\n",
    "        checkpoint = f\"resnet18_lr{lr}_bs{bs}_best.pth\"\n",
    "        \n",
    "        for epoch in range(EPOCHS):\n",
    "            print(f\"Epoch {epoch+1}/{EPOCHS} LR {lr} and batch_size {bs}\")\n",
    "            train_loss, train_acc = train_one_epoch(model, train_loader, optimizer, criterion)\n",
    "            val_loss, val_acc = evaluate(model, val_loader, criterion)\n",
    "            print(f\"  Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.2f}% | \"\n",
    "                  f\"Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.2f}%\")\n",
    "            if val_acc > best_val_acc:\n",
    "                best_val_acc = val_acc\n",
    "                torch.save(model.state_dict(), checkpoint)\n",
    "        \n",
    "        model.load_state_dict(torch.load(checkpoint))\n",
    "        test_loss, test_acc = evaluate(model, test_loader, criterion)\n",
    "        print(f\"Final Test Loss: {test_loss:.4f}, Test Acc: {test_acc:.2f}%\")        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "529a3c85-4b59-4509-a580-0e9d4876172d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-28T01:55:51.217677Z",
     "start_time": "2025-03-28T01:55:50.175586Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/13 LR 0.001 and batch_size 100\n",
      "Batch 60: Loss = 1.8833\n",
      "Batch 120: Loss = 1.6927\n",
      "Batch 180: Loss = 1.6174\n",
      "Batch 240: Loss = 1.6187\n",
      "Batch 300: Loss = 1.4798\n",
      "Batch 360: Loss = 1.4686\n",
      "Batch 420: Loss = 1.4245\n",
      "Batch 480: Loss = 1.4150\n",
      "Batch 540: Loss = 1.3438\n",
      "Batch 600: Loss = 1.3819\n",
      "Batch 660: Loss = 1.4622\n",
      "Batch 720: Loss = 1.6318\n",
      "Batch 780: Loss = 1.3914\n",
      "Batch 840: Loss = 1.3612\n",
      "Batch 900: Loss = 1.4747\n",
      "  Train Loss: 1.5375 | Train Acc: 43.54% | Val Loss: 1.6150 | Val Acc: 42.62%\n",
      "Epoch 2/13 LR 0.001 and batch_size 100\n",
      "Batch 60: Loss = 1.3586\n",
      "Batch 120: Loss = 1.2434\n",
      "Batch 180: Loss = 1.3241\n",
      "Batch 240: Loss = 1.2270\n",
      "Batch 300: Loss = 1.3583\n",
      "Batch 360: Loss = 1.2931\n",
      "Batch 420: Loss = 1.4432\n",
      "Batch 480: Loss = 1.2270\n",
      "Batch 540: Loss = 1.2552\n",
      "Batch 600: Loss = 0.9872\n",
      "Batch 660: Loss = 1.2961\n",
      "Batch 720: Loss = 1.3244\n",
      "Batch 780: Loss = 1.3519\n",
      "Batch 840: Loss = 1.2644\n",
      "Batch 900: Loss = 1.2218\n",
      "  Train Loss: 1.2592 | Train Acc: 54.28% | Val Loss: 1.4024 | Val Acc: 48.62%\n",
      "Epoch 3/13 LR 0.001 and batch_size 100\n",
      "Batch 60: Loss = 1.1514\n",
      "Batch 120: Loss = 1.2297\n",
      "Batch 180: Loss = 1.2328\n",
      "Batch 240: Loss = 1.1379\n",
      "Batch 300: Loss = 1.0305\n",
      "Batch 360: Loss = 1.1778\n",
      "Batch 420: Loss = 1.2213\n",
      "Batch 480: Loss = 1.1503\n",
      "Batch 540: Loss = 1.3047\n",
      "Batch 600: Loss = 1.0883\n",
      "Batch 660: Loss = 1.1134\n",
      "Batch 720: Loss = 1.1619\n",
      "Batch 780: Loss = 1.0579\n",
      "Batch 840: Loss = 1.1372\n",
      "Batch 900: Loss = 1.0742\n",
      "  Train Loss: 1.1338 | Train Acc: 59.33% | Val Loss: 1.2480 | Val Acc: 55.83%\n",
      "Epoch 4/13 LR 0.001 and batch_size 100\n",
      "Batch 60: Loss = 1.1451\n",
      "Batch 120: Loss = 1.1426\n",
      "Batch 180: Loss = 1.0465\n",
      "Batch 240: Loss = 0.8805\n",
      "Batch 300: Loss = 1.0680\n",
      "Batch 360: Loss = 0.9965\n",
      "Batch 420: Loss = 0.9607\n",
      "Batch 480: Loss = 1.0053\n",
      "Batch 540: Loss = 0.9310\n",
      "Batch 600: Loss = 1.2061\n",
      "Batch 660: Loss = 1.1833\n",
      "Batch 720: Loss = 1.1440\n",
      "Batch 780: Loss = 1.1051\n",
      "Batch 840: Loss = 1.0130\n",
      "Batch 900: Loss = 0.9990\n",
      "  Train Loss: 1.0421 | Train Acc: 62.80% | Val Loss: 1.1863 | Val Acc: 58.18%\n",
      "Epoch 5/13 LR 0.001 and batch_size 100\n",
      "Batch 60: Loss = 0.8411\n",
      "Batch 120: Loss = 0.9905\n",
      "Batch 180: Loss = 0.9443\n",
      "Batch 240: Loss = 0.9461\n",
      "Batch 300: Loss = 0.8498\n",
      "Batch 360: Loss = 1.2937\n",
      "Batch 420: Loss = 0.8152\n",
      "Batch 480: Loss = 1.0314\n",
      "Batch 540: Loss = 0.7307\n",
      "Batch 600: Loss = 1.0110\n",
      "Batch 660: Loss = 0.8502\n",
      "Batch 720: Loss = 0.8726\n",
      "Batch 780: Loss = 1.0870\n",
      "Batch 840: Loss = 0.8803\n",
      "Batch 900: Loss = 0.9361\n",
      "  Train Loss: 0.9691 | Train Acc: 65.56% | Val Loss: 1.1542 | Val Acc: 59.43%\n",
      "Epoch 6/13 LR 0.001 and batch_size 100\n",
      "Batch 60: Loss = 0.9257\n",
      "Batch 120: Loss = 1.0381\n",
      "Batch 180: Loss = 0.9710\n",
      "Batch 240: Loss = 0.9106\n",
      "Batch 300: Loss = 1.0388\n",
      "Batch 360: Loss = 0.7904\n",
      "Batch 420: Loss = 0.9456\n",
      "Batch 480: Loss = 0.9536\n",
      "Batch 540: Loss = 0.8552\n",
      "Batch 600: Loss = 0.9218\n",
      "Batch 660: Loss = 0.9857\n",
      "Batch 720: Loss = 0.9201\n",
      "Batch 780: Loss = 1.0065\n",
      "Batch 840: Loss = 0.9415\n",
      "Batch 900: Loss = 0.7696\n",
      "  Train Loss: 0.8983 | Train Acc: 67.99% | Val Loss: 1.1440 | Val Acc: 60.34%\n",
      "Epoch 7/13 LR 0.001 and batch_size 100\n",
      "Batch 60: Loss = 0.8340\n",
      "Batch 120: Loss = 0.8025\n",
      "Batch 180: Loss = 0.7857\n",
      "Batch 240: Loss = 0.9137\n",
      "Batch 300: Loss = 1.0996\n",
      "Batch 360: Loss = 0.9552\n",
      "Batch 420: Loss = 1.0389\n",
      "Batch 480: Loss = 0.8830\n",
      "Batch 540: Loss = 0.7351\n",
      "Batch 600: Loss = 0.8324\n",
      "Batch 660: Loss = 0.6800\n",
      "Batch 720: Loss = 0.6333\n",
      "Batch 780: Loss = 1.0156\n",
      "Batch 840: Loss = 0.8621\n",
      "Batch 900: Loss = 1.1263\n",
      "  Train Loss: 0.8323 | Train Acc: 70.47% | Val Loss: 1.1434 | Val Acc: 60.83%\n",
      "Epoch 8/13 LR 0.001 and batch_size 100\n",
      "Batch 60: Loss = 0.8024\n",
      "Batch 120: Loss = 0.7375\n",
      "Batch 180: Loss = 0.6166\n",
      "Batch 240: Loss = 0.5925\n",
      "Batch 300: Loss = 0.6156\n",
      "Batch 360: Loss = 0.7783\n",
      "Batch 420: Loss = 0.8106\n",
      "Batch 480: Loss = 0.6566\n",
      "Batch 540: Loss = 0.6652\n",
      "Batch 600: Loss = 0.9683\n",
      "Batch 660: Loss = 0.7160\n",
      "Batch 720: Loss = 0.6903\n",
      "Batch 780: Loss = 0.7502\n",
      "Batch 840: Loss = 0.9635\n",
      "Batch 900: Loss = 0.9672\n",
      "  Train Loss: 0.7622 | Train Acc: 72.93% | Val Loss: 1.0705 | Val Acc: 62.57%\n",
      "Epoch 9/13 LR 0.001 and batch_size 100\n",
      "Batch 60: Loss = 0.4659\n",
      "Batch 120: Loss = 0.7165\n",
      "Batch 180: Loss = 0.4877\n",
      "Batch 240: Loss = 0.6415\n",
      "Batch 300: Loss = 0.6629\n",
      "Batch 360: Loss = 0.6534\n",
      "Batch 420: Loss = 0.6823\n",
      "Batch 480: Loss = 0.6816\n",
      "Batch 540: Loss = 0.6931\n",
      "Batch 600: Loss = 0.6901\n",
      "Batch 660: Loss = 0.7494\n",
      "Batch 720: Loss = 0.6498\n",
      "Batch 780: Loss = 0.7630\n",
      "Batch 840: Loss = 0.7019\n",
      "Batch 900: Loss = 0.7448\n",
      "  Train Loss: 0.6913 | Train Acc: 75.49% | Val Loss: 1.0865 | Val Acc: 62.59%\n",
      "Epoch 10/13 LR 0.001 and batch_size 100\n",
      "Batch 60: Loss = 0.4650\n",
      "Batch 120: Loss = 0.5152\n",
      "Batch 180: Loss = 0.6640\n",
      "Batch 240: Loss = 0.5994\n",
      "Batch 300: Loss = 0.5415\n",
      "Batch 360: Loss = 0.4723\n",
      "Batch 420: Loss = 0.6396\n",
      "Batch 480: Loss = 0.5833\n",
      "Batch 540: Loss = 0.6121\n",
      "Batch 600: Loss = 0.6152\n",
      "Batch 660: Loss = 0.8673\n",
      "Batch 720: Loss = 0.5415\n",
      "Batch 780: Loss = 0.5133\n",
      "Batch 840: Loss = 0.6021\n",
      "Batch 900: Loss = 0.6388\n",
      "  Train Loss: 0.6236 | Train Acc: 77.87% | Val Loss: 1.1476 | Val Acc: 61.44%\n",
      "Epoch 11/13 LR 0.001 and batch_size 100\n",
      "Batch 60: Loss = 0.3950\n",
      "Batch 120: Loss = 0.6041\n",
      "Batch 180: Loss = 0.5251\n",
      "Batch 240: Loss = 0.4498\n",
      "Batch 300: Loss = 0.4517\n",
      "Batch 360: Loss = 0.4918\n",
      "Batch 420: Loss = 0.5629\n",
      "Batch 480: Loss = 0.5299\n",
      "Batch 540: Loss = 0.6316\n",
      "Batch 600: Loss = 0.4654\n",
      "Batch 660: Loss = 0.6206\n",
      "Batch 720: Loss = 0.5395\n",
      "Batch 780: Loss = 0.5313\n",
      "Batch 840: Loss = 0.7356\n",
      "Batch 900: Loss = 0.6927\n",
      "  Train Loss: 0.5494 | Train Acc: 80.51% | Val Loss: 1.1236 | Val Acc: 63.82%\n",
      "Epoch 12/13 LR 0.001 and batch_size 100\n",
      "Batch 60: Loss = 0.3522\n",
      "Batch 120: Loss = 0.4837\n",
      "Batch 180: Loss = 0.4156\n",
      "Batch 240: Loss = 0.4230\n",
      "Batch 300: Loss = 0.5676\n",
      "Batch 360: Loss = 0.3365\n",
      "Batch 420: Loss = 0.5284\n",
      "Batch 480: Loss = 0.4650\n",
      "Batch 540: Loss = 0.4004\n",
      "Batch 600: Loss = 0.5664\n",
      "Batch 660: Loss = 0.4274\n",
      "Batch 720: Loss = 0.5026\n",
      "Batch 780: Loss = 0.6488\n",
      "Batch 840: Loss = 0.5402\n",
      "Batch 900: Loss = 0.4947\n",
      "  Train Loss: 0.4835 | Train Acc: 82.86% | Val Loss: 1.1444 | Val Acc: 63.93%\n",
      "Epoch 13/13 LR 0.001 and batch_size 100\n",
      "Batch 60: Loss = 0.2891\n",
      "Batch 120: Loss = 0.2725\n",
      "Batch 180: Loss = 0.7166\n",
      "Batch 240: Loss = 0.3463\n",
      "Batch 300: Loss = 0.3584\n",
      "Batch 360: Loss = 0.3628\n",
      "Batch 420: Loss = 0.4377\n",
      "Batch 480: Loss = 0.3536\n",
      "Batch 540: Loss = 0.3487\n",
      "Batch 600: Loss = 0.4256\n",
      "Batch 660: Loss = 0.4673\n",
      "Batch 720: Loss = 0.4235\n",
      "Batch 780: Loss = 0.6069\n",
      "Batch 840: Loss = 0.4794\n",
      "Batch 900: Loss = 0.4172\n",
      "  Train Loss: 0.4246 | Train Acc: 85.01% | Val Loss: 1.2937 | Val Acc: 62.08%\n",
      "Final Test Loss: 1.1480, Test Acc: 64.04%\n",
      "Epoch 1/13 LR 0.001 and batch_size 100\n",
      "Batch 60: Loss = 1.6872\n",
      "Batch 120: Loss = 1.8299\n",
      "Batch 180: Loss = 1.4934\n",
      "Batch 240: Loss = 1.5543\n",
      "Batch 300: Loss = 1.5680\n",
      "Batch 360: Loss = 1.7281\n",
      "Batch 420: Loss = 1.5554\n",
      "Batch 480: Loss = 1.4338\n",
      "Batch 540: Loss = 1.6056\n",
      "Batch 600: Loss = 1.3347\n",
      "Batch 660: Loss = 1.4129\n",
      "Batch 720: Loss = 1.4064\n",
      "Batch 780: Loss = 1.4451\n",
      "Batch 840: Loss = 1.2752\n",
      "Batch 900: Loss = 1.4102\n",
      "  Train Loss: 1.5519 | Train Acc: 43.04% | Val Loss: 1.3736 | Val Acc: 49.42%\n",
      "Epoch 2/13 LR 0.001 and batch_size 100\n",
      "Batch 60: Loss = 1.4583\n",
      "Batch 120: Loss = 1.3373\n",
      "Batch 180: Loss = 1.1874\n",
      "Batch 240: Loss = 1.3115\n",
      "Batch 300: Loss = 1.3574\n",
      "Batch 360: Loss = 1.2115\n",
      "Batch 420: Loss = 1.3490\n",
      "Batch 480: Loss = 1.2346\n",
      "Batch 540: Loss = 1.2836\n",
      "Batch 600: Loss = 1.5286\n",
      "Batch 660: Loss = 1.0505\n",
      "Batch 720: Loss = 1.4139\n",
      "Batch 780: Loss = 1.2274\n",
      "Batch 840: Loss = 1.3287\n",
      "Batch 900: Loss = 1.0945\n",
      "  Train Loss: 1.2918 | Train Acc: 53.50% | Val Loss: 1.3682 | Val Acc: 51.60%\n",
      "Epoch 3/13 LR 0.001 and batch_size 100\n",
      "Batch 60: Loss = 1.3636\n",
      "Batch 120: Loss = 1.1533\n",
      "Batch 180: Loss = 1.4094\n",
      "Batch 240: Loss = 1.2266\n",
      "Batch 300: Loss = 1.0445\n",
      "Batch 360: Loss = 0.9993\n",
      "Batch 420: Loss = 1.4758\n",
      "Batch 480: Loss = 1.2923\n",
      "Batch 540: Loss = 0.9963\n",
      "Batch 600: Loss = 1.0958\n",
      "Batch 660: Loss = 1.4118\n",
      "Batch 720: Loss = 1.0396\n",
      "Batch 780: Loss = 1.3789\n",
      "Batch 840: Loss = 1.1629\n",
      "Batch 900: Loss = 1.3010\n",
      "  Train Loss: 1.1951 | Train Acc: 57.27% | Val Loss: 1.3779 | Val Acc: 50.68%\n",
      "Epoch 4/13 LR 0.001 and batch_size 100\n",
      "Batch 60: Loss = 1.0003\n",
      "Batch 120: Loss = 1.1887\n",
      "Batch 180: Loss = 1.1289\n",
      "Batch 240: Loss = 1.1005\n",
      "Batch 300: Loss = 1.2268\n",
      "Batch 360: Loss = 1.1329\n",
      "Batch 420: Loss = 1.1438\n",
      "Batch 480: Loss = 1.0896\n",
      "Batch 540: Loss = 1.2172\n",
      "Batch 600: Loss = 1.2341\n",
      "Batch 660: Loss = 0.9464\n",
      "Batch 720: Loss = 1.3529\n",
      "Batch 780: Loss = 1.5014\n",
      "Batch 840: Loss = 1.1119\n",
      "Batch 900: Loss = 1.1281\n",
      "  Train Loss: 1.1238 | Train Acc: 60.00% | Val Loss: 1.7265 | Val Acc: 44.09%\n",
      "Epoch 5/13 LR 0.001 and batch_size 100\n",
      "Batch 60: Loss = 1.0884\n",
      "Batch 120: Loss = 0.9722\n",
      "Batch 180: Loss = 1.0438\n",
      "Batch 240: Loss = 0.9028\n",
      "Batch 300: Loss = 1.0640\n",
      "Batch 360: Loss = 1.2146\n",
      "Batch 420: Loss = 1.2843\n",
      "Batch 480: Loss = 1.2269\n",
      "Batch 540: Loss = 0.9861\n",
      "Batch 600: Loss = 0.9573\n",
      "Batch 660: Loss = 0.9760\n",
      "Batch 720: Loss = 1.0013\n",
      "Batch 780: Loss = 1.1922\n",
      "Batch 840: Loss = 1.0922\n",
      "Batch 900: Loss = 1.0908\n",
      "  Train Loss: 1.0721 | Train Acc: 62.05% | Val Loss: 1.2647 | Val Acc: 55.17%\n",
      "Epoch 6/13 LR 0.001 and batch_size 100\n",
      "Batch 60: Loss = 1.1608\n",
      "Batch 120: Loss = 1.0563\n",
      "Batch 180: Loss = 0.9808\n",
      "Batch 240: Loss = 0.9551\n",
      "Batch 300: Loss = 0.8703\n",
      "Batch 360: Loss = 0.9582\n",
      "Batch 420: Loss = 1.0188\n",
      "Batch 480: Loss = 1.1284\n",
      "Batch 540: Loss = 0.9886\n",
      "Batch 600: Loss = 1.0618\n",
      "Batch 660: Loss = 1.0341\n",
      "Batch 720: Loss = 0.8662\n",
      "Batch 780: Loss = 0.9637\n",
      "Batch 840: Loss = 1.2729\n",
      "Batch 900: Loss = 0.9843\n",
      "  Train Loss: 1.0217 | Train Acc: 63.88% | Val Loss: 1.1342 | Val Acc: 59.68%\n",
      "Epoch 7/13 LR 0.001 and batch_size 100\n",
      "Batch 60: Loss = 0.9375\n",
      "Batch 120: Loss = 1.0804\n",
      "Batch 180: Loss = 0.9779\n",
      "Batch 240: Loss = 1.0532\n",
      "Batch 300: Loss = 0.9746\n",
      "Batch 360: Loss = 0.8271\n",
      "Batch 420: Loss = 0.9040\n",
      "Batch 480: Loss = 1.1942\n",
      "Batch 540: Loss = 1.0725\n",
      "Batch 600: Loss = 1.1020\n",
      "Batch 660: Loss = 0.9900\n",
      "Batch 720: Loss = 1.3840\n",
      "Batch 780: Loss = 0.9150\n",
      "Batch 840: Loss = 0.9788\n",
      "Batch 900: Loss = 1.0910\n",
      "  Train Loss: 0.9849 | Train Acc: 65.11% | Val Loss: 1.1555 | Val Acc: 58.75%\n",
      "Epoch 8/13 LR 0.001 and batch_size 100\n",
      "Batch 60: Loss = 0.8560\n",
      "Batch 120: Loss = 1.0022\n",
      "Batch 180: Loss = 0.7755\n",
      "Batch 240: Loss = 1.0285\n",
      "Batch 300: Loss = 0.8025\n",
      "Batch 360: Loss = 1.0044\n",
      "Batch 420: Loss = 1.1587\n",
      "Batch 480: Loss = 1.0607\n",
      "Batch 540: Loss = 0.8440\n",
      "Batch 600: Loss = 1.0517\n",
      "Batch 660: Loss = 0.8415\n",
      "Batch 720: Loss = 1.1384\n",
      "Batch 780: Loss = 1.0567\n",
      "Batch 840: Loss = 1.1612\n",
      "Batch 900: Loss = 1.0824\n",
      "  Train Loss: 0.9496 | Train Acc: 66.37% | Val Loss: 1.1834 | Val Acc: 58.47%\n",
      "Epoch 9/13 LR 0.001 and batch_size 100\n",
      "Batch 60: Loss = 0.9156\n",
      "Batch 120: Loss = 0.9929\n",
      "Batch 180: Loss = 0.7971\n",
      "Batch 240: Loss = 0.9324\n",
      "Batch 300: Loss = 0.8237\n",
      "Batch 360: Loss = 1.0444\n",
      "Batch 420: Loss = 1.1617\n",
      "Batch 480: Loss = 0.9192\n",
      "Batch 540: Loss = 1.0548\n",
      "Batch 600: Loss = 0.9413\n",
      "Batch 660: Loss = 0.9002\n",
      "Batch 720: Loss = 0.9386\n",
      "Batch 780: Loss = 0.8948\n",
      "Batch 840: Loss = 0.8257\n",
      "Batch 900: Loss = 0.8259\n",
      "  Train Loss: 0.9187 | Train Acc: 67.57% | Val Loss: 1.0840 | Val Acc: 61.30%\n",
      "Epoch 10/13 LR 0.001 and batch_size 100\n",
      "Batch 60: Loss = 0.8740\n",
      "Batch 120: Loss = 0.8642\n",
      "Batch 180: Loss = 0.7798\n",
      "Batch 240: Loss = 0.7110\n",
      "Batch 300: Loss = 0.8453\n",
      "Batch 360: Loss = 0.8918\n",
      "Batch 420: Loss = 0.9805\n",
      "Batch 480: Loss = 1.1009\n",
      "Batch 540: Loss = 0.6965\n",
      "Batch 600: Loss = 0.8717\n",
      "Batch 660: Loss = 0.7218\n",
      "Batch 720: Loss = 1.0848\n",
      "Batch 780: Loss = 0.8650\n",
      "Batch 840: Loss = 0.8405\n",
      "Batch 900: Loss = 0.9767\n",
      "  Train Loss: 0.8893 | Train Acc: 68.77% | Val Loss: 1.1041 | Val Acc: 61.22%\n",
      "Epoch 11/13 LR 0.001 and batch_size 100\n",
      "Batch 60: Loss = 0.8635\n",
      "Batch 120: Loss = 0.8333\n",
      "Batch 180: Loss = 0.8346\n",
      "Batch 240: Loss = 0.6911\n",
      "Batch 300: Loss = 0.9669\n",
      "Batch 360: Loss = 0.9755\n",
      "Batch 420: Loss = 0.7683\n",
      "Batch 480: Loss = 0.9841\n",
      "Batch 540: Loss = 0.8968\n",
      "Batch 600: Loss = 1.0095\n",
      "Batch 660: Loss = 0.7269\n",
      "Batch 720: Loss = 0.9292\n",
      "Batch 780: Loss = 0.8336\n",
      "Batch 840: Loss = 1.0471\n",
      "Batch 900: Loss = 0.7930\n",
      "  Train Loss: 0.8637 | Train Acc: 69.52% | Val Loss: 1.1501 | Val Acc: 59.72%\n",
      "Epoch 12/13 LR 0.001 and batch_size 100\n",
      "Batch 60: Loss = 0.6830\n",
      "Batch 120: Loss = 0.7105\n",
      "Batch 180: Loss = 0.7844\n",
      "Batch 240: Loss = 0.9385\n",
      "Batch 300: Loss = 0.8877\n",
      "Batch 360: Loss = 1.0129\n",
      "Batch 420: Loss = 0.8853\n",
      "Batch 480: Loss = 0.6682\n",
      "Batch 540: Loss = 0.7002\n",
      "Batch 600: Loss = 0.9183\n",
      "Batch 660: Loss = 0.8426\n",
      "Batch 720: Loss = 0.7947\n",
      "Batch 780: Loss = 0.7641\n",
      "Batch 840: Loss = 0.9535\n",
      "Batch 900: Loss = 0.7602\n",
      "  Train Loss: 0.8413 | Train Acc: 70.41% | Val Loss: 1.1930 | Val Acc: 58.38%\n",
      "Epoch 13/13 LR 0.001 and batch_size 100\n",
      "Batch 60: Loss = 0.8832\n",
      "Batch 120: Loss = 0.6998\n",
      "Batch 180: Loss = 0.9698\n",
      "Batch 240: Loss = 0.8273\n",
      "Batch 300: Loss = 0.8664\n",
      "Batch 360: Loss = 0.7814\n",
      "Batch 420: Loss = 0.8137\n",
      "Batch 480: Loss = 0.7530\n",
      "Batch 540: Loss = 0.5491\n",
      "Batch 600: Loss = 0.9235\n",
      "Batch 660: Loss = 0.8793\n",
      "Batch 720: Loss = 0.5187\n",
      "Batch 780: Loss = 0.9451\n",
      "Batch 840: Loss = 1.0479\n",
      "Batch 900: Loss = 0.8984\n",
      "  Train Loss: 0.8174 | Train Acc: 71.23% | Val Loss: 1.2373 | Val Acc: 58.36%\n",
      "Final Test Loss: 1.0914, Test Acc: 61.00%\n",
      "Epoch 1/13 LR 0.001 and batch_size 100\n",
      "Batch 60: Loss = 1.8491\n",
      "Batch 120: Loss = 1.6373\n",
      "Batch 180: Loss = 1.7037\n",
      "Batch 240: Loss = 1.7669\n",
      "Batch 300: Loss = 1.7492\n",
      "Batch 360: Loss = 1.5151\n",
      "Batch 420: Loss = 1.5579\n",
      "Batch 480: Loss = 1.5868\n",
      "Batch 540: Loss = 1.6992\n",
      "Batch 600: Loss = 1.6263\n",
      "Batch 660: Loss = 1.4025\n",
      "Batch 720: Loss = 1.5209\n",
      "Batch 780: Loss = 1.4759\n",
      "Batch 840: Loss = 1.6232\n",
      "Batch 900: Loss = 1.6117\n",
      "  Train Loss: 1.6167 | Train Acc: 40.50% | Val Loss: 1.5342 | Val Acc: 43.64%\n",
      "Epoch 2/13 LR 0.001 and batch_size 100\n",
      "Batch 60: Loss = 1.2879\n",
      "Batch 120: Loss = 1.4904\n",
      "Batch 180: Loss = 1.4180\n",
      "Batch 240: Loss = 1.2868\n",
      "Batch 300: Loss = 1.4991\n",
      "Batch 360: Loss = 1.5218\n",
      "Batch 420: Loss = 1.4511\n",
      "Batch 480: Loss = 1.4036\n",
      "Batch 540: Loss = 1.5286\n",
      "Batch 600: Loss = 1.1888\n",
      "Batch 660: Loss = 1.4824\n",
      "Batch 720: Loss = 1.4923\n",
      "Batch 780: Loss = 1.4319\n",
      "Batch 840: Loss = 1.4382\n",
      "Batch 900: Loss = 1.4674\n",
      "  Train Loss: 1.4612 | Train Acc: 47.23% | Val Loss: 1.7546 | Val Acc: 36.39%\n",
      "Epoch 3/13 LR 0.001 and batch_size 100\n",
      "Batch 60: Loss = 1.3894\n",
      "Batch 120: Loss = 1.4241\n",
      "Batch 180: Loss = 1.5028\n",
      "Batch 240: Loss = 1.4677\n",
      "Batch 300: Loss = 1.6497\n",
      "Batch 360: Loss = 1.3540\n",
      "Batch 420: Loss = 1.2704\n",
      "Batch 480: Loss = 1.2986\n",
      "Batch 540: Loss = 1.4328\n",
      "Batch 600: Loss = 1.4212\n",
      "Batch 660: Loss = 1.3242\n",
      "Batch 720: Loss = 1.3047\n",
      "Batch 780: Loss = 1.4927\n",
      "Batch 840: Loss = 1.0930\n",
      "Batch 900: Loss = 1.4087\n",
      "  Train Loss: 1.3922 | Train Acc: 50.17% | Val Loss: 1.6737 | Val Acc: 40.81%\n",
      "Epoch 4/13 LR 0.001 and batch_size 100\n",
      "Batch 60: Loss = 1.3863\n",
      "Batch 120: Loss = 1.4373\n",
      "Batch 180: Loss = 1.4428\n",
      "Batch 240: Loss = 1.3103\n",
      "Batch 300: Loss = 1.4451\n",
      "Batch 360: Loss = 1.1770\n",
      "Batch 420: Loss = 1.2154\n",
      "Batch 480: Loss = 1.4518\n",
      "Batch 540: Loss = 1.4064\n",
      "Batch 600: Loss = 1.3074\n",
      "Batch 660: Loss = 1.4899\n",
      "Batch 720: Loss = 1.5234\n",
      "Batch 780: Loss = 1.3013\n",
      "Batch 840: Loss = 1.4006\n",
      "Batch 900: Loss = 1.3855\n",
      "  Train Loss: 1.3193 | Train Acc: 53.01% | Val Loss: 1.4464 | Val Acc: 48.14%\n",
      "Epoch 5/13 LR 0.001 and batch_size 100\n",
      "Batch 60: Loss = 1.2734\n",
      "Batch 120: Loss = 1.3302\n",
      "Batch 180: Loss = 1.2221\n",
      "Batch 240: Loss = 1.2098\n",
      "Batch 300: Loss = 1.2908\n",
      "Batch 360: Loss = 1.3013\n",
      "Batch 420: Loss = 1.1538\n",
      "Batch 480: Loss = 1.1451\n",
      "Batch 540: Loss = 1.2998\n",
      "Batch 600: Loss = 1.3347\n",
      "Batch 660: Loss = 1.1775\n",
      "Batch 720: Loss = 1.3471\n",
      "Batch 780: Loss = 1.1330\n",
      "Batch 840: Loss = 1.2034\n",
      "Batch 900: Loss = 1.2733\n",
      "  Train Loss: 1.2864 | Train Acc: 54.30% | Val Loss: 1.3061 | Val Acc: 53.04%\n",
      "Epoch 6/13 LR 0.001 and batch_size 100\n",
      "Batch 60: Loss = 1.2975\n",
      "Batch 120: Loss = 1.1846\n",
      "Batch 180: Loss = 1.3476\n",
      "Batch 240: Loss = 1.4034\n",
      "Batch 300: Loss = 1.2899\n",
      "Batch 360: Loss = 1.1962\n",
      "Batch 420: Loss = 1.2750\n",
      "Batch 480: Loss = 1.1798\n",
      "Batch 540: Loss = 1.1248\n",
      "Batch 600: Loss = 1.2247\n",
      "Batch 660: Loss = 1.4648\n",
      "Batch 720: Loss = 1.1762\n",
      "Batch 780: Loss = 1.3544\n",
      "Batch 840: Loss = 1.3138\n",
      "Batch 900: Loss = 1.2307\n",
      "  Train Loss: 1.2545 | Train Acc: 55.35% | Val Loss: 1.4145 | Val Acc: 49.27%\n",
      "Epoch 7/13 LR 0.001 and batch_size 100\n",
      "Batch 60: Loss = 1.3314\n",
      "Batch 120: Loss = 1.2299\n",
      "Batch 180: Loss = 1.2333\n",
      "Batch 240: Loss = 1.2442\n",
      "Batch 300: Loss = 1.2245\n",
      "Batch 360: Loss = 1.4211\n",
      "Batch 420: Loss = 1.3842\n",
      "Batch 480: Loss = 1.2358\n",
      "Batch 540: Loss = 1.0800\n",
      "Batch 600: Loss = 1.2110\n",
      "Batch 660: Loss = 1.2073\n",
      "Batch 720: Loss = 1.1341\n",
      "Batch 780: Loss = 1.3728\n",
      "Batch 840: Loss = 1.1433\n",
      "Batch 900: Loss = 1.2347\n",
      "  Train Loss: 1.2414 | Train Acc: 55.95% | Val Loss: 1.5894 | Val Acc: 44.31%\n",
      "Epoch 8/13 LR 0.001 and batch_size 100\n",
      "Batch 60: Loss = 1.1213\n",
      "Batch 120: Loss = 1.1986\n",
      "Batch 180: Loss = 1.3449\n",
      "Batch 240: Loss = 1.2395\n",
      "Batch 300: Loss = 1.1086\n",
      "Batch 360: Loss = 1.2180\n",
      "Batch 420: Loss = 1.2831\n",
      "Batch 480: Loss = 1.3249\n",
      "Batch 540: Loss = 1.2673\n",
      "Batch 600: Loss = 1.1211\n",
      "Batch 660: Loss = 1.0351\n",
      "Batch 720: Loss = 1.3385\n",
      "Batch 780: Loss = 1.2749\n",
      "Batch 840: Loss = 1.2851\n",
      "Batch 900: Loss = 1.2088\n",
      "  Train Loss: 1.2286 | Train Acc: 56.22% | Val Loss: 1.5569 | Val Acc: 44.77%\n",
      "Epoch 9/13 LR 0.001 and batch_size 100\n",
      "Batch 60: Loss = 1.1645\n",
      "Batch 120: Loss = 1.2314\n",
      "Batch 180: Loss = 1.2911\n",
      "Batch 240: Loss = 1.2611\n",
      "Batch 300: Loss = 1.1310\n",
      "Batch 360: Loss = 1.2524\n",
      "Batch 420: Loss = 1.3189\n",
      "Batch 480: Loss = 1.3105\n",
      "Batch 540: Loss = 1.1183\n",
      "Batch 600: Loss = 1.0405\n",
      "Batch 660: Loss = 1.0980\n",
      "Batch 720: Loss = 1.2642\n",
      "Batch 780: Loss = 1.2490\n",
      "Batch 840: Loss = 1.1546\n",
      "Batch 900: Loss = 1.2882\n",
      "  Train Loss: 1.2167 | Train Acc: 56.74% | Val Loss: 1.4210 | Val Acc: 49.20%\n",
      "Epoch 10/13 LR 0.001 and batch_size 100\n",
      "Batch 60: Loss = 1.1411\n",
      "Batch 120: Loss = 1.3259\n",
      "Batch 180: Loss = 1.2150\n",
      "Batch 240: Loss = 1.2881\n",
      "Batch 300: Loss = 1.0736\n",
      "Batch 360: Loss = 1.1295\n",
      "Batch 420: Loss = 1.4270\n",
      "Batch 480: Loss = 1.1004\n",
      "Batch 540: Loss = 1.0894\n",
      "Batch 600: Loss = 1.0492\n",
      "Batch 660: Loss = 1.0388\n",
      "Batch 720: Loss = 1.2449\n",
      "Batch 780: Loss = 1.1627\n",
      "Batch 840: Loss = 1.2405\n",
      "Batch 900: Loss = 1.0049\n",
      "  Train Loss: 1.2077 | Train Acc: 57.17% | Val Loss: 1.3030 | Val Acc: 54.26%\n",
      "Epoch 11/13 LR 0.001 and batch_size 100\n",
      "Batch 60: Loss = 1.2624\n",
      "Batch 120: Loss = 1.1030\n",
      "Batch 180: Loss = 1.1315\n",
      "Batch 240: Loss = 1.2103\n",
      "Batch 300: Loss = 1.0364\n",
      "Batch 360: Loss = 1.1455\n",
      "Batch 420: Loss = 1.1261\n",
      "Batch 480: Loss = 1.0578\n",
      "Batch 540: Loss = 1.1444\n",
      "Batch 600: Loss = 1.3291\n",
      "Batch 660: Loss = 1.1142\n",
      "Batch 720: Loss = 1.2940\n",
      "Batch 780: Loss = 1.1410\n",
      "Batch 840: Loss = 1.2965\n",
      "Batch 900: Loss = 1.3193\n",
      "  Train Loss: 1.1970 | Train Acc: 57.50% | Val Loss: 1.3846 | Val Acc: 50.91%\n",
      "Epoch 12/13 LR 0.001 and batch_size 100\n",
      "Batch 60: Loss = 1.3156\n",
      "Batch 120: Loss = 1.2646\n",
      "Batch 180: Loss = 1.1940\n",
      "Batch 240: Loss = 1.2117\n",
      "Batch 300: Loss = 1.2713\n",
      "Batch 360: Loss = 1.1209\n",
      "Batch 420: Loss = 1.2422\n",
      "Batch 480: Loss = 1.0965\n",
      "Batch 540: Loss = 1.1816\n",
      "Batch 600: Loss = 1.1218\n",
      "Batch 660: Loss = 1.0463\n",
      "Batch 720: Loss = 1.2459\n",
      "Batch 780: Loss = 1.2842\n",
      "Batch 840: Loss = 1.1396\n",
      "Batch 900: Loss = 1.1925\n",
      "  Train Loss: 1.1921 | Train Acc: 57.69% | Val Loss: 1.4786 | Val Acc: 47.61%\n",
      "Epoch 13/13 LR 0.001 and batch_size 100\n",
      "Batch 60: Loss = 1.2017\n",
      "Batch 120: Loss = 1.1111\n",
      "Batch 180: Loss = 1.1749\n",
      "Batch 240: Loss = 1.2578\n",
      "Batch 300: Loss = 1.1763\n",
      "Batch 360: Loss = 1.1545\n",
      "Batch 420: Loss = 1.2307\n",
      "Batch 480: Loss = 1.3189\n",
      "Batch 540: Loss = 1.1645\n",
      "Batch 600: Loss = 1.1390\n",
      "Batch 660: Loss = 1.2790\n",
      "Batch 720: Loss = 1.2351\n",
      "Batch 780: Loss = 1.2396\n",
      "Batch 840: Loss = 1.1997\n",
      "Batch 900: Loss = 1.1659\n",
      "  Train Loss: 1.1867 | Train Acc: 58.17% | Val Loss: 1.2291 | Val Acc: 55.90%\n",
      "Final Test Loss: 1.2355, Test Acc: 55.95%\n",
      "Epoch 1/13 LR 0.001 and batch_size 100\n",
      "Batch 60: Loss = 1.7465\n",
      "Batch 120: Loss = 1.7480\n",
      "Batch 180: Loss = 1.7299\n",
      "Batch 240: Loss = 1.8460\n",
      "Batch 300: Loss = 1.5518\n",
      "Batch 360: Loss = 1.7192\n",
      "Batch 420: Loss = 1.7582\n",
      "Batch 480: Loss = 1.6918\n",
      "Batch 540: Loss = 1.7626\n",
      "Batch 600: Loss = 1.8411\n",
      "Batch 660: Loss = 1.8060\n",
      "Batch 720: Loss = 1.7766\n",
      "Batch 780: Loss = 1.9170\n",
      "Batch 840: Loss = 1.8295\n",
      "Batch 900: Loss = 1.8264\n",
      "  Train Loss: 1.8329 | Train Acc: 30.98% | Val Loss: 2.1732 | Val Acc: 19.30%\n",
      "Epoch 2/13 LR 0.001 and batch_size 100\n",
      "Batch 60: Loss = 1.9430\n",
      "Batch 120: Loss = 1.7411\n",
      "Batch 180: Loss = 1.7026\n",
      "Batch 240: Loss = 1.6798\n",
      "Batch 300: Loss = 1.7892\n",
      "Batch 360: Loss = 1.7608\n",
      "Batch 420: Loss = 1.7913\n",
      "Batch 480: Loss = 1.8248\n",
      "Batch 540: Loss = 1.8635\n",
      "Batch 600: Loss = 1.7591\n",
      "Batch 660: Loss = 1.8347\n",
      "Batch 720: Loss = 1.8913\n",
      "Batch 780: Loss = 1.8008\n",
      "Batch 840: Loss = 1.8748\n",
      "Batch 900: Loss = 1.7763\n",
      "  Train Loss: 1.8187 | Train Acc: 31.28% | Val Loss: 1.9698 | Val Acc: 25.99%\n",
      "Epoch 3/13 LR 0.001 and batch_size 100\n",
      "Batch 60: Loss = 1.8414\n",
      "Batch 120: Loss = 1.9398\n",
      "Batch 180: Loss = 1.8099\n",
      "Batch 240: Loss = 1.8894\n",
      "Batch 300: Loss = 1.9686\n",
      "Batch 360: Loss = 1.8575\n",
      "Batch 420: Loss = 1.8673\n",
      "Batch 480: Loss = 1.8160\n",
      "Batch 540: Loss = 1.8606\n",
      "Batch 600: Loss = 1.7708\n",
      "Batch 660: Loss = 1.8067\n",
      "Batch 720: Loss = 1.7629\n",
      "Batch 780: Loss = 1.8782\n",
      "Batch 840: Loss = 1.8313\n",
      "Batch 900: Loss = 1.8577\n",
      "  Train Loss: 1.8501 | Train Acc: 31.66% | Val Loss: 1.9607 | Val Acc: 24.71%\n",
      "Epoch 4/13 LR 0.001 and batch_size 100\n",
      "Batch 60: Loss = 1.8149\n",
      "Batch 120: Loss = 1.7728\n",
      "Batch 180: Loss = 1.8234\n",
      "Batch 240: Loss = 1.7527\n",
      "Batch 300: Loss = 1.8305\n",
      "Batch 360: Loss = 1.7812\n",
      "Batch 420: Loss = 1.8442\n",
      "Batch 480: Loss = 1.7504\n",
      "Batch 540: Loss = 1.7849\n",
      "Batch 600: Loss = 1.8119\n",
      "Batch 660: Loss = 1.8930\n",
      "Batch 720: Loss = 1.7708\n",
      "Batch 780: Loss = 1.7653\n",
      "Batch 840: Loss = 1.7725\n",
      "Batch 900: Loss = 1.8406\n",
      "  Train Loss: 1.8177 | Train Acc: 32.64% | Val Loss: 1.8987 | Val Acc: 28.49%\n",
      "Epoch 5/13 LR 0.001 and batch_size 100\n",
      "Batch 60: Loss = 1.7718\n",
      "Batch 120: Loss = 1.8016\n",
      "Batch 180: Loss = 1.8449\n",
      "Batch 240: Loss = 1.7715\n",
      "Batch 300: Loss = 1.8251\n",
      "Batch 360: Loss = 1.8528\n",
      "Batch 420: Loss = 1.8395\n",
      "Batch 480: Loss = 1.7472\n",
      "Batch 540: Loss = 1.8079\n",
      "Batch 600: Loss = 1.8646\n",
      "Batch 660: Loss = 1.8124\n",
      "Batch 720: Loss = 1.8457\n",
      "Batch 780: Loss = 1.8707\n",
      "Batch 840: Loss = 1.8255\n",
      "Batch 900: Loss = 1.7607\n",
      "  Train Loss: 1.8008 | Train Acc: 33.04% | Val Loss: 2.0507 | Val Acc: 21.55%\n",
      "Epoch 6/13 LR 0.001 and batch_size 100\n",
      "Batch 60: Loss = 1.7800\n",
      "Batch 120: Loss = 1.8722\n",
      "Batch 180: Loss = 1.7894\n",
      "Batch 240: Loss = 1.8117\n",
      "Batch 300: Loss = 1.8702\n",
      "Batch 360: Loss = 1.7835\n",
      "Batch 420: Loss = 1.7590\n",
      "Batch 480: Loss = 1.8452\n",
      "Batch 540: Loss = 1.8050\n",
      "Batch 600: Loss = 1.7885\n",
      "Batch 660: Loss = 1.8920\n",
      "Batch 720: Loss = 1.7512\n",
      "Batch 780: Loss = 1.7795\n",
      "Batch 840: Loss = 1.8160\n",
      "Batch 900: Loss = 1.7200\n",
      "  Train Loss: 1.7891 | Train Acc: 33.16% | Val Loss: 1.8999 | Val Acc: 28.70%\n",
      "Epoch 7/13 LR 0.001 and batch_size 100\n",
      "Batch 60: Loss = 1.7590\n",
      "Batch 120: Loss = 1.8310\n",
      "Batch 180: Loss = 1.7739\n",
      "Batch 240: Loss = 1.7852\n",
      "Batch 300: Loss = 1.7858\n",
      "Batch 360: Loss = 1.8023\n",
      "Batch 420: Loss = 1.8009\n",
      "Batch 480: Loss = 1.8680\n",
      "Batch 540: Loss = 1.8213\n",
      "Batch 600: Loss = 1.7123\n",
      "Batch 660: Loss = 1.7986\n",
      "Batch 720: Loss = 1.6866\n",
      "Batch 780: Loss = 1.7915\n",
      "Batch 840: Loss = 1.6745\n",
      "Batch 900: Loss = 1.7212\n",
      "  Train Loss: 1.7812 | Train Acc: 33.28% | Val Loss: 2.1580 | Val Acc: 19.38%\n",
      "Epoch 8/13 LR 0.001 and batch_size 100\n",
      "Batch 60: Loss = 1.6993\n",
      "Batch 120: Loss = 1.7604\n",
      "Batch 180: Loss = 1.8122\n",
      "Batch 240: Loss = 1.7173\n",
      "Batch 300: Loss = 1.8423\n",
      "Batch 360: Loss = 1.7883\n",
      "Batch 420: Loss = 1.8791\n",
      "Batch 480: Loss = 1.7451\n",
      "Batch 540: Loss = 1.7851\n",
      "Batch 600: Loss = 1.8409\n",
      "Batch 660: Loss = 1.7924\n",
      "Batch 720: Loss = 1.7634\n",
      "Batch 780: Loss = 1.7097\n",
      "Batch 840: Loss = 1.7080\n",
      "Batch 900: Loss = 1.8161\n",
      "  Train Loss: 1.7722 | Train Acc: 33.45% | Val Loss: 2.0651 | Val Acc: 24.16%\n",
      "Epoch 9/13 LR 0.001 and batch_size 100\n",
      "Batch 60: Loss = 1.8167\n",
      "Batch 120: Loss = 1.7690\n",
      "Batch 180: Loss = 1.7021\n",
      "Batch 240: Loss = 1.6540\n",
      "Batch 300: Loss = 1.7481\n",
      "Batch 360: Loss = 1.5988\n",
      "Batch 420: Loss = 1.8614\n",
      "Batch 480: Loss = 1.6956\n",
      "Batch 540: Loss = 1.7749\n",
      "Batch 600: Loss = 1.8701\n",
      "Batch 660: Loss = 1.7351\n",
      "Batch 720: Loss = 1.8119\n",
      "Batch 780: Loss = 1.7429\n",
      "Batch 840: Loss = 1.7250\n",
      "Batch 900: Loss = 1.7618\n",
      "  Train Loss: 1.7722 | Train Acc: 33.39% | Val Loss: 2.0646 | Val Acc: 22.92%\n",
      "Epoch 10/13 LR 0.001 and batch_size 100\n",
      "Batch 60: Loss = 1.7654\n",
      "Batch 120: Loss = 1.7222\n",
      "Batch 180: Loss = 1.7444\n",
      "Batch 240: Loss = 1.7762\n",
      "Batch 300: Loss = 1.7837\n",
      "Batch 360: Loss = 1.6241\n",
      "Batch 420: Loss = 1.7956\n",
      "Batch 480: Loss = 1.6707\n",
      "Batch 540: Loss = 1.8324\n",
      "Batch 600: Loss = 1.8931\n",
      "Batch 660: Loss = 1.8186\n",
      "Batch 720: Loss = 1.7790\n",
      "Batch 780: Loss = 1.7081\n",
      "Batch 840: Loss = 1.7813\n",
      "Batch 900: Loss = 1.8028\n",
      "  Train Loss: 1.7674 | Train Acc: 33.71% | Val Loss: 1.9220 | Val Acc: 26.86%\n",
      "Epoch 11/13 LR 0.001 and batch_size 100\n",
      "Batch 60: Loss = 1.7495\n",
      "Batch 120: Loss = 1.7438\n",
      "Batch 180: Loss = 1.6734\n",
      "Batch 240: Loss = 1.8684\n",
      "Batch 300: Loss = 1.8332\n",
      "Batch 360: Loss = 1.7880\n",
      "Batch 420: Loss = 1.8890\n",
      "Batch 480: Loss = 1.8221\n",
      "Batch 540: Loss = 1.8835\n",
      "Batch 600: Loss = 1.8506\n",
      "Batch 660: Loss = 1.7617\n",
      "Batch 720: Loss = 1.7499\n",
      "Batch 780: Loss = 1.8506\n",
      "Batch 840: Loss = 1.7962\n",
      "Batch 900: Loss = 1.7270\n",
      "  Train Loss: 1.7643 | Train Acc: 33.75% | Val Loss: 1.8923 | Val Acc: 28.67%\n",
      "Epoch 12/13 LR 0.001 and batch_size 100\n",
      "Batch 60: Loss = 1.8174\n",
      "Batch 120: Loss = 1.8977\n",
      "Batch 180: Loss = 1.7175\n",
      "Batch 240: Loss = 1.7017\n",
      "Batch 300: Loss = 1.7861\n",
      "Batch 360: Loss = 1.7412\n",
      "Batch 420: Loss = 1.7804\n",
      "Batch 480: Loss = 1.7642\n",
      "Batch 540: Loss = 1.7087\n",
      "Batch 600: Loss = 1.8993\n",
      "Batch 660: Loss = 1.7608\n",
      "Batch 720: Loss = 1.8335\n",
      "Batch 780: Loss = 1.8050\n",
      "Batch 840: Loss = 1.6935\n",
      "Batch 900: Loss = 1.8003\n",
      "  Train Loss: 1.7622 | Train Acc: 33.78% | Val Loss: 1.9448 | Val Acc: 27.04%\n",
      "Epoch 13/13 LR 0.001 and batch_size 100\n",
      "Batch 60: Loss = 1.7130\n",
      "Batch 120: Loss = 1.6984\n",
      "Batch 180: Loss = 1.7665\n",
      "Batch 240: Loss = 1.7842\n",
      "Batch 300: Loss = 1.7339\n",
      "Batch 360: Loss = 1.7775\n",
      "Batch 420: Loss = 1.7507\n",
      "Batch 480: Loss = 1.7204\n",
      "Batch 540: Loss = 1.7107\n",
      "Batch 600: Loss = 1.7681\n",
      "Batch 660: Loss = 1.8147\n",
      "Batch 720: Loss = 1.7369\n",
      "Batch 780: Loss = 1.7456\n",
      "Batch 840: Loss = 1.6772\n",
      "Batch 900: Loss = 1.8717\n",
      "  Train Loss: 1.7619 | Train Acc: 33.70% | Val Loss: 1.8526 | Val Acc: 29.36%\n",
      "Final Test Loss: 1.8536, Test Acc: 29.09%\n"
     ]
    }
   ],
   "source": [
    "# Implementing weight decay\n",
    "weight_decays = [0.0001, 0.001, 0.01, 0.1]\n",
    "lr = 0.001\n",
    "bs = 100\n",
    "EPOCHS = 13\n",
    "for wd in weight_decays:\n",
    "    train_loader = DataLoader(train_dataset, batch_size=bs, shuffle=True, num_workers=2)\n",
    "    val_loader   = DataLoader(val_dataset, batch_size=bs, shuffle=False, num_workers=2)\n",
    "    test_loader  = DataLoader(test_dataset, batch_size=bs, shuffle=False, num_workers=2)\n",
    "        \n",
    "    # Initialize model and optimizer for current hyper-parameter combination\n",
    "    model = ResNet18(num_classes=NUM_CLASSES).to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr,  weight_decay=wd)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "        \n",
    "    best_val_acc = 0.0\n",
    "    checkpoint = f\"resnet18_lr{lr}_bs{bs}_weightDc{wd}.pth\"\n",
    "        \n",
    "    for epoch in range(EPOCHS):\n",
    "        print(f\"Epoch {epoch+1}/{EPOCHS} LR {lr} and batch_size {bs} wd {wd}\")\n",
    "        train_loss, train_acc = train_one_epoch(model, train_loader, optimizer, criterion)\n",
    "        val_loss, val_acc = evaluate(model, val_loader, criterion)\n",
    "        print(f\"  Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.2f}% | \"\n",
    "                  f\"Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.2f}%\")\n",
    "        if val_acc > best_val_acc:\n",
    "                best_val_acc = val_acc\n",
    "                torch.save(model.state_dict(), checkpoint)\n",
    "        \n",
    "    model.load_state_dict(torch.load(checkpoint))\n",
    "    test_loss, test_acc = evaluate(model, test_loader, criterion)\n",
    "    print(f\"Final Test Loss: {test_loss:.4f}, Test Acc: {test_acc:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "988d0238-df4f-4144-8435-739f15e1a5d9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-28T22:17:50.832409Z",
     "start_time": "2025-03-28T22:17:50.781072Z"
    }
   },
   "outputs": [],
   "source": [
    "class ResNet18_Dropout(nn.Module):\n",
    "    def __init__(self, num_classes=10, dropout_prob=0.5):\n",
    "        super(ResNet18_Dropout, self).__init__()\n",
    "        # Conv1: 7x7, stride=2, padding=3\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        \n",
    "        # MaxPool: 3x3, stride=2, padding=1\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        \n",
    "        # Residual layers\n",
    "        self.layer1 = self._make_layer(64, 64, 2, stride=1)\n",
    "        self.layer2 = self._make_layer(64, 128, 2, stride=2)\n",
    "        self.layer3 = self._make_layer(128, 256, 2, stride=2)\n",
    "        self.layer4 = self._make_layer(256, 512, 2, stride=2)\n",
    "        \n",
    "        # Global Average Pooling\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        # Dropout layer added before the FC layer\n",
    "        self.dropout = nn.Dropout(p=dropout_prob)\n",
    "        # Fully Connected Layer\n",
    "        self.fc = nn.Linear(512, num_classes)\n",
    "    \n",
    "    def _make_layer(self, in_channels, out_channels, blocks, stride):\n",
    "        layers = []\n",
    "        layers.append(ResidualBlock(in_channels, out_channels, stride))\n",
    "        for _ in range(1, blocks):\n",
    "            layers.append(ResidualBlock(out_channels, out_channels))\n",
    "        return nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.bn1(self.conv1(x)))\n",
    "        x = self.maxpool(x)\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.dropout(x)  # Apply dropout here\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "808e6404504a9aeb",
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/18\n",
      "Batch 60: Loss = 1.9162\n",
      "Batch 120: Loss = 2.0143\n",
      "Batch 180: Loss = 1.8944\n",
      "Batch 240: Loss = 1.6966\n",
      "Batch 300: Loss = 1.6603\n",
      "Batch 360: Loss = 1.5144\n",
      "Batch 420: Loss = 1.4640\n",
      "Batch 480: Loss = 1.3634\n",
      "Batch 540: Loss = 1.5885\n",
      "Batch 600: Loss = 1.4111\n",
      "Batch 660: Loss = 1.4478\n",
      "Batch 720: Loss = 1.4875\n",
      "Batch 780: Loss = 1.4945\n",
      "Batch 840: Loss = 1.4675\n",
      "Batch 900: Loss = 1.4661\n",
      "Epoch [1/18] Train Loss: 1.6176 | Train Acc: 41.15% | Val Loss: 1.4402 | Val Acc: 46.64%\n",
      "Epoch 2/18\n",
      "Batch 60: Loss = 1.2241\n",
      "Batch 120: Loss = 1.3590\n",
      "Batch 180: Loss = 1.3764\n",
      "Batch 240: Loss = 1.1423\n",
      "Batch 300: Loss = 1.3065\n",
      "Batch 360: Loss = 1.1892\n",
      "Batch 420: Loss = 1.3948\n",
      "Batch 480: Loss = 1.2771\n",
      "Batch 540: Loss = 1.4246\n",
      "Batch 600: Loss = 1.4870\n",
      "Batch 660: Loss = 1.2383\n",
      "Batch 720: Loss = 1.3962\n",
      "Batch 780: Loss = 1.3794\n",
      "Batch 840: Loss = 1.1717\n",
      "Batch 900: Loss = 1.4169\n",
      "Epoch [2/18] Train Loss: 1.2982 | Train Acc: 53.12% | Val Loss: 1.2917 | Val Acc: 54.04%\n",
      "Epoch 3/18\n",
      "Batch 60: Loss = 0.9653\n",
      "Batch 120: Loss = 0.9101\n",
      "Batch 180: Loss = 1.2097\n",
      "Batch 240: Loss = 1.4637\n",
      "Batch 300: Loss = 1.1986\n",
      "Batch 360: Loss = 1.1376\n",
      "Batch 420: Loss = 1.1336\n",
      "Batch 480: Loss = 1.2198\n",
      "Batch 540: Loss = 0.9928\n",
      "Batch 600: Loss = 1.0736\n",
      "Batch 660: Loss = 1.2403\n",
      "Batch 720: Loss = 1.1806\n",
      "Batch 780: Loss = 1.2157\n",
      "Batch 840: Loss = 1.2227\n",
      "Batch 900: Loss = 1.2380\n",
      "Epoch [3/18] Train Loss: 1.1626 | Train Acc: 58.55% | Val Loss: 1.3069 | Val Acc: 53.44%\n",
      "Epoch 4/18\n",
      "Batch 60: Loss = 0.9328\n",
      "Batch 120: Loss = 1.0104\n",
      "Batch 180: Loss = 1.0264\n",
      "Batch 240: Loss = 1.0745\n",
      "Batch 300: Loss = 0.9645\n",
      "Batch 360: Loss = 1.0506\n",
      "Batch 420: Loss = 0.9842\n",
      "Batch 480: Loss = 0.9509\n",
      "Batch 540: Loss = 1.2312\n",
      "Batch 600: Loss = 1.1927\n",
      "Batch 660: Loss = 1.1135\n",
      "Batch 720: Loss = 1.1453\n",
      "Batch 780: Loss = 1.0528\n",
      "Batch 840: Loss = 1.2295\n",
      "Batch 900: Loss = 0.9772\n",
      "Epoch [4/18] Train Loss: 1.0639 | Train Acc: 62.21% | Val Loss: 1.2413 | Val Acc: 56.24%\n",
      "Epoch 5/18\n",
      "Batch 60: Loss = 0.9173\n",
      "Batch 120: Loss = 1.0714\n",
      "Batch 180: Loss = 1.0707\n",
      "Batch 240: Loss = 1.1047\n",
      "Batch 300: Loss = 0.8214\n",
      "Batch 360: Loss = 0.9381\n",
      "Batch 420: Loss = 0.9061\n",
      "Batch 480: Loss = 0.9981\n",
      "Batch 540: Loss = 0.9996\n",
      "Batch 600: Loss = 1.0398\n",
      "Batch 660: Loss = 1.0696\n",
      "Batch 720: Loss = 0.9236\n",
      "Batch 780: Loss = 0.8839\n",
      "Batch 840: Loss = 1.0054\n",
      "Batch 900: Loss = 0.9226\n",
      "Epoch [5/18] Train Loss: 0.9815 | Train Acc: 65.28% | Val Loss: 1.0807 | Val Acc: 61.66%\n",
      "Epoch 6/18\n",
      "Batch 60: Loss = 0.8564\n",
      "Batch 120: Loss = 0.8968\n",
      "Batch 180: Loss = 1.0355\n",
      "Batch 240: Loss = 0.9088\n",
      "Batch 300: Loss = 1.1512\n",
      "Batch 360: Loss = 1.0059\n",
      "Batch 420: Loss = 0.8680\n",
      "Batch 480: Loss = 0.9177\n",
      "Batch 540: Loss = 0.7494\n",
      "Batch 600: Loss = 0.7855\n",
      "Batch 660: Loss = 0.9181\n",
      "Batch 720: Loss = 0.8634\n",
      "Batch 780: Loss = 0.8757\n",
      "Batch 840: Loss = 0.8582\n",
      "Batch 900: Loss = 1.0934\n",
      "Epoch [6/18] Train Loss: 0.8998 | Train Acc: 68.06% | Val Loss: 1.1512 | Val Acc: 60.57%\n",
      "Epoch 7/18\n",
      "Batch 60: Loss = 0.6461\n",
      "Batch 120: Loss = 0.7496\n",
      "Batch 180: Loss = 0.6246\n",
      "Batch 240: Loss = 0.8217\n",
      "Batch 300: Loss = 0.7389\n",
      "Batch 360: Loss = 0.8070\n",
      "Batch 420: Loss = 0.7711\n",
      "Batch 480: Loss = 0.8481\n",
      "Batch 540: Loss = 0.7703\n",
      "Batch 600: Loss = 0.9547\n",
      "Batch 660: Loss = 0.9912\n",
      "Batch 720: Loss = 0.9827\n",
      "Batch 780: Loss = 0.7522\n",
      "Batch 840: Loss = 1.0697\n",
      "Batch 900: Loss = 0.8628\n",
      "Epoch [7/18] Train Loss: 0.8209 | Train Acc: 70.95% | Val Loss: 1.0873 | Val Acc: 62.01%\n",
      "Epoch 8/18\n",
      "Batch 60: Loss = 0.6500\n",
      "Batch 120: Loss = 0.9090\n",
      "Batch 180: Loss = 0.5974\n",
      "Batch 240: Loss = 0.7071\n",
      "Batch 300: Loss = 0.6613\n",
      "Batch 360: Loss = 0.7189\n",
      "Batch 420: Loss = 0.7156\n",
      "Batch 480: Loss = 0.6889\n",
      "Batch 540: Loss = 0.8421\n",
      "Batch 600: Loss = 0.7749\n",
      "Batch 660: Loss = 0.8339\n",
      "Batch 720: Loss = 0.6981\n",
      "Batch 780: Loss = 0.8749\n",
      "Batch 840: Loss = 0.7495\n",
      "Batch 900: Loss = 0.7074\n",
      "Epoch [8/18] Train Loss: 0.7364 | Train Acc: 73.80% | Val Loss: 1.1319 | Val Acc: 61.67%\n",
      "Epoch 9/18\n",
      "Batch 60: Loss = 0.5864\n",
      "Batch 120: Loss = 0.5979\n",
      "Batch 180: Loss = 0.6470\n",
      "Batch 240: Loss = 0.8255\n",
      "Batch 300: Loss = 0.5462\n",
      "Batch 360: Loss = 0.6240\n",
      "Batch 420: Loss = 0.5216\n",
      "Batch 480: Loss = 0.4761\n",
      "Batch 540: Loss = 0.5933\n",
      "Batch 600: Loss = 0.8725\n",
      "Batch 660: Loss = 0.3725\n",
      "Batch 720: Loss = 0.5241\n",
      "Batch 780: Loss = 0.5961\n",
      "Batch 840: Loss = 0.7059\n",
      "Batch 900: Loss = 0.7676\n",
      "Epoch [9/18] Train Loss: 0.6506 | Train Acc: 76.91% | Val Loss: 1.1374 | Val Acc: 61.16%\n",
      "Epoch 10/18\n",
      "Batch 60: Loss = 0.6596\n",
      "Batch 120: Loss = 0.4635\n",
      "Batch 180: Loss = 0.5011\n",
      "Batch 240: Loss = 0.5570\n",
      "Batch 300: Loss = 0.5188\n",
      "Batch 360: Loss = 0.4732\n",
      "Batch 420: Loss = 0.7118\n",
      "Batch 480: Loss = 0.7053\n",
      "Batch 540: Loss = 0.6403\n",
      "Batch 600: Loss = 0.6136\n",
      "Batch 660: Loss = 0.6018\n",
      "Batch 720: Loss = 0.5403\n",
      "Batch 780: Loss = 0.5250\n",
      "Batch 840: Loss = 0.5628\n",
      "Batch 900: Loss = 0.5867\n",
      "Epoch [10/18] Train Loss: 0.5643 | Train Acc: 80.00% | Val Loss: 1.1556 | Val Acc: 63.64%\n",
      "Epoch 11/18\n",
      "Batch 60: Loss = 0.3273\n",
      "Batch 120: Loss = 0.4780\n",
      "Batch 180: Loss = 0.5594\n",
      "Batch 240: Loss = 0.5540\n",
      "Batch 300: Loss = 0.4501\n",
      "Batch 360: Loss = 0.5111\n",
      "Batch 420: Loss = 0.4867\n",
      "Batch 480: Loss = 0.6105\n",
      "Batch 540: Loss = 0.4429\n",
      "Batch 600: Loss = 0.4895\n",
      "Batch 660: Loss = 0.3779\n",
      "Batch 720: Loss = 0.4786\n",
      "Batch 780: Loss = 0.4492\n",
      "Batch 840: Loss = 0.4753\n",
      "Batch 900: Loss = 0.7534\n",
      "Epoch [11/18] Train Loss: 0.4730 | Train Acc: 83.20% | Val Loss: 1.2843 | Val Acc: 61.75%\n",
      "Epoch 12/18\n",
      "Batch 60: Loss = 0.4868\n",
      "Batch 120: Loss = 0.3160\n",
      "Batch 180: Loss = 0.4264\n",
      "Batch 240: Loss = 0.2530\n",
      "Batch 300: Loss = 0.4401\n",
      "Batch 360: Loss = 0.5720\n",
      "Batch 420: Loss = 0.3569\n",
      "Batch 480: Loss = 0.3499\n",
      "Batch 540: Loss = 0.4093\n",
      "Batch 600: Loss = 0.4028\n",
      "Batch 660: Loss = 0.4186\n",
      "Batch 720: Loss = 0.4149\n",
      "Batch 780: Loss = 0.4742\n",
      "Batch 840: Loss = 0.3007\n",
      "Batch 900: Loss = 0.3960\n",
      "Epoch [12/18] Train Loss: 0.3971 | Train Acc: 85.89% | Val Loss: 1.4039 | Val Acc: 61.90%\n",
      "Epoch 13/18\n",
      "Batch 60: Loss = 0.2597\n",
      "Batch 120: Loss = 0.1835\n",
      "Batch 180: Loss = 0.2530\n",
      "Batch 240: Loss = 0.1687\n",
      "Batch 300: Loss = 0.2442\n",
      "Batch 360: Loss = 0.2617\n",
      "Batch 420: Loss = 0.3151\n",
      "Batch 480: Loss = 0.3562\n",
      "Batch 540: Loss = 0.3186\n",
      "Batch 600: Loss = 0.3985\n",
      "Batch 660: Loss = 0.2722\n",
      "Batch 720: Loss = 0.5400\n",
      "Batch 780: Loss = 0.3288\n",
      "Batch 840: Loss = 0.2986\n",
      "Batch 900: Loss = 0.4430\n",
      "Epoch [13/18] Train Loss: 0.3325 | Train Acc: 88.14% | Val Loss: 1.4438 | Val Acc: 62.55%\n",
      "Epoch 14/18\n",
      "Batch 60: Loss = 0.4121\n",
      "Batch 120: Loss = 0.1731\n",
      "Batch 180: Loss = 0.1872\n",
      "Batch 240: Loss = 0.2086\n",
      "Batch 300: Loss = 0.2054\n",
      "Batch 360: Loss = 0.2198\n",
      "Batch 420: Loss = 0.2411\n",
      "Batch 480: Loss = 0.2596\n",
      "Batch 540: Loss = 0.3082\n",
      "Batch 600: Loss = 0.2754\n",
      "Batch 660: Loss = 0.2370\n",
      "Batch 720: Loss = 0.3591\n",
      "Batch 780: Loss = 0.4453\n",
      "Batch 840: Loss = 0.3897\n",
      "Batch 900: Loss = 0.2962\n",
      "Epoch [14/18] Train Loss: 0.2764 | Train Acc: 90.25% | Val Loss: 1.4995 | Val Acc: 62.50%\n",
      "Epoch 15/18\n",
      "Batch 60: Loss = 0.1430\n",
      "Batch 120: Loss = 0.2536\n",
      "Batch 180: Loss = 0.2056\n",
      "Batch 240: Loss = 0.1926\n",
      "Batch 300: Loss = 0.1998\n",
      "Batch 360: Loss = 0.2066\n",
      "Batch 420: Loss = 0.2853\n",
      "Batch 480: Loss = 0.1799\n",
      "Batch 540: Loss = 0.1563\n",
      "Batch 600: Loss = 0.1623\n",
      "Batch 660: Loss = 0.3081\n",
      "Batch 720: Loss = 0.2131\n",
      "Batch 780: Loss = 0.1889\n",
      "Batch 840: Loss = 0.4942\n",
      "Batch 900: Loss = 0.2235\n",
      "Epoch [15/18] Train Loss: 0.2291 | Train Acc: 91.80% | Val Loss: 1.6471 | Val Acc: 62.25%\n",
      "Epoch 16/18\n",
      "Batch 60: Loss = 0.1093\n",
      "Batch 120: Loss = 0.0969\n",
      "Batch 180: Loss = 0.2621\n",
      "Batch 240: Loss = 0.2003\n",
      "Batch 300: Loss = 0.1923\n",
      "Batch 360: Loss = 0.2465\n",
      "Batch 420: Loss = 0.0858\n",
      "Batch 480: Loss = 0.2091\n",
      "Batch 540: Loss = 0.2135\n",
      "Batch 600: Loss = 0.2280\n",
      "Batch 660: Loss = 0.3287\n",
      "Batch 720: Loss = 0.3005\n",
      "Batch 780: Loss = 0.2801\n",
      "Batch 840: Loss = 0.2871\n",
      "Batch 900: Loss = 0.1859\n",
      "Epoch [16/18] Train Loss: 0.2009 | Train Acc: 92.90% | Val Loss: 1.6604 | Val Acc: 62.71%\n",
      "Epoch 17/18\n",
      "Batch 60: Loss = 0.1102\n",
      "Batch 120: Loss = 0.1276\n",
      "Batch 180: Loss = 0.4468\n",
      "Batch 240: Loss = 0.2139\n",
      "Batch 300: Loss = 0.2435\n",
      "Batch 360: Loss = 0.1792\n",
      "Batch 420: Loss = 0.1651\n",
      "Batch 480: Loss = 0.2326\n",
      "Batch 540: Loss = 0.3098\n",
      "Batch 600: Loss = 0.2494\n",
      "Batch 660: Loss = 0.2033\n",
      "Batch 720: Loss = 0.1508\n",
      "Batch 780: Loss = 0.1921\n",
      "Batch 840: Loss = 0.1475\n",
      "Batch 900: Loss = 0.2395\n",
      "Epoch [17/18] Train Loss: 0.1837 | Train Acc: 93.62% | Val Loss: 1.8678 | Val Acc: 61.92%\n",
      "Epoch 18/18\n",
      "Batch 60: Loss = 0.0603\n",
      "Batch 120: Loss = 0.0801\n",
      "Batch 180: Loss = 0.1233\n",
      "Batch 240: Loss = 0.1513\n",
      "Batch 300: Loss = 0.1017\n",
      "Batch 360: Loss = 0.2493\n",
      "Batch 420: Loss = 0.1840\n",
      "Batch 480: Loss = 0.2508\n",
      "Batch 540: Loss = 0.1982\n",
      "Batch 600: Loss = 0.1696\n",
      "Batch 660: Loss = 0.1866\n",
      "Batch 720: Loss = 0.1411\n",
      "Batch 780: Loss = 0.1157\n",
      "Batch 840: Loss = 0.2561\n",
      "Batch 900: Loss = 0.1989\n",
      "Epoch [18/18] Train Loss: 0.1637 | Train Acc: 94.38% | Val Loss: 1.8349 | Val Acc: 62.05%\n",
      "\n",
      "Final Test Loss: 1.1681, Test Acc: 63.43%\n"
     ]
    }
   ],
   "source": [
    "lr = 0.001\n",
    "bs = 100\n",
    "EPOCHS = 18\n",
    "model = ResNet18_Dropout(num_classes=NUM_CLASSES).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "train_loader = DataLoader(train_dataset, batch_size=bs, shuffle=True, num_workers=2)\n",
    "val_loader   = DataLoader(val_dataset, batch_size=bs, shuffle=False, num_workers=2)\n",
    "test_loader  = DataLoader(test_dataset, batch_size=bs, shuffle=False, num_workers=2)  \n",
    "checkpoint = f\"resnet18_lr{lr}_bs{bs}_drop_out_0.5.pth\"\n",
    "best_val_acc = 0.0\n",
    "for epoch in range(EPOCHS):\n",
    "    print(f\"Epoch {epoch+1}/{EPOCHS}\")\n",
    "    train_loss, train_acc = train_one_epoch(model, train_loader, optimizer, criterion)\n",
    "    val_loss, val_acc = evaluate(model, val_loader, criterion)\n",
    "    \n",
    "    print(f\"Epoch [{epoch+1}/{EPOCHS}] \"\n",
    "          f\"Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.2f}% | \"\n",
    "          f\"Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.2f}%\")\n",
    "    \n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        torch.save(model.state_dict(), checkpoint)\n",
    "        \n",
    "model.load_state_dict(torch.load(checkpoint))\n",
    "test_loss, test_acc = evaluate(model, test_loader, criterion)\n",
    "print(f\"\\nFinal Test Loss: {test_loss:.4f}, Test Acc: {test_acc:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "0d9ce9b4-39be-458d-a6f0-12edaf54faf0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18000"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_train = len(train_dataset)\n",
    "few_shot_ratio = 0.2\n",
    "few_shot_size = int(num_train * few_shot_ratio)\n",
    "few_shot_indices = np.random.choice(num_train, few_shot_size, replace=False)\n",
    "few_shot_train_dataset = Subset(train_dataset, few_shot_indices)\n",
    "len(few_shot_train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "3ce90885-50dd-40c6-9576-e28c9d717950",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Few-Show Learning on 18000 samples\n",
      "Epoch 1/10\n",
      "Batch 60: Loss = 1.8033\n",
      "Batch 120: Loss = 1.7145\n",
      "Batch 180: Loss = 1.9002\n",
      "Epoch [1/10] Train Loss: 1.8200 | Train Acc: 33.17% | Val Loss: 1.8907 | Val Acc: 31.27%\n",
      "Epoch 2/10\n",
      "Batch 60: Loss = 1.6396\n",
      "Batch 120: Loss = 1.6243\n",
      "Batch 180: Loss = 1.6345\n",
      "Epoch [2/10] Train Loss: 1.5981 | Train Acc: 41.15% | Val Loss: 1.6817 | Val Acc: 39.10%\n",
      "Epoch 3/10\n",
      "Batch 60: Loss = 1.4286\n",
      "Batch 120: Loss = 1.5280\n",
      "Batch 180: Loss = 1.4164\n",
      "Epoch [3/10] Train Loss: 1.4772 | Train Acc: 45.71% | Val Loss: 1.6992 | Val Acc: 40.28%\n",
      "Epoch 4/10\n",
      "Batch 60: Loss = 1.4132\n",
      "Batch 120: Loss = 1.4344\n",
      "Batch 180: Loss = 1.2734\n",
      "Epoch [4/10] Train Loss: 1.3653 | Train Acc: 50.02% | Val Loss: 1.8454 | Val Acc: 36.40%\n",
      "Epoch 5/10\n",
      "Batch 60: Loss = 1.2202\n",
      "Batch 120: Loss = 0.9384\n",
      "Batch 180: Loss = 1.2721\n",
      "Epoch [5/10] Train Loss: 1.2746 | Train Acc: 54.22% | Val Loss: 1.4987 | Val Acc: 46.19%\n",
      "Epoch 6/10\n",
      "Batch 60: Loss = 1.0798\n",
      "Batch 120: Loss = 1.2046\n",
      "Batch 180: Loss = 0.8470\n",
      "Epoch [6/10] Train Loss: 1.1791 | Train Acc: 57.36% | Val Loss: 1.6838 | Val Acc: 42.89%\n",
      "Epoch 7/10\n",
      "Batch 60: Loss = 1.1200\n",
      "Batch 120: Loss = 1.2226\n",
      "Batch 180: Loss = 1.1097\n",
      "Epoch [7/10] Train Loss: 1.0812 | Train Acc: 61.06% | Val Loss: 1.4548 | Val Acc: 48.86%\n",
      "Epoch 8/10\n",
      "Batch 60: Loss = 0.9006\n",
      "Batch 120: Loss = 1.1099\n",
      "Batch 180: Loss = 0.9442\n",
      "Epoch [8/10] Train Loss: 0.9832 | Train Acc: 64.61% | Val Loss: 1.7721 | Val Acc: 45.31%\n",
      "Epoch 9/10\n",
      "Batch 60: Loss = 0.8547\n",
      "Batch 120: Loss = 0.7578\n",
      "Batch 180: Loss = 0.9334\n",
      "Epoch [9/10] Train Loss: 0.8749 | Train Acc: 68.68% | Val Loss: 1.8618 | Val Acc: 45.22%\n",
      "Epoch 10/10\n",
      "Batch 60: Loss = 0.8424\n",
      "Batch 120: Loss = 0.7339\n",
      "Batch 180: Loss = 0.7932\n",
      "Epoch [10/10] Train Loss: 0.7704 | Train Acc: 72.66% | Val Loss: 1.6616 | Val Acc: 47.14%\n",
      "\n",
      "Final Test Loss: 1.4639, Test Acc: 48.36%\n"
     ]
    }
   ],
   "source": [
    "# Few-shot DataLoader: using the reduced dataset\n",
    "lr = 0.001\n",
    "bs = 100\n",
    "EPOCHS = 10\n",
    "few_shot_train_loader = DataLoader(few_shot_train_dataset, batch_size=bs, shuffle=True, num_workers=2)\n",
    "val_loader  = DataLoader(val_dataset, batch_size=bs, shuffle=False, num_workers=2)\n",
    "test_loader = DataLoader(test_dataset, batch_size=bs, shuffle=False, num_workers=2)\n",
    "model = ResNet18(num_classes=NUM_CLASSES).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "print(f\"Few-Show Learning on 18000 samples\")\n",
    "checkpoint = f\"resnet18_lr{lr}_bs{bs}_few_shot-18000.pth\"\n",
    "best_val_acc = 0.0\n",
    "for epoch in range(EPOCHS):\n",
    "    print(f\"Epoch {epoch+1}/{EPOCHS}\")\n",
    "    train_loss, train_acc = train_one_epoch(model, few_shot_train_loader, optimizer, criterion)\n",
    "    val_loss, val_acc = evaluate(model, val_loader, criterion)\n",
    "    \n",
    "    print(f\"Epoch [{epoch+1}/{EPOCHS}] \"\n",
    "          f\"Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.2f}% | \"\n",
    "          f\"Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.2f}%\")\n",
    "    \n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        torch.save(model.state_dict(), checkpoint)\n",
    "        \n",
    "model.load_state_dict(torch.load(checkpoint))\n",
    "test_loss, test_acc = evaluate(model, test_loader, criterion)\n",
    "print(f\"\\nFinal Test Loss: {test_loss:.4f}, Test Acc: {test_acc:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2aa9bda1379835b3",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Let's test also Spatial Dropout\n",
    "class ResNet18_SpatialDropout(nn.Module):\n",
    "    def __init__(self, num_classes=10, dropout_prob=0.5):\n",
    "        super(ResNet18_SpatialDropout, self).__init__()\n",
    "        # Conv1: 7x7, stride=2, padding=3\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        self.bn1   = nn.BatchNorm2d(64)\n",
    "        self.relu  = nn.ReLU(inplace=True)\n",
    "        # MaxPool: 3x3, stride=2, padding=1\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        # Residual layers\n",
    "        self.layer1 = self._make_layer(64, 64, blocks=2, stride=1)    # Output: (N, 64, 8, 8)\n",
    "        self.layer2 = self._make_layer(64, 128, blocks=2, stride=2)   # Output: (N, 128, 4, 4)\n",
    "        # Apply Spatial Dropout on intermediate feature maps after Layer 2\n",
    "        self.spatial_dropout = nn.Dropout2d(p=dropout_prob)\n",
    "        self.layer3 = self._make_layer(128, 256, blocks=2, stride=2)  # Output: (N, 256, 2, 2)\n",
    "        self.layer4 = self._make_layer(256, 512, blocks=2, stride=2)  # Output: (N, 512, 1, 1)\n",
    "        # Global Average Pooling and Fully Connected Layer\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(512, num_classes)\n",
    "    \n",
    "    def _make_layer(self, in_channels, out_channels, blocks, stride):\n",
    "        layers = []\n",
    "        # The first block in the layer may downsample and/or change channels.\n",
    "        layers.append(ResidualBlock(in_channels, out_channels, stride))\n",
    "        for _ in range(1, blocks):\n",
    "            layers.append(ResidualBlock(out_channels, out_channels))\n",
    "        return nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.bn1(self.conv1(x))) \n",
    "        x = self.maxpool(x)                   \n",
    "        x = self.layer1(x)          \n",
    "        x = self.layer2(x)               \n",
    "        x = self.spatial_dropout(x)   \n",
    "        x = self.layer3(x)           \n",
    "        x = self.layer4(x)              \n",
    "        x = self.avgpool(x)                \n",
    "        x = torch.flatten(x, 1)               \n",
    "        x = self.fc(x)                         \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1455361d7437eb8c",
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "train_transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.RandomHorizontalFlip(p=0.5),\n",
    "        transforms.RandomCrop(32, padding=4),\n",
    "        transforms.RandomRotation(degrees=15),\n",
    "        transforms.ToTensor()\n",
    "    ]\n",
    ")\n",
    "train_dataset = ImageFolder(TRAIN_DIR, transform=train_transform)\n",
    "val_dataset   = ImageFolder(VAL_DIR, transform=basic_transform)\n",
    "test_dataset  = ImageFolder(TEST_DIR, transform=basic_transform)\n",
    "lr = 0.001\n",
    "bs = 100\n",
    "train_loader = DataLoader(train_dataset, batch_size=bs, shuffle=True, num_workers= 2)\n",
    "val_loader  = DataLoader(val_dataset, batch_size=bs, shuffle=False, num_workers=2)\n",
    "test_loader = DataLoader(test_dataset, batch_size=bs, shuffle=False, num_workers=2)\n",
    "model = ResNet18_SpatialDropout(num_classes=10, dropout_prob=0.5).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ef5e3809270705a5",
   "metadata": {
    "collapsed": false,
    "is_executing": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/13\n",
      "Batch 60: Loss = 2.0431\n",
      "Batch 120: Loss = 1.8371\n",
      "Batch 180: Loss = 1.7491\n",
      "Batch 240: Loss = 1.9268\n",
      "Batch 300: Loss = 1.6043\n",
      "Batch 360: Loss = 1.6432\n",
      "Batch 420: Loss = 1.7053\n",
      "Batch 480: Loss = 1.6597\n",
      "Batch 540: Loss = 1.5237\n",
      "Batch 600: Loss = 1.5422\n",
      "Batch 660: Loss = 1.6333\n",
      "Batch 720: Loss = 1.7824\n",
      "Batch 780: Loss = 1.6826\n",
      "Batch 840: Loss = 1.6450\n",
      "Batch 900: Loss = 1.5491\n",
      "Epoch [1/13] Train Loss: 1.7680 | Train Acc: 34.10% | Val Loss: 1.7762 | Val Acc: 37.77%\n",
      "Epoch 2/13\n",
      "Batch 60: Loss = 1.4788\n",
      "Batch 120: Loss = 1.7629\n",
      "Batch 180: Loss = 1.5412\n",
      "Batch 240: Loss = 1.5569\n",
      "Batch 300: Loss = 1.6144\n",
      "Batch 360: Loss = 1.6789\n",
      "Batch 420: Loss = 1.4230\n",
      "Batch 480: Loss = 1.4854\n",
      "Batch 540: Loss = 1.5872\n",
      "Batch 600: Loss = 1.5668\n",
      "Batch 660: Loss = 1.4766\n",
      "Batch 720: Loss = 1.4374\n",
      "Batch 780: Loss = 1.4660\n",
      "Batch 840: Loss = 1.3530\n",
      "Batch 900: Loss = 1.4471\n",
      "Epoch [2/13] Train Loss: 1.5248 | Train Acc: 44.14% | Val Loss: 1.4378 | Val Acc: 47.57%\n",
      "Epoch 3/13\n",
      "Batch 60: Loss = 1.4244\n",
      "Batch 120: Loss = 1.5255\n",
      "Batch 180: Loss = 1.3920\n",
      "Batch 240: Loss = 1.3173\n",
      "Batch 300: Loss = 1.3909\n",
      "Batch 360: Loss = 1.3888\n",
      "Batch 420: Loss = 1.3734\n",
      "Batch 480: Loss = 1.4512\n",
      "Batch 540: Loss = 1.4685\n",
      "Batch 600: Loss = 1.3578\n",
      "Batch 660: Loss = 1.4584\n",
      "Batch 720: Loss = 1.4647\n",
      "Batch 780: Loss = 1.2164\n",
      "Batch 840: Loss = 1.4092\n",
      "Batch 900: Loss = 1.3323\n",
      "Epoch [3/13] Train Loss: 1.4138 | Train Acc: 48.58% | Val Loss: 1.5494 | Val Acc: 45.36%\n",
      "Epoch 4/13\n",
      "Batch 60: Loss = 1.3508\n",
      "Batch 120: Loss = 1.3874\n",
      "Batch 180: Loss = 1.3186\n",
      "Batch 240: Loss = 1.1270\n",
      "Batch 300: Loss = 1.0702\n",
      "Batch 360: Loss = 1.5039\n",
      "Batch 420: Loss = 1.4717\n",
      "Batch 480: Loss = 1.4141\n",
      "Batch 540: Loss = 1.3221\n",
      "Batch 600: Loss = 1.1999\n",
      "Batch 660: Loss = 1.3583\n",
      "Batch 720: Loss = 1.3333\n",
      "Batch 780: Loss = 1.3522\n",
      "Batch 840: Loss = 1.3734\n",
      "Batch 900: Loss = 1.3793\n",
      "Epoch [4/13] Train Loss: 1.3367 | Train Acc: 51.80% | Val Loss: 1.2990 | Val Acc: 52.93%\n",
      "Epoch 5/13\n",
      "Batch 60: Loss = 1.2873\n",
      "Batch 120: Loss = 1.3466\n",
      "Batch 180: Loss = 1.2839\n",
      "Batch 240: Loss = 1.4679\n",
      "Batch 300: Loss = 1.2760\n",
      "Batch 360: Loss = 1.3800\n",
      "Batch 420: Loss = 1.3160\n",
      "Batch 480: Loss = 1.1634\n",
      "Batch 540: Loss = 1.2979\n",
      "Batch 600: Loss = 1.4568\n",
      "Batch 660: Loss = 1.2791\n",
      "Batch 720: Loss = 1.2474\n",
      "Batch 780: Loss = 1.0937\n",
      "Batch 840: Loss = 1.2754\n",
      "Batch 900: Loss = 1.3768\n",
      "Epoch [5/13] Train Loss: 1.2767 | Train Acc: 54.07% | Val Loss: 1.3104 | Val Acc: 52.73%\n",
      "Epoch 6/13\n",
      "Batch 60: Loss = 1.1392\n",
      "Batch 120: Loss = 1.4505\n",
      "Batch 180: Loss = 1.3280\n",
      "Batch 240: Loss = 1.2506\n",
      "Batch 300: Loss = 1.1185\n",
      "Batch 360: Loss = 1.2666\n",
      "Batch 420: Loss = 1.3178\n",
      "Batch 480: Loss = 1.1766\n",
      "Batch 540: Loss = 1.4418\n",
      "Batch 600: Loss = 1.2194\n",
      "Batch 660: Loss = 1.1466\n",
      "Batch 720: Loss = 1.1899\n",
      "Batch 780: Loss = 1.2931\n",
      "Batch 840: Loss = 1.2236\n",
      "Batch 900: Loss = 1.1987\n",
      "Epoch [6/13] Train Loss: 1.2303 | Train Acc: 55.88% | Val Loss: 1.2602 | Val Acc: 55.85%\n",
      "Epoch 7/13\n",
      "Batch 60: Loss = 1.1285\n",
      "Batch 120: Loss = 1.0721\n",
      "Batch 180: Loss = 1.1345\n",
      "Batch 240: Loss = 1.1094\n",
      "Batch 300: Loss = 1.1087\n",
      "Batch 360: Loss = 1.1671\n",
      "Batch 420: Loss = 1.0687\n",
      "Batch 480: Loss = 1.1052\n",
      "Batch 540: Loss = 1.0512\n",
      "Batch 600: Loss = 1.0917\n",
      "Batch 660: Loss = 1.1478\n",
      "Batch 720: Loss = 1.0915\n",
      "Batch 780: Loss = 1.0845\n",
      "Batch 840: Loss = 1.3500\n",
      "Batch 900: Loss = 1.2495\n",
      "Epoch [7/13] Train Loss: 1.1911 | Train Acc: 57.22% | Val Loss: 1.2679 | Val Acc: 55.77%\n",
      "Epoch 8/13\n",
      "Batch 60: Loss = 1.2652\n",
      "Batch 120: Loss = 1.0750\n",
      "Batch 180: Loss = 1.1310\n",
      "Batch 240: Loss = 1.1183\n",
      "Batch 300: Loss = 1.1902\n",
      "Batch 360: Loss = 1.4547\n",
      "Batch 420: Loss = 1.1078\n",
      "Batch 480: Loss = 1.3348\n",
      "Batch 540: Loss = 1.0738\n",
      "Batch 600: Loss = 1.1494\n",
      "Batch 660: Loss = 1.1827\n",
      "Batch 720: Loss = 1.0595\n",
      "Batch 780: Loss = 1.0433\n",
      "Batch 840: Loss = 1.1251\n",
      "Batch 900: Loss = 1.0243\n",
      "Epoch [8/13] Train Loss: 1.1580 | Train Acc: 58.71% | Val Loss: 1.0938 | Val Acc: 60.62%\n",
      "Epoch 9/13\n",
      "Batch 60: Loss = 1.1326\n",
      "Batch 120: Loss = 1.2677\n",
      "Batch 180: Loss = 1.4351\n",
      "Batch 240: Loss = 1.0416\n",
      "Batch 300: Loss = 1.1070\n",
      "Batch 360: Loss = 1.0872\n",
      "Batch 420: Loss = 1.1609\n",
      "Batch 480: Loss = 1.1666\n",
      "Batch 540: Loss = 1.1217\n",
      "Batch 600: Loss = 1.1350\n",
      "Batch 660: Loss = 1.1534\n",
      "Batch 720: Loss = 1.2557\n",
      "Batch 780: Loss = 1.3039\n",
      "Batch 840: Loss = 1.0344\n",
      "Batch 900: Loss = 1.3041\n",
      "Epoch [9/13] Train Loss: 1.1277 | Train Acc: 59.81% | Val Loss: 1.1057 | Val Acc: 61.01%\n",
      "Epoch 10/13\n",
      "Batch 60: Loss = 1.1995\n",
      "Batch 120: Loss = 1.0799\n",
      "Batch 180: Loss = 1.2816\n",
      "Batch 240: Loss = 1.2650\n",
      "Batch 300: Loss = 0.9016\n",
      "Batch 360: Loss = 1.0746\n",
      "Batch 420: Loss = 1.0239\n",
      "Batch 480: Loss = 0.9555\n",
      "Batch 540: Loss = 0.9272\n",
      "Batch 600: Loss = 1.1232\n",
      "Batch 660: Loss = 0.9894\n",
      "Batch 720: Loss = 1.1720\n",
      "Batch 780: Loss = 1.2329\n",
      "Batch 840: Loss = 1.3046\n",
      "Batch 900: Loss = 1.1198\n",
      "Epoch [10/13] Train Loss: 1.1012 | Train Acc: 60.78% | Val Loss: 1.1225 | Val Acc: 59.83%\n",
      "Epoch 11/13\n",
      "Batch 60: Loss = 1.1666\n",
      "Batch 120: Loss = 1.1412\n",
      "Batch 180: Loss = 0.9707\n",
      "Batch 240: Loss = 1.0961\n",
      "Batch 300: Loss = 0.9666\n",
      "Batch 360: Loss = 1.1028\n",
      "Batch 420: Loss = 1.0706\n",
      "Batch 480: Loss = 1.0598\n",
      "Batch 540: Loss = 1.0140\n",
      "Batch 600: Loss = 0.9168\n",
      "Batch 660: Loss = 1.1195\n",
      "Batch 720: Loss = 1.2405\n",
      "Batch 780: Loss = 1.0179\n",
      "Batch 840: Loss = 1.2803\n",
      "Batch 900: Loss = 0.9932\n",
      "Epoch [11/13] Train Loss: 1.0819 | Train Acc: 61.32% | Val Loss: 1.3043 | Val Acc: 55.32%\n",
      "Epoch 12/13\n",
      "Batch 60: Loss = 0.9656\n",
      "Batch 120: Loss = 1.0555\n",
      "Batch 180: Loss = 0.9946\n",
      "Batch 240: Loss = 1.0809\n",
      "Batch 300: Loss = 1.2112\n",
      "Batch 360: Loss = 1.1834\n",
      "Batch 420: Loss = 1.0524\n",
      "Batch 480: Loss = 1.1747\n",
      "Batch 540: Loss = 1.0414\n",
      "Batch 600: Loss = 1.1489\n",
      "Batch 660: Loss = 1.0105\n",
      "Batch 720: Loss = 0.9792\n",
      "Batch 780: Loss = 0.8389\n",
      "Batch 840: Loss = 1.1835\n",
      "Batch 900: Loss = 1.0035\n",
      "Epoch [12/13] Train Loss: 1.0574 | Train Acc: 62.42% | Val Loss: 1.1132 | Val Acc: 60.65%\n",
      "Epoch 13/13\n",
      "Batch 60: Loss = 1.0097\n",
      "Batch 120: Loss = 1.0167\n",
      "Batch 180: Loss = 1.0200\n",
      "Batch 240: Loss = 0.9586\n",
      "Batch 300: Loss = 0.9332\n",
      "Batch 360: Loss = 1.1044\n",
      "Batch 420: Loss = 1.0629\n",
      "Batch 480: Loss = 0.9919\n",
      "Batch 540: Loss = 1.1223\n",
      "Batch 600: Loss = 1.1473\n",
      "Batch 660: Loss = 1.1386\n",
      "Batch 720: Loss = 0.8499\n",
      "Batch 780: Loss = 1.0575\n",
      "Batch 840: Loss = 1.1354\n",
      "Batch 900: Loss = 0.9425\n",
      "Epoch [13/13] Train Loss: 1.0371 | Train Acc: 63.07% | Val Loss: 1.0308 | Val Acc: 63.22%\n",
      "\n",
      "Final Test Loss: 1.0383, Test Acc: 63.15%\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 13\n",
    "best_val_acc = 0.0\n",
    "checkpoint = f\"resnet18_lr{lr}_bs{bs}_data_aug_spatial_dropout.pth\"\n",
    "for epoch in range(EPOCHS):\n",
    "    print(f\"Epoch {epoch+1}/{EPOCHS}\")\n",
    "    train_loss, train_acc = train_one_epoch(model, train_loader, optimizer, criterion)\n",
    "    val_loss, val_acc = evaluate(model, val_loader, criterion)\n",
    "    \n",
    "    print(f\"Epoch [{epoch+1}/{EPOCHS}] \"\n",
    "          f\"Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.2f}% | \"\n",
    "          f\"Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.2f}%\")\n",
    "    \n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        torch.save(model.state_dict(), checkpoint)\n",
    "        \n",
    "model.load_state_dict(torch.load(checkpoint))\n",
    "test_loss, test_acc = evaluate(model, test_loader, criterion)\n",
    "print(f\"\\nFinal Test Loss: {test_loss:.4f}, Test Acc: {test_acc:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9390c263e98e0eb2",
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "Batch 60: Loss = 1.0699\n",
      "Batch 120: Loss = 1.0164\n",
      "Batch 180: Loss = 0.8923\n",
      "Batch 240: Loss = 0.9853\n",
      "Batch 300: Loss = 0.9955\n",
      "Batch 360: Loss = 0.9890\n",
      "Batch 420: Loss = 0.9817\n",
      "Batch 480: Loss = 0.9759\n",
      "Batch 540: Loss = 0.8682\n",
      "Batch 600: Loss = 1.0499\n",
      "Batch 660: Loss = 1.0242\n",
      "Batch 720: Loss = 0.8905\n",
      "Batch 780: Loss = 1.0600\n",
      "Batch 840: Loss = 0.9122\n",
      "Batch 900: Loss = 1.0407\n",
      "Epoch [1/10] Train Loss: 1.0202 | Train Acc: 63.90% | Val Loss: 1.0276 | Val Acc: 63.82%\n",
      "Epoch 2/10\n",
      "Batch 60: Loss = 1.0698\n",
      "Batch 120: Loss = 0.9862\n",
      "Batch 180: Loss = 1.0656\n",
      "Batch 240: Loss = 1.1703\n",
      "Batch 300: Loss = 1.1025\n",
      "Batch 360: Loss = 0.8921\n",
      "Batch 420: Loss = 0.8121\n",
      "Batch 480: Loss = 1.0352\n",
      "Batch 540: Loss = 1.0161\n",
      "Batch 600: Loss = 0.9526\n",
      "Batch 660: Loss = 1.0170\n",
      "Batch 720: Loss = 1.3744\n",
      "Batch 780: Loss = 1.0009\n",
      "Batch 840: Loss = 1.0095\n",
      "Batch 900: Loss = 0.9669\n",
      "Epoch [2/10] Train Loss: 1.0024 | Train Acc: 64.52% | Val Loss: 0.9854 | Val Acc: 64.56%\n",
      "Epoch 3/10\n",
      "Batch 60: Loss = 0.9003\n",
      "Batch 120: Loss = 0.9983\n",
      "Batch 180: Loss = 1.0047\n",
      "Batch 240: Loss = 1.0961\n",
      "Batch 300: Loss = 0.9778\n",
      "Batch 360: Loss = 0.9029\n",
      "Batch 420: Loss = 1.1102\n",
      "Batch 480: Loss = 0.9911\n",
      "Batch 540: Loss = 0.9111\n",
      "Batch 600: Loss = 0.8475\n",
      "Batch 660: Loss = 0.8332\n",
      "Batch 720: Loss = 1.0784\n",
      "Batch 780: Loss = 0.9089\n",
      "Batch 840: Loss = 1.0601\n",
      "Batch 900: Loss = 1.0465\n",
      "Epoch [3/10] Train Loss: 0.9914 | Train Acc: 64.89% | Val Loss: 1.1408 | Val Acc: 60.67%\n",
      "Epoch 4/10\n",
      "Batch 60: Loss = 1.0576\n",
      "Batch 120: Loss = 0.9063\n",
      "Batch 180: Loss = 1.0189\n",
      "Batch 240: Loss = 0.8928\n",
      "Batch 300: Loss = 0.8774\n",
      "Batch 360: Loss = 1.0238\n",
      "Batch 420: Loss = 0.9147\n",
      "Batch 480: Loss = 0.9009\n",
      "Batch 540: Loss = 0.8850\n",
      "Batch 600: Loss = 1.1133\n",
      "Batch 660: Loss = 0.9127\n",
      "Batch 720: Loss = 1.0239\n",
      "Batch 780: Loss = 1.0076\n",
      "Batch 840: Loss = 1.0639\n",
      "Batch 900: Loss = 0.8534\n",
      "Epoch [4/10] Train Loss: 0.9769 | Train Acc: 65.36% | Val Loss: 0.9624 | Val Acc: 65.96%\n",
      "Epoch 5/10\n",
      "Batch 60: Loss = 1.1556\n",
      "Batch 120: Loss = 1.0051\n",
      "Batch 180: Loss = 0.9292\n",
      "Batch 240: Loss = 0.8531\n",
      "Batch 300: Loss = 0.9485\n",
      "Batch 360: Loss = 0.8813\n",
      "Batch 420: Loss = 0.9984\n",
      "Batch 480: Loss = 0.9856\n",
      "Batch 540: Loss = 1.0691\n",
      "Batch 600: Loss = 0.9264\n",
      "Batch 660: Loss = 0.9814\n",
      "Batch 720: Loss = 0.9041\n",
      "Batch 780: Loss = 0.9964\n",
      "Batch 840: Loss = 0.7558\n",
      "Batch 900: Loss = 1.3719\n",
      "Epoch [5/10] Train Loss: 0.9568 | Train Acc: 66.10% | Val Loss: 0.9780 | Val Acc: 64.99%\n",
      "Epoch 6/10\n",
      "Batch 60: Loss = 0.8411\n",
      "Batch 120: Loss = 0.9675\n",
      "Batch 180: Loss = 0.9887\n",
      "Batch 240: Loss = 0.9228\n",
      "Batch 300: Loss = 1.1751\n",
      "Batch 360: Loss = 0.8885\n",
      "Batch 420: Loss = 0.9531\n",
      "Batch 480: Loss = 1.0059\n",
      "Batch 540: Loss = 1.0011\n",
      "Batch 600: Loss = 1.0414\n",
      "Batch 660: Loss = 0.8601\n",
      "Batch 720: Loss = 0.9339\n",
      "Batch 780: Loss = 0.7320\n",
      "Batch 840: Loss = 0.9953\n",
      "Batch 900: Loss = 0.9842\n",
      "Epoch [6/10] Train Loss: 0.9491 | Train Acc: 66.33% | Val Loss: 0.9702 | Val Acc: 65.80%\n",
      "Epoch 7/10\n",
      "Batch 60: Loss = 0.8019\n",
      "Batch 120: Loss = 0.9335\n",
      "Batch 180: Loss = 0.9769\n",
      "Batch 240: Loss = 0.8830\n",
      "Batch 300: Loss = 0.8439\n",
      "Batch 360: Loss = 1.0224\n",
      "Batch 420: Loss = 0.9940\n",
      "Batch 480: Loss = 0.8867\n",
      "Batch 540: Loss = 0.9526\n",
      "Batch 600: Loss = 0.7228\n",
      "Batch 660: Loss = 1.1308\n",
      "Batch 720: Loss = 0.9418\n",
      "Batch 780: Loss = 0.9382\n",
      "Batch 840: Loss = 0.9725\n",
      "Batch 900: Loss = 1.0381\n",
      "Epoch [7/10] Train Loss: 0.9376 | Train Acc: 66.78% | Val Loss: 0.9119 | Val Acc: 67.77%\n",
      "Epoch 8/10\n",
      "Batch 60: Loss = 0.7876\n",
      "Batch 120: Loss = 0.8137\n",
      "Batch 180: Loss = 0.6767\n",
      "Batch 240: Loss = 0.9042\n",
      "Batch 300: Loss = 0.8867\n",
      "Batch 360: Loss = 0.8117\n",
      "Batch 420: Loss = 0.7534\n",
      "Batch 480: Loss = 0.8804\n",
      "Batch 540: Loss = 1.1505\n",
      "Batch 600: Loss = 0.7575\n",
      "Batch 660: Loss = 0.9490\n",
      "Batch 720: Loss = 1.0704\n",
      "Batch 780: Loss = 0.9899\n",
      "Batch 840: Loss = 1.0516\n",
      "Batch 900: Loss = 0.9860\n",
      "Epoch [8/10] Train Loss: 0.9223 | Train Acc: 67.37% | Val Loss: 0.9275 | Val Acc: 67.15%\n",
      "Epoch 9/10\n",
      "Batch 60: Loss = 0.9081\n",
      "Batch 120: Loss = 0.7415\n",
      "Batch 180: Loss = 0.8767\n",
      "Batch 240: Loss = 0.9811\n",
      "Batch 300: Loss = 0.7806\n",
      "Batch 360: Loss = 1.0986\n",
      "Batch 420: Loss = 0.8307\n",
      "Batch 480: Loss = 0.7605\n",
      "Batch 540: Loss = 1.2295\n",
      "Batch 600: Loss = 0.8728\n",
      "Batch 660: Loss = 0.7860\n",
      "Batch 720: Loss = 0.8909\n",
      "Batch 780: Loss = 0.9872\n",
      "Batch 840: Loss = 0.7835\n",
      "Batch 900: Loss = 0.7158\n",
      "Epoch [9/10] Train Loss: 0.9123 | Train Acc: 67.73% | Val Loss: 0.9365 | Val Acc: 66.73%\n",
      "Epoch 10/10\n",
      "Batch 60: Loss = 0.7858\n",
      "Batch 120: Loss = 0.7710\n",
      "Batch 180: Loss = 0.9359\n",
      "Batch 240: Loss = 1.0720\n",
      "Batch 300: Loss = 0.8780\n",
      "Batch 360: Loss = 0.9221\n",
      "Batch 420: Loss = 0.7647\n",
      "Batch 480: Loss = 1.1277\n",
      "Batch 540: Loss = 0.8335\n",
      "Batch 600: Loss = 0.9976\n",
      "Batch 660: Loss = 0.8321\n",
      "Batch 720: Loss = 1.1557\n",
      "Batch 780: Loss = 0.9214\n",
      "Batch 840: Loss = 0.8910\n",
      "Batch 900: Loss = 1.0538\n",
      "Epoch [10/10] Train Loss: 0.9023 | Train Acc: 67.92% | Val Loss: 0.9384 | Val Acc: 67.16%\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "Caught OSError in DataLoader worker process 1.\nOriginal Traceback (most recent call last):\n  File \"/usr/local/Cellar/jupyterlab/4.3.1_1/libexec/lib/python3.12/site-packages/torch/utils/data/_utils/worker.py\", line 308, in _worker_loop\n    data = fetcher.fetch(index)\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/Cellar/jupyterlab/4.3.1_1/libexec/lib/python3.12/site-packages/torch/utils/data/_utils/fetch.py\", line 51, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n            ~~~~~~~~~~~~^^^^^\n  File \"/usr/local/Cellar/jupyterlab/4.3.1_1/libexec/lib/python3.12/site-packages/torchvision/datasets/folder.py\", line 229, in __getitem__\n    sample = self.loader(path)\n             ^^^^^^^^^^^^^^^^^\n  File \"/usr/local/Cellar/jupyterlab/4.3.1_1/libexec/lib/python3.12/site-packages/torchvision/datasets/folder.py\", line 268, in default_loader\n    return pil_loader(path)\n           ^^^^^^^^^^^^^^^^\n  File \"/usr/local/Cellar/jupyterlab/4.3.1_1/libexec/lib/python3.12/site-packages/torchvision/datasets/folder.py\", line 247, in pil_loader\n    img = Image.open(f)\n          ^^^^^^^^^^^^^\n  File \"/usr/local/Cellar/jupyterlab/4.3.1_1/libexec/lib/python3.12/site-packages/PIL/Image.py\", line 3480, in open\n    prefix = fp.read(16)\n             ^^^^^^^^^^^\nOSError: [Errno 11] Resource deadlock avoided\n",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mOSError\u001B[0m                                   Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[18], line 18\u001B[0m\n\u001B[1;32m     15\u001B[0m         torch\u001B[38;5;241m.\u001B[39msave(model\u001B[38;5;241m.\u001B[39mstate_dict(), checkpoint)\n\u001B[1;32m     17\u001B[0m model\u001B[38;5;241m.\u001B[39mload_state_dict(torch\u001B[38;5;241m.\u001B[39mload(checkpoint))\n\u001B[0;32m---> 18\u001B[0m test_loss, test_acc \u001B[38;5;241m=\u001B[39m \u001B[43mevaluate\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtest_loader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcriterion\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     19\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124mFinal Test Loss: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mtest_loss\u001B[38;5;132;01m:\u001B[39;00m\u001B[38;5;124m.4f\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m, Test Acc: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mtest_acc\u001B[38;5;132;01m:\u001B[39;00m\u001B[38;5;124m.2f\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m%\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "Cell \u001B[0;32mIn[6], line 46\u001B[0m, in \u001B[0;36mevaluate\u001B[0;34m(model, loader, criterion)\u001B[0m\n\u001B[1;32m     43\u001B[0m total \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m\n\u001B[1;32m     45\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mno_grad():\n\u001B[0;32m---> 46\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mimages\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlabels\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mloader\u001B[49m\u001B[43m:\u001B[49m\n\u001B[1;32m     47\u001B[0m \u001B[43m        \u001B[49m\u001B[43mimages\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlabels\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mimages\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mto\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlabels\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mto\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     49\u001B[0m \u001B[43m        \u001B[49m\u001B[43moutputs\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[43mimages\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/usr/local/Cellar/jupyterlab/4.3.1_1/libexec/lib/python3.12/site-packages/torch/utils/data/dataloader.py:631\u001B[0m, in \u001B[0;36m_BaseDataLoaderIter.__next__\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    628\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sampler_iter \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    629\u001B[0m     \u001B[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001B[39;00m\n\u001B[1;32m    630\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_reset()  \u001B[38;5;66;03m# type: ignore[call-arg]\u001B[39;00m\n\u001B[0;32m--> 631\u001B[0m data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_next_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    632\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[1;32m    633\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_dataset_kind \u001B[38;5;241m==\u001B[39m _DatasetKind\u001B[38;5;241m.\u001B[39mIterable \u001B[38;5;129;01mand\u001B[39;00m \\\n\u001B[1;32m    634\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m \\\n\u001B[1;32m    635\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m>\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called:\n",
      "File \u001B[0;32m/usr/local/Cellar/jupyterlab/4.3.1_1/libexec/lib/python3.12/site-packages/torch/utils/data/dataloader.py:1346\u001B[0m, in \u001B[0;36m_MultiProcessingDataLoaderIter._next_data\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   1344\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m   1345\u001B[0m     \u001B[38;5;28;01mdel\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_task_info[idx]\n\u001B[0;32m-> 1346\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_process_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdata\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/usr/local/Cellar/jupyterlab/4.3.1_1/libexec/lib/python3.12/site-packages/torch/utils/data/dataloader.py:1372\u001B[0m, in \u001B[0;36m_MultiProcessingDataLoaderIter._process_data\u001B[0;34m(self, data)\u001B[0m\n\u001B[1;32m   1370\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_try_put_index()\n\u001B[1;32m   1371\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(data, ExceptionWrapper):\n\u001B[0;32m-> 1372\u001B[0m     \u001B[43mdata\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mreraise\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1373\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m data\n",
      "File \u001B[0;32m/usr/local/Cellar/jupyterlab/4.3.1_1/libexec/lib/python3.12/site-packages/torch/_utils.py:722\u001B[0m, in \u001B[0;36mExceptionWrapper.reraise\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    718\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m:\n\u001B[1;32m    719\u001B[0m     \u001B[38;5;66;03m# If the exception takes multiple arguments, don't try to\u001B[39;00m\n\u001B[1;32m    720\u001B[0m     \u001B[38;5;66;03m# instantiate since we don't know how to\u001B[39;00m\n\u001B[1;32m    721\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(msg) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m--> 722\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m exception\n",
      "\u001B[0;31mOSError\u001B[0m: Caught OSError in DataLoader worker process 1.\nOriginal Traceback (most recent call last):\n  File \"/usr/local/Cellar/jupyterlab/4.3.1_1/libexec/lib/python3.12/site-packages/torch/utils/data/_utils/worker.py\", line 308, in _worker_loop\n    data = fetcher.fetch(index)\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/Cellar/jupyterlab/4.3.1_1/libexec/lib/python3.12/site-packages/torch/utils/data/_utils/fetch.py\", line 51, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n            ~~~~~~~~~~~~^^^^^\n  File \"/usr/local/Cellar/jupyterlab/4.3.1_1/libexec/lib/python3.12/site-packages/torchvision/datasets/folder.py\", line 229, in __getitem__\n    sample = self.loader(path)\n             ^^^^^^^^^^^^^^^^^\n  File \"/usr/local/Cellar/jupyterlab/4.3.1_1/libexec/lib/python3.12/site-packages/torchvision/datasets/folder.py\", line 268, in default_loader\n    return pil_loader(path)\n           ^^^^^^^^^^^^^^^^\n  File \"/usr/local/Cellar/jupyterlab/4.3.1_1/libexec/lib/python3.12/site-packages/torchvision/datasets/folder.py\", line 247, in pil_loader\n    img = Image.open(f)\n          ^^^^^^^^^^^^^\n  File \"/usr/local/Cellar/jupyterlab/4.3.1_1/libexec/lib/python3.12/site-packages/PIL/Image.py\", line 3480, in open\n    prefix = fp.read(16)\n             ^^^^^^^^^^^\nOSError: [Errno 11] Resource deadlock avoided\n"
     ]
    }
   ],
   "source": [
    "# Extra 10 epochs\n",
    "EPOCHS = 10\n",
    "best_val_acc = 0.0\n",
    "model.load_state_dict(torch.load(\"drop-out/resnet18_lr0.001_bs100_data_aug_spatial_dropout.pth\"))\n",
    "for epoch in range(EPOCHS):\n",
    "    print(f\"Epoch {epoch+1}/{EPOCHS}\")\n",
    "    train_loss, train_acc = train_one_epoch(model, train_loader, optimizer, criterion)\n",
    "    val_loss, val_acc = evaluate(model, val_loader, criterion)\n",
    "    \n",
    "    print(f\"Epoch [{epoch+1}/{EPOCHS}] \"\n",
    "          f\"Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.2f}% | \"\n",
    "          f\"Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.2f}%\")\n",
    "    \n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        torch.save(model.state_dict(), checkpoint)\n",
    "        \n",
    "model.load_state_dict(torch.load(checkpoint))\n",
    "test_loss, test_acc = evaluate(model, test_loader, criterion)\n",
    "print(f\"\\nFinal Test Loss: {test_loss:.4f}, Test Acc: {test_acc:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4f078889-6f20-4ddc-b3ea-eede85d35a16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final Test Loss: 0.9168, Test Acc: 67.52%\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(\"drop-out/resnet18_lr0.001_bs100_data_aug_spatial_dropout_acc_67.pth\"))\n",
    "test_loss, test_acc = evaluate(model, test_loader, criterion)\n",
    "print(f\"\\nFinal Test Loss: {test_loss:.4f}, Test Acc: {test_acc:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "61681a1f-57a2-4bca-a27c-475f88744897",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "Batch 60: Loss = 0.7987\n",
      "Batch 120: Loss = 0.9822\n",
      "Batch 180: Loss = 0.9593\n",
      "Batch 240: Loss = 0.8798\n",
      "Batch 300: Loss = 0.7022\n",
      "Batch 360: Loss = 0.9033\n",
      "Batch 420: Loss = 0.8978\n",
      "Batch 480: Loss = 0.9722\n",
      "Batch 540: Loss = 0.9636\n",
      "Batch 600: Loss = 0.8018\n",
      "Batch 660: Loss = 0.8686\n",
      "Batch 720: Loss = 0.8972\n",
      "Batch 780: Loss = 1.0834\n",
      "Batch 840: Loss = 0.9211\n",
      "Batch 900: Loss = 0.9056\n",
      "Epoch [1/10] Train Loss: 0.9247 | Train Acc: 67.13% | Val Loss: 1.1080 | Val Acc: 62.00%\n",
      "Epoch 2/10\n",
      "Batch 60: Loss = 1.0011\n",
      "Batch 120: Loss = 1.1084\n",
      "Batch 180: Loss = 0.8666\n",
      "Batch 240: Loss = 0.9083\n",
      "Batch 300: Loss = 0.9009\n",
      "Batch 360: Loss = 1.0323\n",
      "Batch 420: Loss = 0.7668\n",
      "Batch 480: Loss = 0.9138\n",
      "Batch 540: Loss = 0.9409\n",
      "Batch 600: Loss = 0.8340\n",
      "Batch 660: Loss = 0.7921\n",
      "Batch 720: Loss = 0.9307\n",
      "Batch 780: Loss = 0.9577\n",
      "Batch 840: Loss = 0.9988\n",
      "Batch 900: Loss = 0.8040\n",
      "Epoch [2/10] Train Loss: 0.9177 | Train Acc: 67.49% | Val Loss: 0.9761 | Val Acc: 65.50%\n",
      "Epoch 3/10\n",
      "Batch 60: Loss = 0.7785\n",
      "Batch 120: Loss = 0.9578\n",
      "Batch 180: Loss = 0.6695\n",
      "Batch 240: Loss = 1.0267\n",
      "Batch 300: Loss = 0.8905\n",
      "Batch 360: Loss = 1.0045\n",
      "Batch 420: Loss = 0.8772\n",
      "Batch 480: Loss = 0.9166\n",
      "Batch 540: Loss = 0.9749\n",
      "Batch 600: Loss = 0.8660\n",
      "Batch 660: Loss = 0.9974\n",
      "Batch 720: Loss = 1.1653\n",
      "Batch 780: Loss = 1.0285\n",
      "Batch 840: Loss = 1.0266\n",
      "Batch 900: Loss = 0.8463\n",
      "Epoch [3/10] Train Loss: 0.9047 | Train Acc: 67.83% | Val Loss: 0.9351 | Val Acc: 67.24%\n",
      "Epoch 4/10\n",
      "Batch 60: Loss = 1.1533\n",
      "Batch 120: Loss = 0.9312\n",
      "Batch 180: Loss = 0.8682\n",
      "Batch 240: Loss = 0.7586\n",
      "Batch 300: Loss = 0.7371\n",
      "Batch 360: Loss = 0.9720\n",
      "Batch 420: Loss = 0.9215\n",
      "Batch 480: Loss = 0.8007\n",
      "Batch 540: Loss = 0.7883\n",
      "Batch 600: Loss = 0.8634\n",
      "Batch 660: Loss = 1.0150\n",
      "Batch 720: Loss = 0.8171\n",
      "Batch 780: Loss = 0.8609\n",
      "Batch 840: Loss = 0.8517\n",
      "Batch 900: Loss = 0.9603\n",
      "Epoch [4/10] Train Loss: 0.8978 | Train Acc: 68.12% | Val Loss: 0.9402 | Val Acc: 67.15%\n",
      "Epoch 5/10\n",
      "Batch 60: Loss = 0.8023\n",
      "Batch 120: Loss = 0.8566\n",
      "Batch 180: Loss = 0.8599\n",
      "Batch 240: Loss = 0.7665\n",
      "Batch 300: Loss = 0.8113\n",
      "Batch 360: Loss = 0.7237\n",
      "Batch 420: Loss = 0.8962\n",
      "Batch 480: Loss = 1.0769\n",
      "Batch 540: Loss = 0.9001\n",
      "Batch 600: Loss = 0.9725\n",
      "Batch 660: Loss = 0.8297\n",
      "Batch 720: Loss = 0.8040\n",
      "Batch 780: Loss = 1.1123\n",
      "Batch 840: Loss = 1.0464\n",
      "Batch 900: Loss = 0.8533\n",
      "Epoch [5/10] Train Loss: 0.8878 | Train Acc: 68.47% | Val Loss: 0.9197 | Val Acc: 67.75%\n",
      "Epoch 6/10\n",
      "Batch 60: Loss = 0.8085\n",
      "Batch 120: Loss = 0.9607\n",
      "Batch 180: Loss = 0.7878\n",
      "Batch 240: Loss = 0.9580\n",
      "Batch 300: Loss = 0.8080\n",
      "Batch 360: Loss = 0.7701\n",
      "Batch 420: Loss = 0.7779\n",
      "Batch 480: Loss = 0.8177\n",
      "Batch 540: Loss = 0.8315\n",
      "Batch 600: Loss = 0.7423\n",
      "Batch 660: Loss = 0.8736\n",
      "Batch 720: Loss = 0.9977\n",
      "Batch 780: Loss = 0.8674\n",
      "Batch 840: Loss = 0.9423\n",
      "Batch 900: Loss = 0.7842\n",
      "Epoch [6/10] Train Loss: 0.8772 | Train Acc: 68.77% | Val Loss: 0.9289 | Val Acc: 67.25%\n",
      "Epoch 7/10\n",
      "Batch 60: Loss = 0.8376\n",
      "Batch 120: Loss = 0.7972\n",
      "Batch 180: Loss = 0.8356\n",
      "Batch 240: Loss = 0.8453\n",
      "Batch 300: Loss = 0.8601\n",
      "Batch 360: Loss = 0.9028\n",
      "Batch 420: Loss = 0.7210\n",
      "Batch 480: Loss = 0.9364\n",
      "Batch 540: Loss = 0.8574\n",
      "Batch 600: Loss = 0.7459\n",
      "Batch 660: Loss = 0.9137\n",
      "Batch 720: Loss = 0.7682\n",
      "Batch 780: Loss = 1.1626\n",
      "Batch 840: Loss = 0.9627\n",
      "Batch 900: Loss = 1.1169\n",
      "Epoch [7/10] Train Loss: 0.8749 | Train Acc: 68.84% | Val Loss: 1.0474 | Val Acc: 63.02%\n",
      "Epoch 8/10\n",
      "Batch 60: Loss = 0.6996\n",
      "Batch 120: Loss = 0.7894\n",
      "Batch 180: Loss = 0.8326\n",
      "Batch 240: Loss = 0.9918\n",
      "Batch 300: Loss = 0.8352\n",
      "Batch 360: Loss = 0.9030\n",
      "Batch 420: Loss = 1.0113\n",
      "Batch 480: Loss = 0.8441\n",
      "Batch 540: Loss = 0.8661\n",
      "Batch 600: Loss = 0.8272\n",
      "Batch 660: Loss = 0.7436\n",
      "Batch 720: Loss = 0.9267\n",
      "Batch 780: Loss = 0.8488\n",
      "Batch 840: Loss = 0.8142\n",
      "Batch 900: Loss = 0.8744\n",
      "Epoch [8/10] Train Loss: 0.8620 | Train Acc: 69.29% | Val Loss: 0.8672 | Val Acc: 69.38%\n",
      "Epoch 9/10\n",
      "Batch 60: Loss = 0.8453\n",
      "Batch 120: Loss = 0.7712\n",
      "Batch 180: Loss = 1.0384\n",
      "Batch 240: Loss = 0.8160\n",
      "Batch 300: Loss = 0.7411\n",
      "Batch 360: Loss = 0.9116\n",
      "Batch 420: Loss = 0.7318\n",
      "Batch 480: Loss = 0.7100\n",
      "Batch 540: Loss = 0.8103\n",
      "Batch 600: Loss = 0.6685\n",
      "Batch 660: Loss = 1.1428\n",
      "Batch 720: Loss = 0.8005\n",
      "Batch 780: Loss = 0.7505\n",
      "Batch 840: Loss = 0.8880\n",
      "Batch 900: Loss = 1.0047\n",
      "Epoch [9/10] Train Loss: 0.8566 | Train Acc: 69.50% | Val Loss: 0.9575 | Val Acc: 66.63%\n",
      "Epoch 10/10\n",
      "Batch 60: Loss = 0.9045\n",
      "Batch 120: Loss = 0.8789\n",
      "Batch 180: Loss = 0.8390\n",
      "Batch 240: Loss = 0.8600\n",
      "Batch 300: Loss = 0.7608\n",
      "Batch 360: Loss = 0.9433\n",
      "Batch 420: Loss = 0.8235\n",
      "Batch 480: Loss = 0.7930\n",
      "Batch 540: Loss = 0.8633\n",
      "Batch 600: Loss = 0.6881\n",
      "Batch 660: Loss = 0.9508\n",
      "Batch 720: Loss = 0.7861\n",
      "Batch 780: Loss = 0.8391\n",
      "Batch 840: Loss = 0.7448\n",
      "Batch 900: Loss = 0.6565\n",
      "Epoch [10/10] Train Loss: 0.8516 | Train Acc: 69.80% | Val Loss: 0.9931 | Val Acc: 66.21%\n",
      "\n",
      "Final Test Loss: 0.8748, Test Acc: 69.45%\n"
     ]
    }
   ],
   "source": [
    "# Extra ten epochs\n",
    "EPOCHS = 10\n",
    "best_val_acc = 0.0\n",
    "model.load_state_dict(torch.load(\"drop-out/the-best-resnet18_lr0.001_bs100_data_aug_spatial_dropout.pth\"))\n",
    "checkpoint = f\"resnet18_lr{lr}_bs{bs}_data_aug_spatial_dropout.pth\"\n",
    "for epoch in range(EPOCHS):\n",
    "    print(f\"Epoch {epoch+1}/{EPOCHS}\")\n",
    "    train_loss, train_acc = train_one_epoch(model, train_loader, optimizer, criterion)\n",
    "    val_loss, val_acc = evaluate(model, val_loader, criterion)\n",
    "    \n",
    "    print(f\"Epoch [{epoch+1}/{EPOCHS}] \"\n",
    "          f\"Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.2f}% | \"\n",
    "          f\"Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.2f}%\")\n",
    "    \n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        torch.save(model.state_dict(), checkpoint)\n",
    "        \n",
    "model.load_state_dict(torch.load(checkpoint))\n",
    "test_loss, test_acc = evaluate(model, test_loader, criterion)\n",
    "print(f\"\\nFinal Test Loss: {test_loss:.4f}, Test Acc: {test_acc:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# For final metric if issue arises please try it on Colab\n",
    "model.load_state_dict(torch.load(\"drop-out/resnet18_lr0.001_bs100_data_aug_spatial_dropout_72.pth\", map_location=torch.device('cpu')))\n",
    "model.eval()\n",
    "\n",
    "all_preds = []\n",
    "all_true = []\n",
    "all_probs = []  \n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        probs = F.softmax(outputs, dim=1)\n",
    "        preds = probs.argmax(dim=1)\n",
    "        \n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_true.extend(labels.cpu().numpy())\n",
    "        all_probs.append(probs.cpu())\n",
    "\n",
    "# Concatenate all probability tensors\n",
    "all_probs = torch.cat(all_probs, dim=0)  # Shape: (N, num_classes)\n",
    "\n",
    "# Compute overall metrics using sklearn\n",
    "accuracy = accuracy_score(all_true, all_preds)\n",
    "precision = precision_score(all_true, all_preds, average='macro')\n",
    "recall = recall_score(all_true, all_preds, average='macro')\n",
    "f1 = f1_score(all_true, all_preds, average='macro')\n",
    "\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(all_true, all_preds, digits=4))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f61e8c86eb100c9d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# For Plotting\n",
    "# Adjust the path if needed\n",
    "log_file_path = \"logs/weight_decay_logs.txt\"\n",
    "results = defaultdict(lambda: defaultdict(dict))\n",
    "header_pattern = re.compile(r\"Epoch\\s+1/\\d+\\s+LR\\s+([\\d\\.eE+-]+)\\s+and\\s+batch_size\\s+(\\d+)\\s+wd\\s+([\\d\\.]+)\")\n",
    "train_val_pattern = re.compile(\n",
    "    r\"^\\s*Train Loss:\\s*([\\d\\.]+)\\s*\\|\\s*Train Acc:\\s*([\\d\\.]+)%\\s*\\|\\s*Val Loss:\\s*([\\d\\.]+)\\s*\\|\\s*Val Acc:\\s*([\\d\\.]+)%\"\n",
    ")\n",
    "\n",
    "current_wd = None\n",
    "current_epoch = 0\n",
    "with open(log_file_path, \"r\") as f:\n",
    "    for line in f:\n",
    "        header_match = header_pattern.search(line)\n",
    "        if header_match:\n",
    "            current_lr = float(header_match.group(1))\n",
    "            current_bs = int(header_match.group(2))\n",
    "            current_wd = float(header_match.group(3))\n",
    "            print(current_wd)\n",
    "            current_epoch = 0\n",
    "            continue\n",
    "\n",
    "        tv_match = train_val_pattern.search(line)\n",
    "        if tv_match and current_lr is not None and current_bs is not None:\n",
    "            current_epoch+=1;\n",
    "            train_loss_str = tv_match.group(1)\n",
    "            train_acc_str  = tv_match.group(2)\n",
    "            val_loss_str   = tv_match.group(3)\n",
    "            val_acc_str    = tv_match.group(4)\n",
    "\n",
    "            # Convert to floats\n",
    "            train_acc = float(train_acc_str)\n",
    "            val_acc   = float(val_acc_str)\n",
    "\n",
    "            # Store in our results dictionary\n",
    "            results[current_wd][current_epoch] = {\n",
    "                \"train_acc\": train_acc,\n",
    "                \"val_acc\":   val_acc\n",
    "            }\n",
    "    "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a16f93ea022c7d85"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "num_wds = len(results)\n",
    "fig, axs = plt.subplots(nrows=num_wds, ncols=1, figsize=(10, 5*num_wds), sharex=True)\n",
    "if num_wds == 1:\n",
    "    axs = [axs]\n",
    "\n",
    "for ax, wd in zip(axs, sorted(results.keys())):\n",
    "    epochs = sorted(results[wd].keys())\n",
    "    train_accs = [results[wd][e][\"train_acc\"] for e in epochs]\n",
    "    val_accs = [results[wd][e][\"val_acc\"] for e in epochs]\n",
    "\n",
    "    ax.plot(epochs, train_accs, label=\"Train Accuracy\", linestyle='-', marker='o')\n",
    "    ax.plot(epochs, val_accs, label=\"Validation Accuracy\", linestyle='--', marker='x')\n",
    "    ax.set_title(f\"Weight Decay: {wd}\")\n",
    "    ax.set_xlabel(\"Epoch\")\n",
    "    ax.set_ylabel(\"Accuracy (%)\")\n",
    "    ax.legend()\n",
    "    ax.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"weight_decay_parm_validation_test.png\")\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c78d87936fdea1a2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Adjust the path if needed\n",
    "log_file_path = \"logs/few-shots-18000.txt\"\n",
    "results = defaultdict(lambda: defaultdict(dict))\n",
    "header_pattern = re.compile(r\"Epoch\\s+1/\\d+\")\n",
    "train_val_pattern = re.compile(\n",
    "    r\"Epoch\\s*\\[(\\d+)/(\\d+)\\]\\s+Train Loss:\\s*([\\d\\.]+)\\s*\\|\\s*Train Acc:\\s*([\\d\\.]+)%\\s*\\|\\s*Val Loss:\\s*([\\d\\.]+)\\s*\\|\\s*Val Acc:\\s*([\\d\\.]+)%\"\n",
    ")\n",
    "current_epoch = 0\n",
    "with open(log_file_path, \"r\") as f:\n",
    "    for line in f:\n",
    "        header_match = header_pattern.search(line)\n",
    "        if header_match:\n",
    "            print(\"in\")\n",
    "            current_epoch = 0\n",
    "            continue\n",
    "\n",
    "        tv_match = train_val_pattern.search(line)\n",
    "        if tv_match and current_lr is not None:\n",
    "            current_epoch+=1;\n",
    "            train_acc_str  = tv_match.group(4)\n",
    "            val_acc_str   = tv_match.group(6)\n",
    "\n",
    "            # Convert to floats\n",
    "            train_acc = float(train_acc_str)\n",
    "            val_acc   = float(val_acc_str)\n",
    "\n",
    "            # Store in our results dictionary\n",
    "            results[current_epoch] = {\n",
    "                \"train_acc\": train_acc,\n",
    "                \"val_acc\":   val_acc\n",
    "            }\n",
    "           "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8454f57c643e7f2d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "epochs = len(results)\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Few Shots learning(18000samples) lr 0.001 bs 100\")\n",
    "plt.plot(results.keys(), [results[e][\"train_acc\"] for e in results.keys()], linestyle=\"-\", marker=\".\") \n",
    "plt.plot(results.keys(), [results[e][\"val_acc\"] for e in results.keys()], linestyle=\"--\", marker=\"x\") \n",
    "plt.savefig(\"few_shots_lr_0001_bs_100.png\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3a76b1886d91239a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "log_file_paths = [\n",
    "    \"logs/data_aug_spatial_dropot_lr_0001_bs_100.txt\",\n",
    "    \"logs/data_aug_spatial_dropout_lr_0001_bs_100_extra10.txt\",\n",
    "    \"logs/data_aug_spatial_droput_lr_0001_bs_100._extra10_after10.txt\",\n",
    "    \"logs/colab_1_st_30ep_data_aug_spatial_drop_out_lr_0001_bs_100.txt\",\n",
    "    \"logs/colab_2nd_40ep_data_aug_spatial_drop_out_lr_0001_bs_100.txt\"\n",
    "]\n",
    "results = defaultdict(lambda: defaultdict(dict))\n",
    "header_pattern = re.compile(r\"Epoch\\s+1/10\\s+m\")\n",
    "train_val_pattern = re.compile(\n",
    "    r\"Epoch\\s*\\[(\\d+)/(\\d+)\\]\\s+Train Loss:\\s*([\\d\\.]+)\\s*\\|\\s*Train Acc:\\s*([\\d\\.]+)%\\s*\\|\\s*Val Loss:\\s*([\\d\\.]+)\\s*\\|\\s*Val Acc:\\s*([\\d\\.]+)%\"\n",
    ")\n",
    "current_epoch = 0\n",
    "for file_path in log_file_paths:\n",
    "    with open(file_path, \"r\") as f:\n",
    "        for line in f:\n",
    "            tv_match = train_val_pattern.search(line)\n",
    "            if tv_match is not None:\n",
    "                current_epoch+=1;\n",
    "                train_acc_str  = tv_match.group(4)\n",
    "                val_acc_str   = tv_match.group(6)\n",
    "\n",
    "            # Convert to floats\n",
    "                train_acc = float(train_acc_str)\n",
    "                val_acc   = float(val_acc_str)\n",
    "\n",
    "            # Store in our results dictionary\n",
    "                results[current_epoch] = {\n",
    "                    \"train_acc\": train_acc,\n",
    "                    \"val_acc\":   val_acc\n",
    "                }\n",
    "               "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5caeb05274620c1f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "results.keys()\n",
    "epochs = len(results)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Data Augemnt. Spartial Drp. Out lr 0.001 bs 100\")\n",
    "plt.plot(results.keys(), [results[e][\"train_acc\"] for e in results.keys()], linestyle=\"-\", marker=\".\")\n",
    "plt.plot(results.keys(), [results[e][\"val_acc\"] for e in results.keys()], linestyle=\"--\", marker=\"x\")\n",
    "plt.savefig(\"Data_Augument_Spatial_Drp_lr_0001_bs_100.png\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "195fd8ad8172d7b9"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "log_file_paths = [\n",
    "    \"logs/5_epoch_trail_bs_20_lr_0001.txt\",\n",
    "    \"logs/8_additional_epoch_trail_bs_20_lr_0001.txt\",\n",
    "]\n",
    "\n",
    "results = defaultdict(lambda: defaultdict(dict))\n",
    "header_pattern = re.compile(r\"Epoch\\s+1/10\\s+m\")\n",
    "train_val_pattern = re.compile(\n",
    "    r\"Epoch\\s*\\[(\\d+)/(\\d+)\\]\\s+Train Loss:\\s*([\\d\\.]+)\\s*\\|\\s*Train Acc:\\s*([\\d\\.]+)%\\s*\\|\\s*Val Loss:\\s*([\\d\\.]+)\\s*\\|\\s*Val Acc:\\s*([\\d\\.]+)%\"\n",
    ")\n",
    "current_epoch = 0\n",
    "for file_path in log_file_paths:\n",
    "    with open(file_path, \"r\") as f:\n",
    "        for line in f:\n",
    "            tv_match = train_val_pattern.search(line)\n",
    "            if tv_match is not None:\n",
    "                current_epoch+=1;\n",
    "                train_acc_str  = tv_match.group(4)\n",
    "                val_acc_str   = tv_match.group(6)\n",
    "                train_acc = float(train_acc_str)\n",
    "                val_acc   = float(val_acc_str)\n",
    "                results[current_epoch] = {\n",
    "                    \"train_acc\": train_acc,\n",
    "                    \"val_acc\":   val_acc\n",
    "                }\n",
    "  "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6afc10644e3d8844"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "epochs = len(results)\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Batche size 20 learning rate 0.001\")\n",
    "plt.plot(results.keys(), [results[e][\"train_acc\"] for e in results.keys()], linestyle=\"-\", marker=\".\") \n",
    "plt.plot(results.keys(), [results[e][\"val_acc\"] for e in results.keys()], linestyle=\"--\", marker=\"x\") \n",
    "plt.savefig(\"Bs_20_lr_0001_accuracy.png\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1bfdb09f211b2a1e"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
